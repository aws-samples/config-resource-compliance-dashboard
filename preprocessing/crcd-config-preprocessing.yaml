AWSTemplateFormatVersion: '2010-09-09'
Description: 'AWS Config Resource Compliance Dashboard (CRCD) - Preprocessing AWS Config files using AWS Glue'

# In environments with a large number of resources, AWS Config may generate files that are bigger that 32MB (uncompressed).
# This will break an Athena service limit and make it impossible to run queries, as all queries that try to open one of those files will fail.
# These resources will pre-process AWS Config files and generate smaller files that can potentially avoid the issue.

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      -
        Label:
          default: "AWS Config logging - the source of data for the pre-processing job, where your AWS Config files are delivered"
        Parameters:
          # TODO ask for the LogArchive account id and validate it is THE CURRENT account - LogArchiveAccountId
          - LogArchiveBucketName
          # TODO support KMS encrypted log archive bucket - LogArchiveBucketKmsKeyArn
      -
        Label:
          # TODO remove default: "Dashboard bucket - destination bucket of the pre-processing job and source of data for the dashboard" 
          default: "Pre-processing job configuration"
        Parameters:
          - ConfigureS3EventNotificationToLambdaJobProducer
          - DashboardBucketNamePrefix
      -
        Label:
          default: "Technical Parameters (DO NOT CHANGE)"
        Parameters:
          - PreProcessingLambdaProducerName
          - PreProcessingGlueJobName
          - DynamoDBJobTrackingTableName # TODO, make this optional
          - LambdaSupportJobTriggerConfigurationName
          - LambdaSupportGlueJobUploaderName
    
    ParameterLabels:
      LogArchiveBucketName:
        default: "Log Archive bucket"
      DashboardBucketNamePrefix:
        default: "Dashboard bucket (prefix)"
      PreProcessingGlueJobName:
        default: "CRCD Glue Job name"
      DynamoDBJobTrackingTableName:
        default: "Dynamo DB tracking table"
      ConfigureS3EventNotificationToLambdaJobProducer:
        default: "Configure S3 event notification to trigger the job automatically on every new AWS Config file" 
      LambdaSupportJobTriggerConfigurationName:
        default: "Name of the Lambda function that performs supporting configurations: S3 object notification"
      LambdaSupportGlueJobUploaderName:
        default: "Name of the Lambda function that performs supporting configurations: Glue Job script upload"
      PreProcessingLambdaProducerName:
        default: "Name of the Lambda function that triggers the pre-processing job"


Parameters:
  LogArchiveBucketName:
    Type: String
    Description: "Name of the Amazon S3 bucket collecting AWS Config files (Required)."
    MinLength: 1
    AllowedPattern: '^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$'
    ConstraintDescription: "Log Archive bucket name is missing or does not satisfy the Amazon S3 naming convention."

  DashboardBucketNamePrefix:
    # TODO if customer installs on Log Archive account, this will be the Dashboard bucket. 
    # If the customer installs on a dedicated Dashboard account, this will be confusing. Think about a better name.
    Type: String
    Description: "Prefix of the Amazon S3 bucket that will store the transformed AWS Config files. This can be used as source of data for the dashboard. The Dashboard bucket will be created with the prefix you specify here, account ID and regions will be added automatically (Required)."
    AllowedPattern: '^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$' # TODO remove length of region and 12 digit account number
    MinLength: 1
    Default: 'crcd-dashboard-bucket' 
    ConstraintDescription: "Dashboard bucket prefix name is missing or does not satisfy the Amazon S3 naming convention."

  PreProcessingLambdaProducerName:
    Type: String
    Default: "crcd-config-preprocessing-producer"
    MinLength: 1
    ConstraintDescription: "Required: AWS Lambda function that triggers the preprocessing job of AWS Config files."
    Description: "Name of the AWS Lambda function that triggers the preprocessing job of AWS Config files (Required)."

  PreProcessingGlueJobName:
    Type: String
    Default: "crcd-config-file-preprocessing-job"
    MinLength: 1
    ConstraintDescription: "Required: name of the AWS Glue job that pre-processes AWS Config files."
    Description: "Name of the AWS Glue job that pre-processes AWS Config files (Required)."

  DynamoDBJobTrackingTableName:
    Type: String
    Default: 'CRCDPreprocessingJobTracking'
    MinLength: 1
    ConstraintDescription: "Required: name of the Amazon Dynamo DB table used to track the activities of the pre-processing Glue job."
    Description: "Name of the Amazon Dynamo DB table used to track the activities of the pre-processing Glue job (Required)."

  ConfigureS3EventNotificationToLambdaJobProducer:
    Type: "String"
    Description:  "Select 'yes' to configure event notifications that will trigger the pre-processing job whenever there is a new AWS Config file added to the bucket. Select 'no' if you already have S3 event notifications configured on the Log Archive bucket. In this case, you must configure S3 event notifications manually."
    AllowedValues: ['<select>', 'yes', 'no']
    Default: "<select>" # This value is used below, careful if you change it

  LambdaSupportJobTriggerConfigurationName:
    Type: "String"
    Default: "crcd-support-configure-s3-notification-preprocessing-job"
    MinLength: 1
    ConstraintDescription: "Required: AWS Lambda function that configures S3 notifications on the Log Archive bucket."
    Description: "Name of the AWS Lambda function that configures S3 notifications on the Log Archive bucket (Required)."

  LambdaSupportGlueJobUploaderName:
    Type: "String"
    Default: "crcd-support-configure-glue-job"
    MinLength: 1
    ConstraintDescription: "Required: AWS Lambda function that uploads the source code for the preprocessing Glue Job."
    Description: "Name of the AWS Lambda function that uploads the source code for the preprocessing Glue Job (Required)."


Conditions:
  IsConfigureS3EventNotificationToLambdaJobProducer:
    # some customers may already have S3 notifications out of the Log Archive bucket
    # we must not override that - and users will have to do that manually later
    !Equals [!Ref ConfigureS3EventNotificationToLambdaJobProducer, 'yes']

Rules:
  MandatoryLogArchiveBucketName:
    Assertions:
      - Assert: !Not
         - !Equals
          - !Ref LogArchiveBucketName
          - ''
        AssertDescription: "Log Archive bucket name is required"

  MandatoryDashboardBucketNamePrefix:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref DashboardBucketNamePrefix
          - ''
        AssertDescription: "Dashboard bucket name prefix is required"
  
  MandatoryPreProcessingLambdaProducerName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref PreProcessingLambdaProducerName
          - ''
        AssertDescription: "Lambda function name is required"

  MandatoryPreProcessingGlueJobName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref PreProcessingGlueJobName
          - ''
        AssertDescription: "AWS Glue job name is required"

  MandatoryLambdaSupportJobTriggerConfigurationName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref LambdaSupportJobTriggerConfigurationName
          - ''
        AssertDescription: "Lambda function name is required"

  MandatoryLambdaSupportGlueJobUploaderName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref LambdaSupportGlueJobUploaderName
          - ''
        AssertDescription: "Lambda function name is required"

  ValidateConfigureS3EventNotificationToLambdaJobProducer:
    # must be selected
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref ConfigureS3EventNotificationToLambdaJobProducer
          - '<select>'
        AssertDescription: "REQUIRED PARAMETER - Select 'yes' if you want this template to configure the S3 event notification that triggers the pre-processing job when new AWS Config files are received. Select 'no' if you already have S3 event notifications on the Log Archive bucket and want to configure this manually."




Resources:

  # *************************************************************************************************************************
  # *************************************************************************************************************************
  # S3 buckets and resources that deal with S3 buckets
  # *************************************************************************************************************************
  # *************************************************************************************************************************

  # S3 notifications to trigger the Lambda Job Producer function - only if user selected that
  # S3 Bucket Permission for Lambda - Log Archive bucket to trigger this Lambda on each new object
  LambdaInvocationPermissionPreprocessingJobProducer:
    Type: AWS::Lambda::Permission
    Condition: IsConfigureS3EventNotificationToLambdaJobProducer
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref ConfigFilePreprocessingProducerLambda # Lambda to trigger
      Principal: s3.amazonaws.com
      SourceAccount: !Ref AWS::AccountId
      SourceArn: !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}"


  ConfigBucketNotificationConfigurationLambdaJobProducerTrigger:
    Type: Custom::ConfigBucketLambdaJobProducer
    Condition: IsConfigureS3EventNotificationToLambdaJobProducer
    DependsOn:
    - LambdaInvocationPermissionPreprocessingJobProducer
    Properties:
      ServiceToken: !GetAtt ConfigBucketNotificationConfigurationLambdaJobProducer.Arn
      Bucket: !Ref LogArchiveBucketName
      # this is needed so that the Lambda function deletes only S3 notification configurations if they have this name
      # passing it here and as part of NotificationConfiguration so that I do not have to hard-code this string on the Lambda code 
      EventNotificationName: "CRCDPreprocessingJobTrigger"
      NotificationConfiguration:
        LambdaFunctionConfigurations:
        - Events: ['s3:ObjectCreated:*']
          LambdaFunctionArn: !GetAtt ConfigFilePreprocessingProducerLambda.Arn
          # this is needed so that the Lambda function deletes only S3 notification configurations if they have this name
          Id: "CRCDPreprocessingJobTrigger"

  ConfigBucketNotificationConfigurationLambdaJobProducerRole:
    Type: AWS::IAM::Role
    Condition: IsConfigureS3EventNotificationToLambdaJobProducer
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3BucketNotificationPolicyJobProducer
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:PutBucketNotification'
                  - 's3:GetBucketNotification'
                Resource: !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}"

  # Lambda triggered by CloudFormation to configure bucket notification on AWS Config Log Archive bucket
  ConfigBucketNotificationConfigurationLambdaJobProducer:
  # checkov:skip=CKV_AWS_115: This is called only once by the CFN template.
  # checkov:skip=CKV_AWS_116: No DLQ by design. If this lambda fails, the whole CFN template fails.
    Type: AWS::Lambda::Function
    Condition: IsConfigureS3EventNotificationToLambdaJobProducer
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      FunctionName: !Sub "${LambdaSupportJobTriggerConfigurationName}"
      Description: "CRCD Dashboard - Configures the Log Archive bucket S3 event notification to trigger the CRCD Lambda Preprocessing Job Producer function (one-time execution)"
      Handler: index.lambda_handler
      Role: !GetAtt ConfigBucketNotificationConfigurationLambdaJobProducerRole.Arn
      Timeout: 300
      Runtime: "python3.12"
      MemorySize: 128
      EphemeralStorage:
        Size: 512
      Architectures:
        - "x86_64"
      TracingConfig:
        Mode: "Active"
      VpcConfig:
        SecurityGroupIds: []
        SubnetIds: []
        Ipv6AllowedForDualStack: false
      RuntimeManagementConfig:
        UpdateRuntimeOn: "Auto"
      LoggingConfig:
        LogFormat: "Text"
        # this is already by default LogGroup: "/aws/lambda/crcd-support-configure-s3-event-notification"
      Code:
        ZipFile: |
          import json
          import boto3

          def lambda_handler(event, context):
              
              print("Request received:\n", json.dumps(event))
              
              # Get the necessary parameters from the event
              params = event['ResourceProperties']
              bucket_name = params['Bucket']
              notification_event_name = params['EventNotificationName']
              notification_config = params['NotificationConfiguration']

              s3 = boto3.client('s3')

              if event['RequestType'] == 'Create':
                  # Check for existing notification configuration
                  # The CRCD Dashboard will not overwrite existing event notifications, and this function will fail
                  try:
                      existing_config = s3.get_bucket_notification_configuration(Bucket=bucket_name)
                      if has_notifications(existing_config):
                          # If there's an existing configuration, fail the Lambda function
                          error_message = f"Existing S3 event configuration found on bucket {bucket_name}. CRCD dashboard deployment cannot proceed."
                          print(error_message)
                          print(f"Existing S3 event configuration: {existing_config}")
                          send_response(event, context, 'FAILED', {'Error': error_message})
                          return  # Exit the function early
                  except Exception as e:
                      error_message = f"Error checking existing configuration: {str(e)}"
                      print(error_message)
                      send_response(event, context, 'FAILED', {'Error': error_message})
                      return  # Exit the function early

              if event['RequestType'] == 'Delete':
                  # Must check if the current event notification is the one from CRCD, only in that case it will be deleted
                  # Will cycle to all notification configurations and delete only the one pertaining to CRCD
                  try:
                      existing_config = s3.get_bucket_notification_configuration(Bucket=bucket_name)

                      # Initialize new configuration
                      new_notification_config = {
                          'LambdaFunctionConfigurations': [],
                          'QueueConfigurations': [],
                          'TopicConfigurations': []
                      }
                      
                      # Flag to check if we found and removed the matching configuration
                      config_removed = False

                      # Iterate through all types of configurations
                      # There could be non overlapping event notifications out of the bucket
                      for config_type in ['LambdaFunctionConfigurations', 'QueueConfigurations', 'TopicConfigurations']:
                          for config in existing_config.get(config_type, []):
                              if config.get('Id') == notification_event_name:
                                  config_removed = True
                                  print(f"Removing CRCD configuration with Id: {notification_event_name}")
                              else:
                                  print(f"Found non-CRCD configuration: {config.get('Id')}")
                                  new_notification_config[config_type].append(config)

                      # Remove empty lists from the new_notification_config
                      # e.g. 'TopicConfigurations' if none were found
                      new_notification_config = {k: v for k, v in new_notification_config.items() if v}

                      if not config_removed:
                          print(f"No matching event notification found for {notification_event_name}. No changes made.")
                          send_response(event, context, 'SUCCESS', {})
                          return

                      # Update notification_config to be used in put_bucket_notification_configuration
                      notification_config = new_notification_config
                      
                  except Exception as e:
                      error_message = f"Error processing existing configuration: {str(e)}"
                      print(error_message)
                      send_response(event, context, 'FAILED', {'Error': error_message})
                      return
              
              try:
                  # Configure the bucket notification
                  s3.put_bucket_notification_configuration(
                      Bucket=bucket_name,
                      NotificationConfiguration=notification_config
                  )
                  
                  # Send a successful response back to CloudFormation
                  send_response(event, context, 'SUCCESS', {})
              
              except Exception as e:
                  # Send a failed response back to CloudFormation
                  send_response(event, context, 'FAILED', {'Error': str(e)})

          def has_notifications(config):
              # Check if the configuration has any S3 notifications as indicated by the types below
              # pass the outcome of s3.get_bucket_notification_configuration
              notification_types = ['TopicConfigurations', 'QueueConfigurations', 'LambdaFunctionConfigurations']
              return any(config.get(type_) for type_ in notification_types)

          def send_response(event, context, response_status, response_data):
              response_body = json.dumps({
                  'Status': response_status,
                  'Reason': 'See the details in CloudWatch Log Stream: ' + context.log_stream_name,
                  'PhysicalResourceId': context.log_stream_name,
                  'StackId': event['StackId'],
                  'RequestId': event['RequestId'],
                  'LogicalResourceId': event['LogicalResourceId'],
                  'Data': response_data
              })

              response_url = event['ResponseURL']
              
              import urllib.request
              req = urllib.request.Request(response_url, data=response_body.encode('utf-8'), method='PUT')
              with urllib.request.urlopen(req) as f:
                  print(f.read())
                  print(f.info())
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W92
            reason: "This function does not need reserved concurrent executions"


  ConfigBucketNotificationConfigurationLambdaJobProducerLogGroup:
  # checkov:skip=CKV_AWS_158: No KMS encryption needed
    Type: AWS::Logs::LogGroup
    Condition: IsConfigureS3EventNotificationToLambdaJobProducer
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaSupportJobTriggerConfigurationName}"
      RetentionInDays: 14
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W84
            reason: "No KMS encryption needed"

  DashboardBucket:
  # checkov:skip=CKV_AWS_18: No need of Access Logging, customers will add it if required
    Type: AWS::S3::Bucket
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      BucketName: !Sub '${DashboardBucketNamePrefix}-${AWS::AccountId}-${AWS::Region}'
      # TODO bucket encryption must be KMS if the original Log Archive bucked is KMS encrypted
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      AccessControl: BucketOwnerFullControl
      # This bucket needs versioning because the default Control Tower AWS Config bucket has it
      # And the Dashboard account installation will use replication, that requires versioning
      VersioningConfiguration:
        Status: Enabled
      OwnershipControls:
        Rules:
          - ObjectOwnership: BucketOwnerEnforced
      # BSC12 risk mitigation
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
    Metadata:
      cfn-lint:
        config:
          ignore_checks:
            - W3045 # Consider using AWS::S3::BucketPolicy instead of AccessControl
      # TODO see if you need this
      #cfn_nag:
      #  rules_to_suppress:
      #    - id: W35
      #      reason: "We accept Athena query result bucket has no access logging"
      #    - id: W51
      #      reason: "We accept Athena query result bucket has no bucket policy"
  

  # *************************************************************************************************************************
  # *************************************************************************************************************************
  # Lambda that triggers the Glue job for every new AWS Config file on Log Archive bucket
  # *************************************************************************************************************************
  # *************************************************************************************************************************

  IAMRoleConfigFilePreprocessingProducerLambda:
    Type: AWS::IAM::Role
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      RoleName: "crcd-lambda-preprocessing-job-producer-role"
      Description: "CRCD Dashboard - Allows to trigger Glue jobs, send logs to Cloudwatch, receive objects from AWS Config bucket."
      Path: "/"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: crcd-preprocessing-trigger-glue-dynamodb-s3 
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - glue:StartJobRun
                Resource: !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:job/${ConfigFilePreprocessingGlueJob}"
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                Resource: !GetAtt CRCDJobTrackingTable.Arn
              - Effect: Allow
                Action:
                  - "s3:GetObject"
                  - "s3:ListBucket"
                  - "s3:ListBucketVersions"
                  - "s3:GetObjectVersion"
                  - "s3:GetLifecycleConfiguration"
                Resource: 
                  - !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}"
                  - !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}/*"
              - Effect: Allow
                Action:
                  - "s3:PutObject"
                Resource:
                  # TODO check it works 
                  - !Ref DashboardBucket
                  - !Sub '${DashboardBucket.Arn}/*'
                  # - !Sub "arn:${AWS::Partition}:s3:::${DashboardBucketNamePrefix}-${AWS::AccountId}-${AWS::Region}"
                  # - !Sub "arn:${AWS::Partition}:s3:::${DashboardBucketNamePrefix}-${AWS::AccountId}-${AWS::Region}/*"

  # Lambda Function that triggers Glue jobs
  ConfigFilePreprocessingProducerLambda:
  # checkov:skip=CKV_AWS_115: Cuncurrency limit needs are customer-dependent. This lambda is called whenever AWS Config delivers a file to S3, we don't anticipate the need for reserved concurrency for the moment.
  # checkov:skip=CKV_AWS_173: Environment variables are not sensitive.
  # checkov:skip=CKV_AWS_116: No DLQ needed
    Type: AWS::Lambda::Function
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      FunctionName: !Sub "${PreProcessingLambdaProducerName}" 
      Description: "CRCD Dashboard - Lambda function that triggers a Glue job to pre-process new AWS Config files"
      Handler: index.lambda_handler
      Role: !GetAtt IAMRoleConfigFilePreprocessingProducerLambda.Arn
      LoggingConfig:
        LogFormat: "Text"
      Runtime: python3.12
      Timeout: 120
      EphemeralStorage:
        Size: 512
      MemorySize: 128
      Architectures:
        - "x86_64"
      VpcConfig:
        SecurityGroupIds: []
        SubnetIds: []
        Ipv6AllowedForDualStack: false
      Environment:
        Variables:
          DYNAMODB_TRACKING_TABLE_NAME: !Ref DynamoDBJobTrackingTableName
          DASHBOARD_BUCKET_NAME: !Ref DashboardBucket
          GLUE_JOB_NAME: !Ref ConfigFilePreprocessingGlueJob
      Code:
        ZipFile: |
          import boto3
          import json
          from datetime import datetime
          import os
          import re

          # This regular expressions pattern is compatible with how ControlTower Config logs AND also with how Config Logs are stored in S3 in standalone account
          # Structure for Config Snapshots: ORG-ID/AWSLogs/ACCOUNT-NUMBER/Config/REGION/YYYY/MM/DD/ConfigSnapshot/objectname.json.gz
          # Object name follows this pattern: ACCOUNT-NUMBER_Config_REGION_ConfigSnapshot_TIMESTAMP-YYYYMMDDHHMMSS_RANDOM-TEXT.json.gz
          # For example: 123412341234_Config_eu-north-1_ConfigSnapshot_20240306T122755Z_09c5h4kc-3jc7-4897-830v-d7a858325638.json.gz
          PATTERN = r'^(?P<org_id>[\w-]+)?/?AWSLogs/(?P<account_id>\d+)/Config/(?P<region>[\w-]+)/(?P<year>\d+)/(?P<month>\d+)/(?P<day>\d+)/(?P<type>ConfigSnapshot|ConfigHistory)/[^//]+$'

          def lambda_handler(event, context):
              glue = boto3.client('glue')

              # Get the table name from environment variable
              table_name = os.environ["DYNAMODB_TRACKING_TABLE_NAME"]
              
              # Use the environment variable for the table name
              dynamodb = boto3.resource('dynamodb').Table(table_name)

              # Gets more environment variables
              dashboard_bucket_name = os.environ["DASHBOARD_BUCKET_NAME"]
              preprocessing_glue_job = os.environ["GLUE_JOB_NAME"]
              
              records = event['Records']
              for record in records:
                  bucket = record['s3']['bucket']['name']
                  key = record['s3']['object']['key']

                  # process object key
                  match  = re.match(PATTERN, key)
                  if not match:
                      print(f'SKIPPING: Cannot match {key} as AWS Config file, skipping.')
                      continue

                  # Extract the filename from the full path
                  filename = os.path.basename(key)
                  # Remove the .json.gz extension if present
                  filename = filename.replace('.json.gz', '')

                  # Split by underscore and take the last two parts
                  parts = filename.split('_')
                  if len(parts) >= 2:
                      timestamp_and_random = f"{parts[-2]}_{parts[-1]}"
                  else:
                      # Fallback: use the entire filename without extensions
                      timestamp_and_random = filename
                  
                  run_id = timestamp_and_random
                  
                  # Record job start in DynamoDB
                  dynamodb.put_item(
                      Item={
                          'source_file': key,
                          'job_run_id': run_id,
                          'status': 'STARTED',
                          'start_time': datetime.now().isoformat(),
                          'ttl': int((datetime.now().timestamp()) + (90 * 24 * 60 * 60))  # 90 days TTL
                      }
                  )
                  
                  # Start Glue job
                  try:
                      response = glue.start_job_run(
                          JobName=preprocessing_glue_job,
                          Arguments={
                              '--tracking_table_name': f"{table_name}",
                              '--source_path': f"s3://{bucket}/{key}",
                              '--destination_path': f"s3://{dashboard_bucket_name}/",
                              '--CRCD_JOB_RUN_ID': run_id
                          }
                      )
                      
                      print(f"Started Glue job for bucket {bucket} on object {key} with run ID: {response['JobRunId']}")
                      
                  except Exception as e:
                      print(f"Error starting Glue job for {key}: {str(e)}")
                      raise
              
              return {
                  'statusCode': 200,
                  'body': json.dumps('Successfully processed S3 event')
              }
      

  # log group for the lambda, with retention period
  ConfigFilePreprocessingProducerLambdaLogGroup:
  # checkov:skip=CKV_AWS_158: No KMS encryption needed
    Type: AWS::Logs::LogGroup
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      LogGroupName: !Sub "/aws/lambda/${PreProcessingLambdaProducerName}"
      RetentionInDays: 14
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W84
            reason: "No KMS encryption needed"

  # *************************************************************************************************************************
  # *************************************************************************************************************************
  # Dynamo DB table to track the pre-processing activities
  # *************************************************************************************************************************
  # *************************************************************************************************************************

  # DynamoDB Table
  CRCDJobTrackingTable:
  # checkov:skip=CKV_AWS_119: This is a tracking table, no need for encryption
  # checkov:skip=CKV_AWS_28: This is a tracking table, no need for point-in-time recovery
    Type: AWS::DynamoDB::Table
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      TableName: !Ref DynamoDBJobTrackingTableName
      AttributeDefinitions:
        - AttributeName: source_file
          AttributeType: S
        - AttributeName: job_run_id
          AttributeType: S
      KeySchema:
        - AttributeName: source_file
          KeyType: HASH
        - AttributeName: job_run_id
          KeyType: RANGE
      BillingMode: PAY_PER_REQUEST
      TimeToLiveSpecification:
        AttributeName: ttl
        Enabled: true


  # *************************************************************************************************************************
  # *************************************************************************************************************************
  # Glue Job that performs the ETL pre-processing of AWS Config files
  # *************************************************************************************************************************
  # *************************************************************************************************************************

  # Glue Job Script - a Lambda triggered by CloudFormation will create the Glue script
  GlueScriptUploaderLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      Description: "CRCD Dashboard - Glue Job will have permission to deliver ETL objects to a destination bucket"
      RoleName: "crcd-glue-script-uploader-lambda-role"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3DashboardBucketAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource: !Sub '${DashboardBucket.Arn}/crcd-scripts/*'

  GlueScriptUploaderLambda:
  # checkov:skip=CKV_AWS_115: Cuncurrency limit needs are customer dependent. This lambda is called whenever AWS Config delivers a file to S3, we don't anticipate the need for reserved concurrency for the moment.
  # checkov:skip=CKV_AWS_116: No DQL needed.
    Type: AWS::Lambda::Function
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      FunctionName: !Sub "${LambdaSupportGlueJobUploaderName}"
      Description: "CRCD Dashboard - Supports the deployment of the preprocessing Glue Job (one-time execution)"
      Runtime: python3.12
      Timeout: 120
      MemorySize: 128
      Architectures:
        - "x86_64"
      TracingConfig:
        Mode: "Active"
      VpcConfig:
        SecurityGroupIds: []
        SubnetIds: []
        Ipv6AllowedForDualStack: false
      LoggingConfig:
        LogFormat: "Text"
      Handler: index.lambda_handler
      Role: !GetAtt GlueScriptUploaderLambdaRole.Arn
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import json
          
          def lambda_handler(event, context):
              try:
                  if event['RequestType'] in ['Create', 'Update']:
                      s3 = boto3.client('s3')
                      bucket = event['ResourceProperties']['Bucket']
                      key = event['ResourceProperties']['Key']
                      content = event['ResourceProperties']['Content']
                      
                      # Upload the script to S3
                      s3.put_object(
                          Bucket=bucket,
                          Key=key,
                          Body=content,
                          ContentType='text/x-python'
                      )
                      
                      response_data = {
                          'Message': f'Successfully uploaded script to s3://{bucket}/{key}'
                      }
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, response_data)
                  
                  elif event['RequestType'] == 'Delete':
                      # Handle delete event, should we delete the Glue Job file?
                      # Not now. The bucket has all other ETL'ed AWS Config files and would need to be emptied anyway.
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                      
              except Exception as e:
                  cfnresponse.send(event, context, cfnresponse.FAILED, {'Error': str(e)})
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W58
            reason: "Permission to write CloudWatch logs is given in IAMRoleLambdaPartitionerConfig"
          - id: W92
            reason: "This function does not need reserved concurrent executions"

  GlueScriptUploaderLambdaLogGroup:
  # checkov:skip=CKV_AWS_158: No KMS encryption needed
    Type: AWS::Logs::LogGroup
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaSupportGlueJobUploaderName}"
      RetentionInDays: 14
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W84
            reason: "No KMS encryption needed"

  # Triggers the Lmabda from CloudFormation
  GlueScript:
    Type: Custom::GlueScriptUploaderLambda
    Properties:
      ServiceToken: !GetAtt GlueScriptUploaderLambda.Arn
      Bucket: !Ref DashboardBucket
      Key: !Sub 'crcd-scripts/${PreProcessingGlueJobName}-DO-NOT-DELETE.py'
      Content: |
        import sys
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        from awsglue.dynamicframe import DynamicFrame
        import boto3
        from datetime import datetime
        import os
        import json
        import gzip
        import io
        import random
        import string
        import urllib.parse
        from collections import Counter


        # Get job parameters
        args = getResolvedOptions(sys.argv, [
            'JOB_NAME',
            'tracking_table_name',
            'source_path',
            'destination_path',
            'CRCD_JOB_RUN_ID'
        ])

        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        logger = glueContext.get_logger()  # Get the Glue logger
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)

        # Configure Spark for better GZIP processing
        spark.conf.set("spark.sql.files.maxPartitionBytes", 134217728)  # 128MB
        spark.conf.set("spark.sql.files.openCostInBytes", 134217728)  # 128MB
        spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
        spark.conf.set("spark.sql.adaptive.enabled", "true")


        # Initialize DynamoDB client
        dynamodb = boto3.resource('dynamodb')
        # table = dynamodb.Table('CRCDFlatteningJobTracking') # TODO delete when tested
        table = dynamodb.Table(args['tracking_table_name'])


        def decode_s3_key(s3_path):
            """Decode only the key part of the S3 path"""
            if not s3_path.startswith('s3://'):
                return s3_path
            
            # Split the S3 path into bucket and key
            parts = s3_path[5:].split('/', 1)
            if len(parts) < 2:
                return s3_path
            
            bucket = parts[0]
            key = parts[1]
            
            # Decode only the key part
            decoded_key = urllib.parse.unquote(key)
            
            return f"s3://{bucket}/{decoded_key}"

        def get_relative_path(source_path):
            """Extract the relative path from the full S3 path"""
            decoded_path = decode_s3_key(source_path)
            parts = decoded_path.replace('s3://', '').split('/', 1)
            if len(parts) > 1:
                return parts[1]
            return ''

        def get_destination_path(source_path, destination_base, filename):
            """Generate destination path maintaining the source directory structure"""
            relative_path = get_relative_path(source_path)
            dir_path = os.path.dirname(relative_path)
            
            # Ensure the destination base is properly decoded
            decoded_destination_base = decode_s3_key(destination_base)
            
            if dir_path:
                return f"{decoded_destination_base.rstrip('/')}/{dir_path}/{filename}"
            return f"{decoded_destination_base.rstrip('/')}/{filename}"

        def update_job_status(status, processed_items=0, error_message=None):
            try:
                update_expression = "SET #status = :status, end_time = :end_time, processed_items = :processed_items"
                expression_values = {
                    ':status': status,
                    ':end_time': datetime.now().isoformat(),
                    ':processed_items': processed_items
                }
                
                if error_message:
                    update_expression += ", error_message = :error_message"
                    expression_values[':error_message'] = error_message

                source_file = get_relative_path(args['source_path'])
                
                table.update_item(
                    Key={
                        'source_file': source_file,
                        'job_run_id': args['CRCD_JOB_RUN_ID']
                    },
                    UpdateExpression=update_expression,
                    ExpressionAttributeNames={
                        '#status': 'status'
                    },
                    ExpressionAttributeValues=expression_values
                )
            except Exception as e:
                print(f"Error updating DynamoDB: {str(e)}")

        def sanitize_s3_name(name):
            # Generate random string of 10 alphanumeric characters
            default_name = 'unknownResourceId'.join(random.choices(string.ascii_letters + string.digits, k=10))
            if not name:
                return default_name
                
            # Replace characters that could cause issues in S3 paths
            replacements = {
                '/': '-',
                '\\': '-',
                ':': '-',
                '*': '-',
                '?': '-',
                '"': '-',
                '<': '-',
                '>': '-',
                '|': '-',
                ' ': '-',
                '=': '-',
                '@': '-',
                '#': '-',
                '$': '-',
                '&': '-',
                '{': '-',
                '}': '-',
                '[': '-',
                ']': '-',
                '`': '-',
                "'": '-',
                '!': '-',
                '+': '-',
                '^': '-',
                ',': '-'
            }
            
            for char, replacement in replacements.items():
                name = name.replace(char, replacement)
            
            # Replace multiple consecutive dashes with a single dash
            while '--' in name:
                name = name.replace('--', '-')
            
            # Remove leading and trailing dashes
            name = name.strip('-')
            
            # If after all replacements name is empty, generate random string
            if not name:
                return default_name
            
            return name

        # S3 has a limit on the name of objects
        def truncate_s3_key(key, max_length=1000):  # leaving some room for safety
            if len(key.encode('utf-8')) <= max_length:
                return key
            
            # Split the key into parts
            base, extension = os.path.splitext(key)
            if extension == '.gz':  # handle double extension .json.gz
                base, json_ext = os.path.splitext(base)
                extension = json_ext + extension
            
            # Calculate how many bytes we need to remove
            current_length = len(key.encode('utf-8'))
            excess_bytes = current_length - max_length
            
            # Truncate the base name, preserving the extension
            truncated_base = base.encode('utf-8')[:-excess_bytes-1].decode('utf-8', errors='ignore')
            return truncated_base + extension

        def process_file():    
            try:
                logger.info("10====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====")
                logger.info(f"Original source path: {args['source_path']}")
                logger.info(f"DDB Table name: {args['tracking_table_name']}")


                # if %3A is on the file name, glue does not find it
                source_path = decode_s3_key(args['source_path'])
                logger.info("11====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====")
                logger.info(f"Decoded source path: {source_path}")

                
                # Use Glue's native S3 reading capabilities instead of sc.wholeTextFiles
                try:
                    # For gzipped JSON files, we need to use the grokLog format with custom patterns
                    # or use the format="json" with compression="gzip" if the files are properly formatted
                    dynamic_frame = glueContext.create_dynamic_frame.from_options(
                        connection_type="s3",
                        connection_options={
                            "paths": [source_path],
                            "recurse": False,
                            "compressionType": "gzip"
                        },
                        format="json"
                    )
                    
                    # Convert to DataFrame for easier processing
                    df = dynamic_frame.toDF()
                    
                    logger.info("20====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====")
                    logger.info(f"dataframe.count = {df.count()}")
                    
                    # If the DataFrame is empty, try alternative approach
                    if df.count() == 0:
                        logger.info("30====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====")
                        logger.info("No data read using standard approach, trying alternative method...")
                        
                        # Use boto3 to read the file directly
                        s3_path = args['source_path']
                        path_parts = s3_path.replace('s3://', '').split('/', 1)
                        bucket = path_parts[0]
                        key = path_parts[1] if len(path_parts) > 1 else ''
                        
                        s3 = boto3.client('s3')
                        
                        logger.info("40====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====")
                        logger.info(f"getting object {key} from bucket {bucket}")
                        
                        
                        response = s3.get_object(Bucket=bucket, Key=key)
                        content = response['Body'].read()
                        
                        # Decompress if gzipped
                        if key.endswith('.gz'):
                            content = gzip.decompress(content)
                        
                        # Parse JSON
                        json_content = json.loads(content.decode('utf-8'))
                        
                        # Create a DataFrame from the JSON
                        json_rdd = sc.parallelize([json.dumps(json_content)])
                        df = spark.read.json(json_rdd)
                    
                except Exception as e:
                    
                    logger.info("300====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====CRCD====")
                    logger.error(f"Error reading with Glue dynamic frame: {str(e)}")
                    
                    # Fallback to direct boto3 reading
                    s3_path = args['source_path']
                    path_parts = s3_path.replace('s3://', '').split('/', 1)
                    bucket = path_parts[0]
                    key = path_parts[1] if len(path_parts) > 1 else ''
                    
                    s3 = boto3.client('s3')
                    response = s3.get_object(Bucket=bucket, Key=key)
                    content = response['Body'].read()
                    
                    # Decompress if gzipped
                    if key.endswith('.gz'):
                        content = gzip.decompress(content)
                    
                    # Parse JSON
                    json_content = json.loads(content.decode('utf-8'))
                    
                    # Create a DataFrame from the JSON
                    json_rdd = sc.parallelize([json.dumps(json_content)])
                    df = spark.read.json(json_rdd)
                
                # Convert DataFrame back to Python dict for processing
                json_content = json.loads(df.toJSON().collect()[0])
                
                processed_count = 0
                total_items = 0
                processed_items = []
                
                # Check if configurationItems exists and is not empty
                if 'configurationItems' not in json_content:
                    raise ValueError("JSON file does not contain 'configurationItems' field")
                
                if not json_content['configurationItems']:
                    raise ValueError("'configurationItems' array is empty")
                
                # Count and log total items
                total_items = len(json_content['configurationItems'])
                logger.info(f"Found {total_items} items in configurationItems array")
                
                # Get the common fields that need to be preserved
                output_template = {
                    "fileVersion": json_content.get("fileVersion", "1.0"),
                    "configSnapshotId": json_content.get("configSnapshotId", ""),
                    "configurationItems": []
                }
                
                # Process each configuration item
                for index, item in enumerate(json_content['configurationItems'], 1):
                    try:
                        # TODO test if it works without this
                        # Convert configuration field to string if it's an object
                        #if 'configuration' in item and isinstance(item['configuration'], dict):
                        #    item['configuration'] = json.dumps(item['configuration'])

                        # Create a new output object for this item
                        output_json = output_template.copy()
                        output_json['configurationItems'] = [item]
                        
                        resource_type = sanitize_s3_name(item.get('resourceType', '').replace('::', '-'))
                        resource_name = sanitize_s3_name(item.get('resourceId', ''))
                        
                        run_id = args['CRCD_JOB_RUN_ID']
                        
                        # Track the item being processed
                        item_identifier = f"{resource_type}_{resource_name}"
                        processed_items.append(item_identifier)

                        # Want to be sure every file name is different and also relates to its content and origin
                        random_part = ''.join(random.choices(string.ascii_letters + string.digits, k=15))
                                        
                        filename_original = f"{random_part}_{resource_type}_{resource_name}_{run_id}.json.gz"
                        # filename must not be too long for S3 - Object key names may be up to 1024 characters long
                        filename = truncate_s3_key(filename_original)
                    
                        destination = get_destination_path(
                            args['source_path'],
                            args['destination_path'],
                            filename
                        )
                        
                        logger.info(f"Processing item {index}/{total_items}: {filename}")
                        
                        dest_parts = destination.replace('s3://', '').split('/', 1)
                        dest_bucket = dest_parts[0]
                        dest_key = dest_parts[1]
                        
                        # Convert JSON to bytes and compress
                        # AWS Config JSON is all on a single line without indentation, 
                        # while a problematic JSON is formatted with indentation and newlines. 
                        # Athena's JSON SerDe expects each line to be a complete JSON object.
                        # remove the indent=2 parameter
                        # json_bytes = json.dumps(output_json, indent=2).encode('utf-8')
                        json_bytes = json.dumps(output_json).encode('utf-8')
                        
                        # Create a BytesIO object to hold the gzipped data
                        gzip_buffer = io.BytesIO()
                        
                        # Create a GzipFile object and write the JSON data
                        with gzip.GzipFile(mode='wb', fileobj=gzip_buffer) as gz:
                            gz.write(json_bytes)
                        
                        # Get the gzipped content
                        gzip_buffer.seek(0)
                        gzipped_content = gzip_buffer.getvalue()
                        
                        s3 = boto3.client('s3')
                        s3.put_object(
                            Bucket=dest_bucket,
                            Key=dest_key,
                            Body=gzipped_content,
                            ContentType='application/json',
                            ContentEncoding='gzip'
                        )
                        processed_count += 1
                        logger.info(f"Successfully saved {filename} to {destination}")
                        
                    except Exception as item_error:
                        logger.error(f"Error processing item {index}/{total_items}: {str(item_error)}")
                        logger.error(f"Problematic item: {json.dumps(item)}")
                        continue  # Continue with next item instead of failing the whole job
                
                # Log summary
                logger.info(f"\nProcessing Summary:")
                logger.info(f"Total items found: {total_items}")
                logger.info(f"Items processed successfully: {processed_count}")
                logger.info(f"Items missed: {total_items - processed_count}")
                
                if total_items != processed_count:
                    logger.info("\nPossible issues:")
                    # Check for duplicates in processed items
                    item_counts = Counter(processed_items)
                    duplicates = {item: count for item, count in item_counts.items() if count > 1}
                    if duplicates:
                        logger.info("\nFound duplicate resource names (might have overwritten files):")
                        for item, count in duplicates.items():
                            logger.info(f"  {item}: {count} occurrences")
                    
                update_job_status('COMPLETED', processed_count)
                return processed_count
                
            except ValueError as ve:
                # Handle specific validation errors
                error_message = str(ve)
                print(f"Validation error: {error_message}")
                update_job_status('FAILED', 0, error_message)
                raise
            except Exception as e:
                # Handle other unexpected errors
                error_message = f"Unexpected error: {str(e)}"
          
                print(error_message)
                update_job_status('FAILED', 0, error_message)
                raise
        try:
            items_processed = process_file()
            print(f"Successfully processed {items_processed} items")
            job.commit()
        except Exception as e:
            print(f"Error processing file: {str(e)}")
            raise

  PreProcessingGlueJobRole:
    Type: AWS::IAM::Role
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      Description: "CRCD Dashboard - ETL job on AWS Config files"
      RoleName: "crcd-preprocessing-glue-job-role"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      Policies:
        - PolicyName: crcd-dynamodb-dashboard-bucket-policy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:ListBucket
                Resource:
                  # Log Archive bucket is passed as name, Dashboard bucket created here
                  - !Sub "${DashboardBucket.Arn}/*"
                  - !GetAtt DashboardBucket.Arn
                  - !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}"
                  - !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}/*"
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                  - dynamodb:UpdateItem
                  - dynamodb:GetItem
                Resource: !GetAtt CRCDJobTrackingTable.Arn

  # Glue Job
  # TODO check more suggested --conf settings to add to DefaultArguments
  # TODO review the settings on the account. If I use this it means I have big files and I need power: G4x instances, autoscaling, ...
  # --conf spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
  # --conf spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs=true
  # --conf spark.hadoop.mapreduce.task.timeout=300000
  # --conf spark.task.maxFailures=10
  # --conf spark.network.timeout=800s
  ConfigFilePreprocessingGlueJob:
    Type: AWS::Glue::Job
    DependsOn: GlueScript
    Properties:
      Name: !Sub "${PreProcessingGlueJobName}"  #crcd-config-file-processing-job moved to technical parameter
      Description: "CRCD Dashboard - Glue job that transforms AWS Config files and generates a file for each AWS Config item and saves it onto the destination bucket"
      Role: !GetAtt PreProcessingGlueJobRole.Arn
      Command:
        Name: glueetl
        PythonVersion: '3'
        # TODO this fails if the bucket name is  a prefix
        ScriptLocation: !Sub 's3://${DashboardBucket}/crcd-scripts/${PreProcessingGlueJobName}-DO-NOT-DELETE.py'
      DefaultArguments:
        '--enable-metrics': 'true'
        '--enable-continuous-cloudwatch-log': 'true'
        '--job-language': 'python'
        '--enable-spark-ui': 'true'
        '--enable-job-insights': 'true'
        '--enable-glue-datacatalog': 'true'
        '--conf': 'spark.driver.maxResultSize=2g --conf spark.kryoserializer.buffer.max=512m'
      ExecutionProperty:
        MaxConcurrentRuns: 10
      GlueVersion: '4.0'
      MaxRetries: 0
      Timeout: 2880
      NumberOfWorkers: 5
      WorkerType: G.2X

Outputs:
  DashboardBucketName:
    Description: Use this bucket as Log Archive bucked when deploying the AWS Config Resource Compliance dashboard
    Value: !Ref DashboardBucket

  DynamoDBTableName:
    Description: Name of the DynamoDB tracking table
    Value: !Ref CRCDJobTrackingTable

  GlueJobName:
    Description: Name of the preprocessing Glue Job
    Value: !Ref ConfigFilePreprocessingGlueJob

  ConfigFilePreprocessingProducerLambdaArn:
    Description: ARN of the Lambda function that triggers the preprocessing Glue Job
    Value: !GetAtt ConfigFilePreprocessingProducerLambda.Arn
