AWSTemplateFormatVersion: '2010-09-09'
Description: "Deployment of AWS Config Resource Compliance Dashboard (CRCD)"

# Data pipeline installs dashboard resources until an Athena table. Athena views and QuickSight resources are not installed by this template
#
# The following use cases are supported
# 1. deployment on Log Archive account (all in one account)
#    In this case the AWS Config bucket is the source of logs, the Lambda is triggered from this bucket directly
#
# 2. deployment on Dashboard account 
#    AWS Config logs are delivered to the Log Archive account, then replicated to the Dashboard account where all the resources related to the dashboard are deployed
#
#    2.1 - always start by running this template on the Dashboard account
#    Create the S3 bucket for the copy of the logs
#    Add permissions to the bucket to allow replication from the Log Archive Account
#    Trigger the lambda function from the bucket created in the Dashboard account
#    If the Log Archive bucket is encrypted with a KMS key, then the Dashboard bucket will also be encrypted
#
#    2.2
#    Run the Log Archive account to setup replication to the bucket created on 2.1
#
# 3. Deployment on a single account
#    This is exactly the same as use case 1

# The template requires these parameters, which must always be provided by users:
# A: LogArchiveAccountId  X: LogArchiveBucketName
# B: DashboardAccountId   Y: DashboardBucketName
#
# This is how this template determines what to install
#    use case 1 if A=B and X=Y
#    use case 2.1 if A!=B and X!=Y and current account is B
#    use case 2.2 if A!=B and X!=Y and current account is A 
#    use case 3 if A=B and X=Y

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      -
        Label:
          default: "AWS Config logging - where your AWS Config files are delivered"
        Parameters:
          - LogArchiveAccountId
          - LogArchiveBucketName
          - LogArchiveBucketKmsKeyArn
      -
        Label:
          default: "Dashboard resources - where you deploy the dashboard and where its data is" 
        Parameters:
          - DashboardAccountId
          - DashboardBucketName # TODO can this be a prefix, with a value we decide and then append accountID and Region?
          - DashboardBucketKmsKeyArn
          - ConfigureS3EventNotificationToLambda
          - ConfigureS3Replication
      -
        Label:
          default: "Technical Parameters (DO NOT CHANGE)"
        Parameters:
          - AthenaWorkgroupName
          - AthenaDatabaseName
          - AthenaTableName
          - AthenaQueryResultBucketPrefix
          - LambdaPartitioningFunctionName
          - GlueDataCatalogName
          - CrossAccountReplicationRole

    ParameterLabels:
      AthenaWorkgroupName:
        default: "Athena workgroup"
      AthenaDatabaseName:
        default: "Athena database"
      AthenaQueryResultBucketPrefix:
        default: "Prefix of the Athena query results bucket"
      AthenaTableName:
        default: "Athena table for AWS Config data"
      LogArchiveBucketName:
        default: "Log Archive bucket"
      GlueDataCatalogName:
        default: "Existing Glue Data Catalog"
      LambdaPartitioningFunctionName:
        default: "AWS Lambda function that partitions AWS Config files"
      LogArchiveAccountId: 
        default: "Log Archive account ID"
      DashboardBucketName:
        default: "Dashboard bucket"
      LogArchiveBucketKmsKeyArn:
        default: "ARN of the KMS key that encrypts the Log Archive bucket"
      DashboardBucketKmsKeyArn:
        default: "ARN of the KMS key that encrypts the Dashboard bucket"
      DashboardAccountId:
        default: "Dashboard account ID"
      ConfigureS3EventNotificationToLambda:
        default: "Configure S3 event notification to trigger the Lambda Partitioner function on every new AWS Config file" 
      ConfigureS3Replication:
        default: "Configure cross-account replication of AWS Config files from Log Archive to Dashboard account"
      CrossAccountReplicationRole:
        default: "Name of the role used in cross-account replication of AWS Config files"

Parameters:

  LogArchiveBucketName:
    Type: "String"
    Description: "Name of the Amazon S3 bucket collecting AWS Config files (Required)."
    MinLength: 1
    AllowedPattern: '^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$'
    ConstraintDescription: "Log Archive bucket name is missing or does not satisfy the Amazon S3 naming convention."

  LogArchiveAccountId:
    Type: String
    Description: "The number of the AWS account that contains the Amazon S3 bucket collecting AWS Config files (Required)."
    AllowedPattern: '\d{12}'
    MinLength: 12
    ConstraintDescription: "Log Archive account ID is missing or incorrect: AWS account IDs must be a 12-digit number."

  # 1 quicksight role gets permission to use the key for decryption
  # 2.1 the KMS key is needed so that the dashboard bucket will be created KMS encrypted
  # 2.2 is needed so that replication role gets permissions to use the key
  # permissions are to Quicksight in case of installation in log archive bucket (step 1)
  # permissions are to the replication role otherwise, and here there is no need to do that manually
  LogArchiveBucketKmsKeyArn:
    Type: "String"
    Description: "If you encrypt the Log Archive bucket with a KMS key, copy the key's ARN here. If you deploy the dashboard on the Log Archive account, QuickSight will get permissions to use the key for decrypt operations. Leave empty if the bucket is not encrypted with KMS, or if you want to grant key permissions manually. If you deploy the dashboard on a dedicated Dashboard account, you must provide the key's ARN in both steps."
    AllowedPattern: '^$|^arn:(aws|aws-us-gov|aws-cn):kms:\w+(?:-\w+)+:\d{12}:key\/[A-Za-z0-9]+(?:-[A-Za-z0-9]+)+$'
    ConstraintDescription: "The KMS key ARN does not satisfy the conventions for a KMS Amazon Resource Name [arn:<aws_partition>:kms:<aws_region>:<account_id>:key/<key_id>]."

  DashboardAccountId:
    Type: "String"
    Description: "The number of the AWS account that will contain the dashboard's data pipeline resources and the Amazon S3 bucket that is used as source of data for the dashboard. Insert the Log Archive account ID if you want to deploy the dashboard in the same account where AWS Config files are collected. To deploy the dashboard on a dedicated account, insert another existing AWS account ID (Required)."
    AllowedPattern: '\d{12}'
    MinLength: 12
    ConstraintDescription: "Dashboard account ID is missing or incorrect: AWS account IDs must be a 12-digit number."

  DashboardBucketName:
    Type: "String"
    Description: "Name of the Amazon S3 bucket that is used as source of data for the dashboard. If you deploy the dashboard on the Log Archive account, insert the name of the Log Archive bucket again. If you deploy the dashboard on a dedicated Dashboard account, the Dashboard bucket will be created with the name you specify here (Required)."
    AllowedPattern: '^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$'
    MinLength: 1
    ConstraintDescription: "Dashboard bucket name is missing or does not satisfy the Amazon S3 naming convention."

  DashboardBucketKmsKeyArn:
    Type: "String"
    Description: "This parameter is used only in the second step of the Dashboard account deployment. In this case, insert the ARN of the KMS key created in the first step (it's DashboardBucketKmsKeyArn on the CloudFormation Outputs). If you deploy the dashboard on the Log Archive account, or if you are at the first step of the Dashboard account deployment, this parameter is ignored."
    AllowedPattern: '^$|^arn:(aws|aws-us-gov|aws-cn):kms:\w+(?:-\w+)+:\d{12}:key\/[A-Za-z0-9]+(?:-[A-Za-z0-9]+)+$'
    ConstraintDescription: "The KMS key ARN does not satisfy the conventions for a KMS Amazon Resource Name [arn:<aws_partition>:kms:<aws_region>:<account_id>:key/<key_id>]."

  ConfigureS3EventNotificationToLambda:
    Type: "String"
    Description:  "Select 'yes' to configure event notifications that will trigger the data pipeline of the dashboard whenever there is a new AWS Config file added to the bucket. Select 'no' if you already have S3 event notifications configured on the Dashboard bucket. In this case, you must configure this manually."
    AllowedValues: ['<select>', 'yes', 'no']
    Default: "<select>" # This value is used below, careful if you change it

  ConfigureS3Replication:
    Type: "String"
    Description: "Select 'yes' to configure object replication between the Log Archive bucket and the Dashboard bucket. Select 'no' if you already have an S3 replication configurations on the Log Archive bucket. In this case, you must configure this manually."
    AllowedValues: ['<select>', 'yes', 'no']
    Default: "<select>" # This value is used below, careful if you change it
  
  AthenaDatabaseName:
    Type: "String"
    Default: "cid_crcd_database"
    Description: "The Athena/Glue database for the dashboard (Required)."
    MinLength: 1
    # The name for an Athena database
    # Max 255 characters cannot have the symbol '-' and must have lowercase character, '_' is accepted
    # This value is not meant to be changed by the user, but we'll add the allowed pattern anyway
    AllowedPattern: '^[a-z0-9][a-z0-9_]{0,253}[a-z0-9]$'
    ConstraintDescription: "Required: Athena database"

  AthenaQueryResultBucketPrefix:
    Type: "String"
    Default: "crcd-athena-query-results"
    Description: "The Athena query result bucket for the workgroup, account ID and region will be added to the name by this template (Required)."
    # 64 characters in the bucket name, but automatically the template will add 2 dashes, 12 digit account number and region up to 14 characters ap-southeast-4, ap-northeast-1
    # that leaves 36 character for the prefix
    AllowedPattern: '^[a-z0-9][a-z0-9-]{1,33}[a-z0-9]$'
    MinLength: 1
    ConstraintDescription: "Required: Prefix of the Athena query results bucket"

  AthenaWorkgroupName:
    Type: "String"
    Default: "crcd-dashboard"
    Description: "The Athena workgroup for the dashboard (Required)."
    MinLength: 1
    ConstraintDescription: "Required: Athena workgroup"

  AthenaTableName:
    Type: "String"
    Default: "cid_crcd_config"
    Description: "The name that will be assigned to the Athena table that contains the AWS Config data for the dashboard (Required)."
    MinLength: 1
    ConstraintDescription: "Required: Athena table for AWS Config data."

  GlueDataCatalogName:
    Type: "String"
    Default: "AwsDataCatalog"
    Description: "Name of the Glue Data Catalog (Required)."
    MinLength: 1
    ConstraintDescription: "Required: Glue Data Catalog."

  LambdaPartitioningFunctionName:
    Type: "String"
    Default: "crcd-config-file-partitioner"
    MinLength: 1
    ConstraintDescription: "Required: AWS Lambda function that partitions AWS Config files."
    Description: "Name of the AWS Lambda function that partitions AWS Config files (Required)."
  
  CrossAccountReplicationRole: 
    Type: "String"
    # cannot concatenate with source bucket name as there is a limit on 64 characters for role names
    # will use a fixed string
    Default: "crcd-s3replication-role-for-config-files" 
    MinLength: 1
    ConstraintDescription: "Required: Name of the role used in cross-account replication of AWS Config files."
    Description: "Name of the S3 service role used in the AWS Config files replication from the Log Archive bucket to the Data Collection bucket (Required)."


Conditions:
  # used only in case 2.2, when setting up replication on the log archive bucket
  IsDashboardBucketKms: !Not [ !Equals [ !Ref DashboardBucketKmsKeyArn, "" ] ]

  # Additional resources need to be created if the Log Archive bucket is encrypted with KMS
  IsLogArchiveBucketKms: !Not [ !Equals [ !Ref LogArchiveBucketKmsKeyArn, "" ] ]

  IsLogArchiveAccountDeployment:
    # use case 1
    # Full deployment on the Log Archive account
    Fn::And:
      - !Equals [!Ref LogArchiveAccountId, !Ref 'AWS::AccountId']
      - !Equals [!Ref LogArchiveAccountId, !Ref DashboardAccountId]
      - !Equals [!Ref LogArchiveBucketName, !Ref DashboardBucketName]
  
  IsLogArchiveAccountDeploymentKms:
    # use case 1 with KMS encrypted bucket
    Fn::And: 
      - !Condition IsLogArchiveAccountDeployment
      - !Condition IsLogArchiveBucketKms

  IsDashboardAccountDeployment:
    # use case 2.1
    Fn::And:
      - !Not [!Condition IsLogArchiveAccountDeployment]
      - !Equals [!Ref DashboardAccountId, !Ref 'AWS::AccountId']
      - !Not [!Equals [!Ref LogArchiveAccountId, !Ref DashboardAccountId]]
      - !Not [!Equals [!Ref LogArchiveBucketName, !Ref DashboardBucketName]]
  
  IsDashboardAccountDeploymentKms:
    # use case 2.1 with KMS encrypted buckets
    # TODO add IsDashboardBucketKms? but then it would not cover 2.1
    Fn::And:
      - !Condition IsDashboardAccountDeployment
      - !Condition IsLogArchiveBucketKms

  IsS3ReplicationOnly:
    # use case 2.2
    # in the Log Archive installation there is a second cloudformation run where we configure only the S3 object replication
    Fn::And:
      - !Not [!Condition IsLogArchiveAccountDeployment]
      - !Not [!Condition IsDashboardAccountDeployment]
      - !Equals [!Ref LogArchiveAccountId, !Ref 'AWS::AccountId']
      - !Not [!Equals [!Ref LogArchiveAccountId, !Ref DashboardAccountId]]
      - !Not [!Equals [!Ref LogArchiveBucketName, !Ref DashboardBucketName]]

  IsS3ReplicationKms:
    # replication in case buckets are encrypted with a KMS key
    Fn::And:
      - !Condition IsDashboardBucketKms
      - !Condition IsLogArchiveBucketKms
      - !Condition IsS3ReplicationOnly

  IsConfigureS3EventNotificationToLambda:
    # some customers that install on the Log Archive account may already have S3 notifications
    # we must not override that - and users will have to do that manually later
    # case 1: user must select yes from the parameters
    # case 2.1: this is always true, since we create our own bucket in the dashboard account
    # case 2.2: NEVER
    Fn::Or:
      - !Condition IsDashboardAccountDeployment
      - Fn::And:
        - !Equals [!Ref ConfigureS3EventNotificationToLambda, 'yes']
        - !Condition IsLogArchiveAccountDeployment  

  IsConfigureS3EventNotificationToLambdaLogArchiveAccount:
    # resources created in use case 1 with the lambda trigger
    Fn::And:
      - !Equals [!Ref ConfigureS3EventNotificationToLambda, 'yes']
      - !Condition IsLogArchiveAccountDeployment

  IsCreateDashboardResources:
    # resources that must be created in use case 1 or 2.1
    # e.g. the lambda function, athena resources, glue table, ...
    Fn::Or:
      - !Condition IsLogArchiveAccountDeployment
      - !Condition IsDashboardAccountDeployment

  IsConfigureS3Replication:
    # in step 2.2 users can bypass the creation of the S3 replication
    Fn::And:
      - !Condition IsS3ReplicationOnly
      - !Equals [!Ref ConfigureS3Replication, 'yes']

Rules:
  MandatoryLogArchiveAccountId:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref LogArchiveAccountId
          - ''
        AssertDescription: "Log Archive account ID is required"

  MandatoryLogArchiveBucketName:
    Assertions:
      - Assert: !Not
         - !Equals
          - !Ref LogArchiveBucketName
          - ''
        AssertDescription: "Log Archive bucket name is required"

  MandatoryDashboardAccountId:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref DashboardAccountId
          - ''
        AssertDescription: "Dashboard account ID is required"

  MandatoryDashboardBucketName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref DashboardBucketName
          - ''
        AssertDescription: "Dashboard bucket name is required"


  MandatoryAthenaDatabase:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref AthenaDatabaseName
          - ''
        AssertDescription: "Athena database is required"
  
  MandatoryAthenaQueryResultBucketPrefix:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref AthenaQueryResultBucketPrefix
          - ''
        AssertDescription: "Athena query result bucket prefix is required"
  
  MandatoryAthenaWorkgroup:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref AthenaWorkgroupName
          - ''
        AssertDescription: "Athena workgroup is required"
  
  MandatoryAthenaTable:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref AthenaTableName
          - ''
        AssertDescription: "Athena table name is required"
  
  MandatoryGlueDataCatalog:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref GlueDataCatalogName
          - ''
        AssertDescription: "Glue Data Catalog name is required"

  MandatoryLambdaPartitioningFunctionName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref LambdaPartitioningFunctionName
          - ''
        AssertDescription: "Lambda function name is required"
  
  MandatoryCrossAccountReplicationRole:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref CrossAccountReplicationRole
          - ''
        AssertDescription: "Cross-account replication role name is required"

  ValidateConfigureS3EventNotificationToLambda:
    # must be selected in case 1 (and 3)
    # for use case 2.1 this is TRUE by default, we are going to create our own bucket with the event notification
    # The field is not considered in other use cases
    RuleCondition: 
      # Cannot use !Condition IsCreateDashboardResources, must replicate the conditions 1
      Fn::And:
        - !Equals [!Ref LogArchiveAccountId, !Ref 'AWS::AccountId']
        - !Equals [!Ref LogArchiveAccountId, !Ref DashboardAccountId]
        - !Equals [!Ref LogArchiveBucketName, !Ref DashboardBucketName]
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref ConfigureS3EventNotificationToLambda
          - '<select>'
        AssertDescription: "REQUIRED PARAMETER - Select 'yes' if you want this template to configure the S3 event notification that triggers the Lambda Partitioner function when new AWS Config files are received. Select 'no' if you already have S3 event notifications on the Log Archive bucket and want to configure this manually."

  ValidateConfigureS3Replication:
    # Replication must be selected on use case 2.2
    # The field is not considered in other use cases
    RuleCondition:
      # Cannot use !Condition IsS3ReplicationOnly, must replicate the conditions
      Fn::And:
        - !Equals [!Ref LogArchiveAccountId, !Ref 'AWS::AccountId']
        - !Not [!Equals [!Ref LogArchiveAccountId, !Ref DashboardAccountId]]
        - !Not [!Equals [!Ref LogArchiveBucketName, !Ref DashboardBucketName]]
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref ConfigureS3Replication
          - '<select>'
        AssertDescription: "REQUIRED PARAMETER - Select 'yes' if you want this template to configure S3 replication of AWS Config files from the Log Archive bucket to the Dashboard bucket. Select 'no' if you already have S3 object replication configured on the Log Archive bucket and want to configure this manually."

  # Kms key ARN validation
  ValidateDashboardBucketKmsKeyArn:
    # Must be given only at step 2 of the dashboard bucket installation - case 2.2
    # If there is a KMS key specified on the Loig Archive bucket
    # In this case the KMS key ARNs of the buckets are either both specified on not entered.
    RuleCondition:
      Fn::And:
        # These 3 replicate the condition for !Condition IsS3ReplicationOnly
        - !Equals [!Ref LogArchiveAccountId, !Ref 'AWS::AccountId']
        - !Not [!Equals [!Ref LogArchiveAccountId, !Ref DashboardAccountId]]
        - !Not [!Equals [!Ref LogArchiveBucketName, !Ref DashboardBucketName]]
        # a KMS key for the Dashboard bucket was given
        - !Not [ !Equals [ !Ref DashboardBucketKmsKeyArn, "" ] ]
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref LogArchiveBucketKmsKeyArn
          - ''
        AssertDescription: "REQUIRED PARAMETER - cannot replicate objects between buckets with different types of encryption. ARN of the KMS key that encrypts the Log Archive bucket must be provided together with the ARN of the KMS key that encrypts the Dashboard bucket."


Resources:

  # ***********************************************************************************************************************************************
  # ***********************************************************************************************************************************************
  # Core CRCD resources to be installed on use case 1 and 2.1
  # ***********************************************************************************************************************************************
  # ***********************************************************************************************************************************************
  GlueDatabase:
    Type: AWS::Glue::Database
    Condition: IsCreateDashboardResources
    Properties:
      DatabaseInput:
        Description: "Database for AWS Config Resource Compliance Dashboard (CRCD)"
        Name: !Sub "${AthenaDatabaseName}"
      # The AWS account ID for the account in which to create the catalog object.
      CatalogId: !Sub '${AWS::AccountId}'

  AthenaQueryResultBucket:
  # checkov:skip=CKV_AWS_21: We accept this bucket has no versioning
  # checkov:skip=CKV_AWS_18: We accept this bucket has no access logging
    Type: AWS::S3::Bucket
    Condition: IsCreateDashboardResources
    Properties:
      BucketName: !Sub '${AthenaQueryResultBucketPrefix}-${AWS::AccountId}-${AWS::Region}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      AccessControl: BucketOwnerFullControl
      OwnershipControls:
        Rules:
          - ObjectOwnership: BucketOwnerEnforced
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LifecycleConfiguration:
        Rules:
          - Id: DeleteContent
            Status: 'Enabled'
            ExpirationInDays: 7
    Metadata:
      cfn-lint:
        config:
          ignore_checks:
            - W3045 # Consider using AWS::S3::BucketPolicy instead of AccessControl; standard Athena results setup
      cfn_nag:
        rules_to_suppress:
          - id: W35
            reason: "We accept Athena query result bucket has no access logging"
          - id: W51
            reason: "We accept Athena query result bucket has no bucket policy"

  # Athena workgroup to execute CRCD queries with its own result bucket
  # Using RecursiveDeleteOption - The option to delete a workgroup and its contents even if the workgroup contains any named queries. The default is false.
  AthenaWorkgroup:
    Type: AWS::Athena::WorkGroup
    Condition: IsCreateDashboardResources
    Properties:
      Name: !Sub '${AthenaWorkgroupName}'
      Description: 'Used by AWS Config Resource Compliance Dashboard (CRCD)'
      RecursiveDeleteOption: True
      WorkGroupConfiguration:
        EnforceWorkGroupConfiguration: true
        ResultConfiguration:
          EncryptionConfiguration:
            EncryptionOption: SSE_S3
          OutputLocation: !Sub 's3://${AthenaQueryResultBucket}/'

  IAMRoleQuickSightDataSource:
    Type: AWS::IAM::Role
    Condition: IsCreateDashboardResources
    UpdateReplacePolicy: "Delete"
    DeletionPolicy: "Delete"
    Properties:
      Description: "CRCD Dashboard - Allows QuickSight datasource access to Athena/Glue and the S3 bucket that contains AWS Config files"
      Path: "/"
      ManagedPolicyArns:
      - Ref: "IAMManagedPolicyQuickSightDataSource"
      MaxSessionDuration: 3600
      RoleName: "crcd-quicksight-datasource-role"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Action: "sts:AssumeRole"
          Effect: "Allow"
          Principal:
            Service: "quicksight.amazonaws.com"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Role names are as per our design"

  # Must create a KMS key in this case
  # Will encrypt the Dashboard bucket, and QuickSight will be allowed to use it
  DashboardBucketKmsKey:
    Type: AWS::KMS::Key
    UpdateReplacePolicy: "Delete"
    DeletionPolicy: "Delete"
    Condition: IsDashboardAccountDeploymentKms
    Properties:
      Description: "CRCD Dashboard: Key that encrypts the Dashboard bucket in case of deployment on dedicated Dashboard account"
      EnableKeyRotation: true
      KeyPolicy:
        Version: "2012-10-17"
        Id: crcd-dashboardkey-policy
        Statement:
          - Sid: 'Enable IAM user permissions'
            Effect: Allow
            Principal:
              AWS: !Sub "arn:${AWS::Partition}:iam::${DashboardAccountId}:root"
            Action: 'kms:*'
            Resource: '*'
          # Allow the account on the other side (Log Archive) to use this for replication
          # It would be better to allow only the replication role, but it does not exist yet!
          - Sid: 'Allow Log Archive account use the key for replication'
            Effect: Allow
            Principal:
              AWS: !Sub "arn:${AWS::Partition}:iam::${LogArchiveAccountId}:root"
            Action:
              - kms:GenerateDataKey
              - kms:Encrypt
            Resource: '*'

  DashboardBucketKmsKeyAlias:
    Type: AWS::KMS::Alias
    Condition: IsDashboardAccountDeploymentKms
    Properties:
      AliasName: alias/crcd-key-dashboard
      TargetKeyId: !Ref DashboardBucketKmsKey

  IAMManagedPolicyQuickSightDataSource:
    Type: AWS::IAM::ManagedPolicy
    Condition: IsCreateDashboardResources
    UpdateReplacePolicy: "Delete"
    DeletionPolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-quicksight-datasource-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that allows QuickSight to access Glue, Athena and the S3 bucket source of AWS Config files"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Sid: "ReadAthenaLakeFormation"
            Action:
              - "lakeformation:GetDataAccess"
              - "athena:ListDataCatalogs"
              - "athena:ListDatabases"
              - "athena:ListTableMetadata"
            Effect: "Allow"
            Resource: "*"
              # required https://docs.aws.amazon.com/lake-formation/latest/dg/access-control-underlying-data.html
              # Cannot restrict this. See https://docs.aws.amazon.com/athena/latest/ug/datacatalogs-example-policies.html#datacatalog-policy-listing-data-catalogs
          - Sid: "AccessGlueData"
            Action:
              - "glue:GetPartition"
              - "glue:GetPartitions"
              - "glue:GetDatabase"
              - "glue:GetDatabases"
              - "glue:GetTable"
              - "glue:GetTables"
            Effect: "Allow"
            Resource:
              - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:catalog"
              - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database/${AthenaDatabaseName}"
              - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:table/${AthenaDatabaseName}/*"
          - Sid: "AccessAthenaDatabaseWorkgroup"
            Action:
              - "athena:ListDatabases"
              - "athena:ListDataCatalogs"
              - "athena:GetQueryExecution"
              - "athena:GetQueryResults"
              - "athena:StartQueryExecution"
              - "athena:GetQueryResultsStream"
              - "athena:ListTableMetadata"
              - "athena:GetTableMetadata"
            Effect: "Allow"
            Resource:
              - !Sub "arn:${AWS::Partition}:athena:${AWS::Region}:${AWS::AccountId}:datacatalog/${GlueDataCatalogName}"
              - !Sub "arn:${AWS::Partition}:athena:${AWS::Region}:${AWS::AccountId}:workgroup/${AthenaWorkgroupName}"
          - Sid: "AllowReadAndWriteToAthenaQueryResultBucket"
            Action:
              - "s3:GetBucketLocation"
              - "s3:ListBucket"
              - "s3:GetObject"
              - "s3:PutObject"
              - "s3:ListMultipartUploadParts"
              - "s3:ListBucketMultipartUploads"
              - "s3:AbortMultipartUpload"
            Effect: "Allow"
            Resource:
              - !Sub "arn:${AWS::Partition}:s3:::${AthenaQueryResultBucket}"
              - !Sub "arn:${AWS::Partition}:s3:::${AthenaQueryResultBucket}/*"
          - Sid: "AllowListTheS3ConfigBucket"
            Resource:
              - !Sub "arn:${AWS::Partition}:s3:::${DashboardBucketName}"
            Action:
              - "s3:ListBucket"
            Effect: "Allow"
          - Sid: "AllowReadTheS3ConfigBucket"
            Resource:
              - !Sub "arn:${AWS::Partition}:s3:::${DashboardBucketName}/*"
            Action:
              - "s3:GetObject"
              - "s3:GetObjectVersion"
            Effect: "Allow"
          # Gives permissions to quicksight in case of installation with a KMS encrypted bucket
          - !If
            - IsLogArchiveAccountDeploymentKms
            - Sid: "AllowKmsDecryptLogArchiveBucket"
              Effect: Allow
              Action:
                - 'kms:Decrypt'
              Resource: !Ref LogArchiveBucketKmsKeyArn
            - !Ref "AWS::NoValue"
          - !If
            - IsDashboardAccountDeploymentKms
            - Sid: "AllowKmsDecryptDashboardBucket"
              Effect: Allow
              Action:
                - 'kms:Decrypt'
              Resource: !GetAtt DashboardBucketKmsKey.Arn
            - !Ref "AWS::NoValue"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W13
            reason: "Need to use * for Lakeformation and Athena"
          - id: W28
            reason: "Policy names are as per our design"

  IAMManagedPolicyLambdaGlue:
    Type: AWS::IAM::ManagedPolicy
    Condition: IsCreateDashboardResources
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-glue-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that gives Glue permissions to CRCD Lambda execution role"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource:
          - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:table/${AthenaDatabaseName}/*"
          - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database/${AthenaDatabaseName}"
          - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:catalog"
          Action:
          - "glue:UpdatePartition"
          - "glue:GetTables"
          - "glue:GetTable"
          - "glue:GetPartitions"
          - "glue:GetPartition"
          - "glue:DeletePartition"
          - "glue:CreatePartition"
          - "glue:BatchGetPartition"
          - "glue:BatchDeletePartition"
          - "glue:BatchCreatePartition"
          - "glue:GetDatabase"
          Effect: "Allow"
          Sid: "GluePartitions"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  IAMManagedPolicyLambdaS3AthenaQueryResults:
    Type: AWS::IAM::ManagedPolicy
    Condition: IsCreateDashboardResources
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-s3-athenaqueryresults-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that gives permissions on Athena query results S3 bucket to CRCD Lambda execution role"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource:
          - !Sub "arn:${AWS::Partition}:s3:::${AthenaQueryResultBucket}"
          - !Sub "arn:${AWS::Partition}:s3:::${AthenaQueryResultBucket}/*"
          Action:
          - "s3:PutObject"
          - "s3:ListMultipartUploadParts"
          - "s3:ListBucket"
          - "s3:GetObject"
          - "s3:GetBucketLocation"
          Effect: "Allow"
          Sid: "S3AthenaQueryResults"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  IAMManagedPolicyLambdaS3ConfigObject:
    Type: AWS::IAM::ManagedPolicy
    Condition: IsCreateDashboardResources
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-s3-configfile-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that allows CRCD Lambda to receive objects from the Config S3 bucket"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource:
          - !Sub "arn:${AWS::Partition}:s3:::${DashboardBucketName}"
          - !Sub "arn:${AWS::Partition}:s3:::${DashboardBucketName}/*"
          Action:
          - "s3:GetObject"
          - "s3:ListBucket"
          - "s3:ListBucketVersions"
          - "s3:GetObjectVersion"
          - "s3:GetLifecycleConfiguration"
          Effect: "Allow"
          Sid: "S3ConfigFileObject"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  IAMManagedPolicyLambdaCloudWatchLogs:
    Type: AWS::IAM::ManagedPolicy
    Condition: IsCreateDashboardResources
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-cloudwatch-logs-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that gives CloudWatch Logs permissions to CRCD Lambda execution role"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        # keep the log group name the same as the lambda function
        - Resource: !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/${LambdaPartitioningFunctionName}:*"
          Action:
          - "logs:PutLogEvents"
          - "logs:CreateLogStream"
          - "logs:CreateLogGroup"
          Effect: "Allow"
          Sid: "CloudWatchLogGroup"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  IAMManagedPolicyLambdaAthena:
    Type: AWS::IAM::ManagedPolicy
    Condition: IsCreateDashboardResources
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-athena-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that gives Athena permissions to CRCD Lambda execution role"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource: !Sub "arn:${AWS::Partition}:athena:${AWS::Region}:${AWS::AccountId}:workgroup/${AthenaWorkgroupName}"
          Action:
          - "athena:StartQueryExecution"
          - "athena:GetQueryExecution"
          Effect: "Allow"
          Sid: "AthenaAccess"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  LambdaFunctionPartitionerConfigLogGroup:
  # checkov:skip=CKV_AWS_158: No KMS encryption needed
    Type: AWS::Logs::LogGroup
    Condition: IsCreateDashboardResources
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaPartitioningFunctionName}"
      RetentionInDays: 14
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W84
            reason: "No KMS encryption needed"

  LambdaFunctionPartitionerConfig:
  # checkov:skip=CKV_AWS_115: Cuncurrency limit needs are customer dependent. This lambda is called whenever AWS Config delivers a file to S3, we don't anticipate the need for reserved concurrency for the moment.
  # checkov:skip=CKV_AWS_173: Environment variables are not sensitive.
  # checkov:skip=CKV_AWS_116: No DQL needed.
    Type: AWS::Lambda::Function
    Condition: IsCreateDashboardResources
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      FunctionName: !Sub "${LambdaPartitioningFunctionName}" 
      Description: "CRCD Dashboard - Lambda function that adds partitions when there are new AWS Config files"
      MemorySize: 128
      EphemeralStorage:
        Size: 512
      Timeout: 300
      Runtime: "python3.12"
      Architectures:
        - "x86_64"
      TracingConfig:
        Mode: "Active"
      VpcConfig:
        SecurityGroupIds: []
        SubnetIds: []
        Ipv6AllowedForDualStack: false
      RuntimeManagementConfig:
        UpdateRuntimeOn: "Auto"
      Handler: "index.lambda_handler"
      Role: !GetAtt IAMRoleLambdaPartitionerConfig.Arn
      FileSystemConfigs: []
      LoggingConfig:
        LogFormat: "Text"
        # this is already by default LogGroup: "/aws/lambda/{LambdaPartitioningFunctionName}"
      Environment:
        Variables:
          ATHENA_DATABASE_NAME: !Ref AthenaDatabaseName
          ATHENA_QUERY_RESULTS_BUCKET_NAME: !Ref AthenaQueryResultBucket
          ATHENA_WORKGROUP: !Ref AthenaWorkgroupName
          CONFIG_TABLE_NAME: !Ref AthenaTableName
          # Adding variables to enable/disable partitioning of ConfigSnapshot and ConfigHistory records
          # By default: ConfigSnapshot and ConfigHistory are enabled as they are used for different purposes 
          # Pass 0 to skip the record
          # Pass 1 to partition the record
          PARTITION_CONFIG_SNAPSHOT_RECORDS: 1
          PARTITION_CONFIG_HISTORY_RECORDS: 1
      Code:
        ZipFile: |
          import re
          import os
          import time
          import json
          import boto3
          from urllib.parse import unquote
          import gzip


          TABLE_NAME = os.environ.get("CONFIG_TABLE_NAME")
          ATHENA_DATABASE_NAME = os.environ["ATHENA_DATABASE_NAME"]
          ATHENA_QUERY_RESULTS_BUCKET_NAME = os.environ["ATHENA_QUERY_RESULTS_BUCKET_NAME"]
          ATHENA_WORKGROUP = os.environ['ATHENA_WORKGROUP']
          LOGGING_ON = False  # enables additional logging to CloudWatch

          # Partitioning of ConfigSnapshot and ConfigHistory records is enabled from parameters
          # Pass 0 to skip the record
          # Pass 1 to partition the record
          PARTITION_ENABLED = '1'
          PARTITION_DISABLED = '0'
          PARTITION_CONFIG_SNAPSHOT_RECORDS = os.environ['PARTITION_CONFIG_SNAPSHOT_RECORDS']
          PARTITION_CONFIG_HISTORY_RECORDS = os.environ['PARTITION_CONFIG_HISTORY_RECORDS']

          # This regular expressions pattern is compatible with how ControlTower Config logs AND also with how Config Logs are stored in S3 in standalone account
          # Structure for Config Snapshots: ORG-ID/AWSLogs/ACCOUNT-NUMBER/Config/REGION/YYYY/MM/DD/ConfigSnapshot/objectname.json.gz
          # Object name follows this pattern: ACCOUNT-NUMBER_Config_REGION_ConfigSnapshot_TIMESTAMP-YYYYMMDDHHMMSS_RANDOM-TEXT.json.gz
          # For example: 123412341234_Config_eu-north-1_ConfigSnapshot_20240306T122755Z_09c5h4kc-3jc7-4897-830v-d7a858325638.json.gz
          PATTERN = r'^(?P<org_id>[\w-]+)?/?AWSLogs/(?P<account_id>\d+)/Config/(?P<region>[\w-]+)/(?P<year>\d+)/(?P<month>\d+)/(?P<day>\d+)/(?P<type>ConfigSnapshot|ConfigHistory)/[^//]+$'

          # Athena client
          athena = boto3.client('athena')

          # Create an S3 client
          s3 = boto3.client('s3')

          class AthenaException(Exception):
              ''''This is raised only if the Query is not in state SUCCEEDED'''
              pass

          def lambda_handler(event, context):
              if LOGGING_ON: print('This is the event', event)

              event_object_key = None
              event_bucket_name = None
              dataSource = None # this can be ConfigSnapshot or ConfigHistory
              found_event = False

              # deciding what is enabled
              isPartitionConfigSnapshot = (PARTITION_CONFIG_SNAPSHOT_RECORDS == PARTITION_ENABLED)
              isPartitionConfigHistory = (PARTITION_CONFIG_HISTORY_RECORDS == PARTITION_ENABLED)

              # Is this called directly by S3 event notification?
              try:
                  event_object_key = event['Records'][0]['s3']['object']['key']
                  event_bucket_name = event['Records'][0]['s3']['bucket']['name']
                  if LOGGING_ON: print('This is an S3 event')
                  found_event = True
              except KeyError:
                  # key doesn't exist
                  if LOGGING_ON: print('This is not an S3 event, trying SNS')
                  pass

              if not found_event:
                  # Is this called via SNS topic?
                  try:
                      message = json.loads(event['Records'][0]['Sns']['Message'])
                      if LOGGING_ON: print('This is an SNS event, and this is the message', message)
                      event_bucket_name = message['Records'][0]['s3']['bucket']['name']
                      event_object_key = message['Records'][0]['s3']['object']['key']
                  except KeyError:
                      # key doesn't exist
                      print('This is not an SNS event either!!')
                      # at this point I cannot continue
                      return {
                          'statusCode': 200,
                          'body': 'Function was called with an unsupported event type.'
                      }

              if LOGGING_ON:
                  print('This is the object key', event_object_key)
                  print('This is the bucket', event_bucket_name)

              object_key_parent = f's3://{event_bucket_name}/{os.path.dirname(event_object_key)}/'

              # process object key
              match  = re.match(PATTERN, event_object_key)
              if not match:
                  print(f'SKIPPING: Cannot match {event_object_key} as AWS Config file, skipping.')
                  return {
                      'statusCode': 200,
                      'body': 'Object key is not supported, this is not an AWS Config file.'
                  }
              if LOGGING_ON:
                  print('match.groupdict() = ', match.groupdict())
                  
              accountid = match.groupdict()['account_id']
              region = match.groupdict()['region']
              date = '{year}-{month}-{day}'.format(**match.groupdict())
              if 'ConfigSnapshot' in event_object_key:
                  dataSource = 'ConfigSnapshot'

                  # If not enabled, I can return
                  if not isPartitionConfigSnapshot:
                      print(f'SKIPPING: {event_object_key} is a ConfigSnapshot. These records are disabled in the function\'s environment variables.')
                      return {
                          'statusCode': 200,
                          'body': 'ConfigSnapshot files are disabled in the function\'s environment variables.'
                      }
              elif 'ConfigHistory'  in event_object_key:
                  dataSource = 'ConfigHistory'

                  # If not enabled, and does not contain deleted resources notification I can return
                  if not isPartitionConfigHistory:
                      # Resource deletion info comes only from History records
                      # even if they are disabled I will index those containing this info
                      if not contains_deleted_resource(event_bucket_name, event_object_key):
                          print(f'SKIPPING: {event_object_key} is a ConfigHistory. These records are disabled in the function\'s environment variables.')
                          return {
                              'statusCode': 200,
                              'body': 'ConfigHistory files are disabled in the function\'s environment variables.'
                          }
              else:
                  # I can never get here, if the string passed the regex where ConfigSnapshot and ConfigHistory are checks
                  print(f'ERROR: {event_object_key} is neither ConfigSnapshot nor ConfigHistory')
                  return {
                      'statusCode': 200,
                      'body': 'Object key is not supported, this is not an AWS Config file.'
                  }

              drop_partition(accountid, region, date, dataSource)
              add_partition(accountid, region, date, object_key_parent, dataSource)


          # Resource deletion info comes only from History records
          # These must always be indexed
          def contains_deleted_resource(bucket_name, object_key):
              try:
                  # parse the object name before loading it
                  # if ':' is encoded as '%3A' in the object key, the load will fail
                  decoded_object_key = unquote(object_key)
                  print(f'Loading object {decoded_object_key} of bucket {bucket_name} to check it has resource deleted notifications')
                  obj = s3.get_object(Bucket=bucket_name, Key=decoded_object_key)
                      
                  # Read the gzipped JSON data from the S3 object
                  gz_data = obj['Body'].read()
                  json_data = gzip.decompress(gz_data).decode('utf-8')

                  # Load the JSON data into a dictionary
                  data = json.loads(json_data)

                  for item in data.get('configurationItems', []):
                      print(f"Current configurationItemStatus: {item.get('configurationItemStatus')}")

                      if item.get('configurationItemStatus') == 'ResourceDeleted':
                          return True
                  return False

              except s3.exceptions.ClientError as e:
                  if e.response['Error']['Code'] == 'NoSuchKey':
                      print(f"Error: The S3 object does not exist.")
                  else:
                      print(f"Error: {e}")
                  raise e
              except Exception as e:
                  print(f"Error: {e}")
                  raise e


          def add_partition(accountid, region, date, location, dataSource):
              execute_query(f"""
                  ALTER TABLE {TABLE_NAME}
                  ADD PARTITION (accountid='{accountid}', dt='{date}', region='{region}', dataSource='{dataSource}')
                  LOCATION '{location}'
              """)

          def drop_partition(accountid, region, date, dataSource):
              execute_query(f"""
                  ALTER TABLE {TABLE_NAME}
                  DROP IF EXISTS PARTITION (accountid='{accountid}', dt='{date}', region='{region}', dataSource='{dataSource}')
              """)

          def execute_query(query):
              print('Executing query:', query)
              start_query_response = athena.start_query_execution(
                  QueryString=query,
                  QueryExecutionContext={
                      'Database': ATHENA_DATABASE_NAME
                  },
                  ResultConfiguration={
                      'OutputLocation': f's3://{ATHENA_QUERY_RESULTS_BUCKET_NAME}',
                  },
                  WorkGroup=ATHENA_WORKGROUP
              )
              print('Query started')

              is_query_running = True
              while is_query_running:
                  time.sleep(1)
                  execution_status = athena.get_query_execution(
                      QueryExecutionId=start_query_response['QueryExecutionId']
                  )
                  query_state = execution_status['QueryExecution']['Status']['State']
                  is_query_running = query_state in ('RUNNING', 'QUEUED')

                  if not is_query_running and query_state != 'SUCCEEDED':
                      raise AthenaException('Query failed')
              print('Query completed')
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W58
            reason: "Permission to write CloudWatch logs is given in IAMRoleLambdaPartitionerConfig"
          - id: W92
            reason: "This function does not need reserved concurrent executions"

  IAMRoleLambdaPartitionerConfig:
    Type: AWS::IAM::Role
    Condition: IsCreateDashboardResources
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      Description: "CRCD Dashboard - Allows to add partitions to Athena and Glue, send logs to Cloudwatch, access Athena query results S3 bucket, receive objects from Config bucket. Each defined in separate policies"
      Path: "/"
      ManagedPolicyArns:
      - Ref: IAMManagedPolicyLambdaAthena
      - Ref: IAMManagedPolicyLambdaGlue
      - Ref: IAMManagedPolicyLambdaS3AthenaQueryResults
      - Ref: IAMManagedPolicyLambdaCloudWatchLogs
      - Ref: IAMManagedPolicyLambdaS3ConfigObject
      MaxSessionDuration: 3600
      RoleName: "crcd-lambda-partitioner-role"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Action: "sts:AssumeRole"
          Effect: "Allow"
          Principal:
            Service: "lambda.amazonaws.com"
          Sid: "AllowLambdaAssumeRole"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  GlueTable:
    Type: AWS::Glue::Table
    Condition: IsCreateDashboardResources
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref GlueDatabase
      TableInput:
        Name: !Ref AthenaTableName
        TableType: EXTERNAL_TABLE
        StorageDescriptor:
          Columns:
            - Name: fileversion
              Type: string
            - Name: configsnapshotid
              Type: string
            - Name: configurationitems
              Type: array<struct<configurationitemversion:string,configurationitemcapturetime:string,configurationstateid:bigint,awsaccountid:string,configurationitemstatus:string,resourcetype:string,resourceid:string,resourcename:string,arn:string,awsregion:string,availabilityzone:string,configurationstatemd5hash:string,configuration:string,supplementaryconfiguration:map<string,string>,tags:map<string,string>,resourcecreationtime:string>>
          Location: !Sub 's3://${DashboardBucketName}/'
          InputFormat: 'org.apache.hadoop.mapred.TextInputFormat'
          OutputFormat: 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
          SerdeInfo:
            SerializationLibrary: 'org.openx.data.jsonserde.JsonSerDe'
            Parameters:
              case.insensitive: 'false'
              mapping.arn: 'ARN'
              mapping.availabilityzone: 'availabilityZone'
              mapping.awsaccountid: 'awsAccountId'
              mapping.awsregion: 'awsRegion'
              mapping.configsnapshotid: 'configSnapshotId'
              mapping.configurationitemcapturetime: 'configurationItemCaptureTime'
              mapping.configurationitems: 'configurationItems'
              mapping.configurationitemstatus: 'configurationItemStatus'
              mapping.configurationitemversion: 'configurationItemVersion'
              mapping.configurationstateid: 'configurationStateId'
              mapping.configurationstatemd5hash: 'configurationStateMd5Hash'
              mapping.fileversion: 'fileVersion'
              mapping.resourceid: 'resourceId'
              mapping.resourcename: 'resourceName'
              mapping.resourcetype: 'resourceType'
              mapping.supplementaryconfiguration: 'supplementaryConfiguration'
          Compressed: false
          NumberOfBuckets: -1
        PartitionKeys:
          - Name: accountid
            Type: string
          - Name: dt
            Type: string
          - Name: region
            Type: string
          - Name: dataSource
            Type: string

  # Allows the S3 bucket that contains the Config files to invoke the lambda function
  # not needed if the S3 bucket does not send object notifications directly to lambda
  LambdaInvocationPermissionLambdaPartitionerConfig:
    Type: AWS::Lambda::Permission
    Condition: IsConfigureS3EventNotificationToLambda
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      FunctionName: !GetAtt LambdaFunctionPartitionerConfig.Arn
      Action: "lambda:InvokeFunction"
      SourceArn: !Sub "arn:${AWS::Partition}:s3:::${DashboardBucketName}"
      Principal: "s3.amazonaws.com"
      SourceAccount: !Sub ${AWS::AccountId} # Source account is always the local account


  # *********************************************************************************************************************************************************************
  # *********************************************************************************************************************************************************************
  # Resources on Dashboard account only
  # Bucket for the copy of the AWS Config files
  # Trigger configuration for the partitioner Lambda
  # Permissions to receive files from the replication of the objects in the Log Archive account
  # *********************************************************************************************************************************************************************
  # *********************************************************************************************************************************************************************
  
  DashboardBucket:
  # checkov:skip=CKV_AWS_18: We accept Dashboard bucket has no access logging
    Type: AWS::S3::Bucket
    Condition: IsDashboardAccountDeployment # I want this only in case of 2.1
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    DependsOn:
    - LambdaInvocationPermissionLambdaPartitionerConfig
    Properties:
      BucketName: !Ref DashboardBucketName
      BucketEncryption: 
        ServerSideEncryptionConfiguration:
          - !If
            - IsDashboardAccountDeploymentKms
            - ServerSideEncryptionByDefault:
                SSEAlgorithm: 'aws:kms'
                KMSMasterKeyID: !GetAtt DashboardBucketKmsKey.Arn
              BucketKeyEnabled: true
            - ServerSideEncryptionByDefault:
                SSEAlgorithm: 'AES256'
      VersioningConfiguration:
        Status: Enabled
      AccessControl: BucketOwnerFullControl
      OwnershipControls:
        Rules:
          - ObjectOwnership: BucketOwnerEnforced
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      NotificationConfiguration: # CloudFormation does not support the EventName/Id property for S3 NotificationConfiguration, it will get a random text
        LambdaConfigurations:
          - Event: 's3:ObjectCreated:*'
            # Filter: is not required, and we trigger the lambda on all objects anyway
            Filter:
              S3Key:
                  Rules:
                    - Name: prefix
                      Value: ""
                    - Name: suffix
                      Value: ""
            Function: !GetAtt LambdaFunctionPartitionerConfig.Arn
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W35
            reason: "We accept Dashboard bucket has no access logging"
      cfn-lint:
        config:
          ignore_checks:
            - W3045 # Consider using AWS::S3::BucketPolicy instead of AccessControl. This bucket is created with the same configuration as the AWS Config bucket when created by AWS ControlTower

  # Authorizing the replication configuration on the receiving side
  ReplicationConfigurationDataCollection:
    Type: AWS::S3::BucketPolicy
    DeletionPolicy: Delete
    UpdateReplacePolicy: Delete
    Condition: IsDashboardAccountDeployment # I want this only in case of 2.1
    Properties:
      Bucket: !Ref DashboardBucket
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: "Replication permissions for objects"
            Effect: "Allow"
            Principal:
              # The replication service role is always on the Log Archive account
              # Since at this point the role does not exist yet, I can only grant permissions to the entire Log Archive account
              AWS: !Ref LogArchiveAccountId
            Action:
            - "s3:ReplicateObject"
            - "s3:ReplicateDelete"
            Resource: !Sub "arn:${AWS::Partition}:s3:::${DashboardBucketName}/*"
          - Sid: "Replication permissions on the bucket"
            Effect: "Allow"
            Principal:
              # The replication service role is always on the Log Archive account
              # Since at this point the role does not exist yet, I can only grant permissions to the entire Log Archive account
              AWS: !Ref LogArchiveAccountId
            Action:
            - "s3:List*"
            - "s3:GetBucketVersioning"
            - "s3:PutBucketVersioning"
            Resource: !Sub "arn:${AWS::Partition}:s3:::${DashboardBucketName}"

# *****************************************************************************************************************************************
# *****************************************************************************************************************************************
# S3 Replication Only Resources - use case 2.2
# *****************************************************************************************************************************************
# *****************************************************************************************************************************************

  # Service role assumed by S3 when replicating files across accounts
  ReplicationConfigurationConfigBucketRole:
    Type: AWS::IAM::Role
    Condition: IsS3ReplicationOnly
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: s3.amazonaws.com
          Action: sts:AssumeRole
      # the Path property is set to /service-role/, which will create the role as a service role.
      # A service role is an IAM role that a service assumes to perform actions on your behalf. 
      Path: "/service-role/" 
      RoleName: !Ref CrossAccountReplicationRole
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  # Policy for the S3 replication role: must give permissions to both buckets
  ReplicationConfigurationConfigBucketPolicy:
    Type: AWS::IAM::Policy
    Condition: IsS3ReplicationOnly
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Action:
          - s3:ListBucket
          - s3:GetReplicationConfiguration
          - s3:GetObjectVersionForReplication
          - s3:GetObjectVersionAcl
          - s3:GetObjectVersionTagging
          - s3:GetObjectRetention
          - s3:GetObjectLegalHold
          Effect: Allow
          Resource:
          - !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}"
          - !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}/*"
          - !Sub "arn:${AWS::Partition}:s3:::${DashboardBucketName}"
          - !Sub "arn:${AWS::Partition}:s3:::${DashboardBucketName}/*"
        - Action:
          - s3:ReplicateObject
          - s3:ReplicateDelete
          - s3:ReplicateTags
          - s3:ObjectOwnerOverrideToBucketOwner
          Effect: Allow
          Resource:
          - !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}/*"
          - !Sub "arn:${AWS::Partition}:s3:::${DashboardBucketName}/*"
      PolicyName: !Sub "${CrossAccountReplicationRole}_s3"
      Roles:
        - !Ref ReplicationConfigurationConfigBucketRole

  # In case the buckets are encrypted with KMS, the replication role must be able to decrypt using the source key and encrypt using the destination key
  ReplicationConfigurationConfigBucketKmsPolicy:
    Type: AWS::IAM::Policy
    Condition: IsS3ReplicationKms
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Action:
          - kms:Decrypt
          - kms:GenerateDataKey
          Effect: Allow
          Resource: !Ref LogArchiveBucketKmsKeyArn
        - Action:
          - kms:Encrypt
          - kms:GenerateDataKey
          Effect: Allow
          Resource: !Ref DashboardBucketKmsKeyArn
      PolicyName: !Sub "${CrossAccountReplicationRole}_kms"
      Roles:
        - !Ref ReplicationConfigurationConfigBucketRole

  ConfigBucketReplicationConfigurationLambdaTrigger:
    Type: Custom::ConfigBucketReplicationConfigurationLambda
    Condition: IsConfigureS3Replication
    Properties:
      ServiceToken: !GetAtt ConfigBucketReplicationConfigurationLambda.Arn
      SourceBucket: !Ref LogArchiveBucketName
      DestinationBucket: !Ref DashboardBucketName
      DestinationBucketKmsKeyArn: !Ref DashboardBucketKmsKeyArn
      SourceAccountId: !Ref LogArchiveAccountId
      DestinationAccountId: !Ref DashboardAccountId
      ReplicationRoleArn: !GetAtt ReplicationConfigurationConfigBucketRole.Arn
      

  ConfigBucketReplicationConfigurationLambdaRole:
    Type: AWS::IAM::Role
    Condition: IsConfigureS3Replication
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3BucketReplicationPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  # these are the permissions: s3:PutReplicationConfiguration, s3:GetReplicationConfiguration
                  # Put replication configuration has a requirement on iam:PassRole
                  # To perform this operation, the user or role performing the action must have the iam:PassRole permission.
                  # From: https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutBucketReplication.html
                  - 'iam:PassRole'
                  - 's3:PutReplicationConfiguration'
                  - 's3:GetReplicationConfiguration'
                Resource: 
                  - !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}"
                  - !GetAtt ReplicationConfigurationConfigBucketRole.Arn

  # Lambda triggered by CloudFormation
  # to configure bucket replication on AWS Config Logging
  ConfigBucketReplicationConfigurationLambda:
  # checkov:skip=CKV_AWS_115: This is called only once by the CFN template.
  # checkov:skip=CKV_AWS_116: No DLQ by design. If this lambda fails, the whole CFN template fails.
    Type: AWS::Lambda::Function
    Condition: IsConfigureS3Replication
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      FunctionName: "crcd-support-configure-s3-replication"
      Description: "CRCD Dashboard - Configures a cross-account replication on the AWS Config bucket (one-time execution)"
      Handler: index.lambda_handler
      LoggingConfig:
        LogFormat: "Text"
        # this is already by default LogGroup: "/aws/lambda/crcd-support-configure-s3-replication"
      Role: !GetAtt ConfigBucketReplicationConfigurationLambdaRole.Arn
      Timeout: 300
      Runtime: "python3.12"
      MemorySize: 128
      EphemeralStorage:
        Size: 512
      Architectures:
        - "x86_64"
      TracingConfig:
        Mode: "Active"
      VpcConfig:
        SecurityGroupIds: []
        SubnetIds: []
        Ipv6AllowedForDualStack: false
      RuntimeManagementConfig:
        UpdateRuntimeOn: "Auto"
      Code:
        ZipFile: |
          import json
          import boto3
          from botocore.exceptions import ClientError

          def lambda_handler(event, context):
            print("Request received:\n", json.dumps(event))
              
            # Get the necessary parameters from the event
            params = event['ResourceProperties']
            source_bucket_name = params['SourceBucket']
            destination_bucket_kms_key = params['DestinationBucketKmsKeyArn']
            destination_bucket_name = params['DestinationBucket']
            source_account_id = params['SourceAccountId']
            destination_account_id = params['DestinationAccountId']
            replication_role_arn = params['ReplicationRoleArn']
            replication_rule_name = 'crcd-dashboard-configfile-replication'

            s3 = boto3.client('s3')

            if event['RequestType'] == 'Delete':
              # Remove the CRCD replication configuration
              # Get the current replication configuration
              try:
                existing_config = s3.get_bucket_replication(Bucket=source_bucket_name)
                # print("Existing replication configuration:\n", json.dumps(existing_config))

                if 'ReplicationConfiguration' in existing_config:
                  rules = existing_config['ReplicationConfiguration']['Rules']
                  # Filter out the rule with the specified name
                  new_rules = [rule for rule in rules if rule['ID'] != replication_rule_name]
                  if len(new_rules) < len(rules):
                    if new_rules:
                      # Update the replication configuration with the remaining rules
                      new_config = existing_config['ReplicationConfiguration']
                      new_config['Rules'] = new_rules
                      s3.put_bucket_replication(Bucket=source_bucket_name, ReplicationConfiguration=new_config)
                    else:
                      # If no rules remain, delete the entire replication configuration
                      s3.delete_bucket_replication(Bucket=source_bucket_name)
                    print(f"Replication rule '{replication_rule_name}' deleted successfully.")
                  else:
                    print(f"Replication rule '{replication_rule_name}' not found.")
                else:
                  print("No replication configuration found.")
              except ClientError as e:
                if e.response['Error']['Code'] == 'ReplicationConfigurationNotFoundError':
                  print("No replication configuration found.")
                else:
                  send_response(event, context, 'FAILED', {'Error': str(e)})
                  return
            else:
              # Check if replication configuration already exists
              # CRCD dashboard will not override any existing S3 replication configuration
              try:
                existing_config = s3.get_bucket_replication(Bucket=source_bucket_name)
                if 'ReplicationConfiguration' in existing_config:
                  error_message = f"Replication configuration already exists on bucket {source_bucket_name}. CRCD dashboard deployment cannot proceed."
                  print(error_message)

                  # This will make the CloudFormation template fail
                  send_response(event, context, 'FAILED', {'Error': error_message})
                  return
              except ClientError as e:
                if e.response['Error']['Code'] != 'ReplicationConfigurationNotFoundError':
                  send_response(event, context, 'FAILED', {'Error': str(e)})
                  return
              
              # Configure the replication
              try:
                if (not destination_bucket_kms_key):
                  print('Replication configuration with SSE-S3 encrypted buckets')
                  replication_configuration = {
                    'Role': replication_role_arn,
                    'Rules': [
                      {
                        'ID': replication_rule_name,
                        'Status': 'Enabled',
                        'Priority': 1,
                        "DeleteMarkerReplication": {"Status": "Disabled"},
                        "Filter": {"Prefix": ""},
                        'Destination': {
                          'Bucket': f'arn:aws:s3:::{destination_bucket_name}',
                          'Account': destination_account_id
                        }
                      }
                    ]
                  }
                else:
                  print('Replication configuration with KMS encrypted buckets')
                  replication_configuration = {
                    'Role': replication_role_arn,
                    'Rules': [
                      {
                        'ID': replication_rule_name,
                        'Status': 'Enabled',
                        'Priority': 1,
                        "DeleteMarkerReplication": {"Status": "Disabled"},
                        "Filter": {"Prefix": ""},
                        'Destination': {
                          'Bucket': f'arn:aws:s3:::{destination_bucket_name}',
                          'Account': destination_account_id,
                          'EncryptionConfiguration': {
                            'ReplicaKmsKeyID': f'{destination_bucket_kms_key}'
                          },
                        },
                        'SourceSelectionCriteria': {
                          'SseKmsEncryptedObjects': {
                            'Status': 'Enabled'  
                          },
                          'ReplicaModifications': {
                            'Status': 'Disabled'
                          }
                        }
                      }
                    ]
                  }

                print('Calling the s3 API')
                s3.put_bucket_replication(Bucket=source_bucket_name, ReplicationConfiguration=replication_configuration)
                print('Called the S3 API')
              except Exception as e:
                print('ERROR: ' + str(e))
                send_response(event, context, 'FAILED', {'Error': str(e)})
                return
              
            send_response(event, context, 'SUCCESS', {})

          def send_response(event, context, response_status, response_data):
            response_body = json.dumps({
              'Status': response_status,
              'Reason': 'See the details in CloudWatch Log Stream: ' + context.log_stream_name,
              'PhysicalResourceId': context.log_stream_name,
              'StackId': event['StackId'],
              'RequestId': event['RequestId'],
              'LogicalResourceId': event['LogicalResourceId'],
              'Data': response_data
            })

            response_url = event['ResponseURL']
            
            import urllib.request
            req = urllib.request.Request(response_url, data=response_body.encode('utf-8'), method='PUT')
            with urllib.request.urlopen(req) as f:
              print(f.read())
              print(f.info())
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W92
            reason: "This function does not need reserved concurrent executions"
                
  # ********************************************************************************************************************************************************
  # ********************************************************************************************************************************************************
  # Use case 1 - other than the CRCD resources I have to configure the bucket notification from the Config Logging bucket
  # ********************************************************************************************************************************************************
  # ********************************************************************************************************************************************************
  # Authorizing the notification configuration on the log archive bucket that already exists
  ConfigBucketNotificationConfigurationLambdaTrigger:
    Type: Custom::ConfigBucketNotificationConfigurationLambda
    Condition: IsConfigureS3EventNotificationToLambdaLogArchiveAccount
    DependsOn:
    - LambdaInvocationPermissionLambdaPartitionerConfig
    Properties:
      ServiceToken: !GetAtt ConfigBucketNotificationConfigurationLambda.Arn
      Bucket: !Ref LogArchiveBucketName
      # this is needed so that the Lambda function deletes only S3 notification configurations if they have this name
      # passing it here and as part of NotificationConfiguration so that I do not have to hard-code this string on the Lambda code 
      EventNotificationName: "CRCDPartitionerTrigger"
      NotificationConfiguration:
        LambdaFunctionConfigurations:
        - Events: ['s3:ObjectCreated:*']
          LambdaFunctionArn: !GetAtt LambdaFunctionPartitionerConfig.Arn
          # this is needed so that the Lambda function deletes only S3 notification configurations if they have this name
          Id: "CRCDPartitionerTrigger"

  ConfigBucketNotificationConfigurationLambdaRole:
    Type: AWS::IAM::Role
    Condition: IsConfigureS3EventNotificationToLambdaLogArchiveAccount
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3BucketNotificationPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 's3:PutBucketNotification'
                  - 's3:GetBucketNotification'
                Resource: !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}"

  # Lambda triggered by CloudFormation
  # to configure bucket notification on AWS Config Logging
  ConfigBucketNotificationConfigurationLambda:
  # checkov:skip=CKV_AWS_115: This is called only once by the CFN template.
  # checkov:skip=CKV_AWS_116: No DLQ by design. If this lambda fails, the whole CFN template fails.
    Type: AWS::Lambda::Function
    Condition: IsConfigureS3EventNotificationToLambdaLogArchiveAccount
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      FunctionName: "crcd-support-configure-s3-event-notification"
      Description: "CRCD Dashboard - Configures the Log Archive bucket S3 event notification to trigger the CRCD Lambda Partitioner function (one-time execution)"
      Handler: index.lambda_handler
      Role: !GetAtt ConfigBucketNotificationConfigurationLambdaRole.Arn
      Timeout: 300
      Runtime: "python3.12"
      MemorySize: 128
      EphemeralStorage:
        Size: 512
      Architectures:
        - "x86_64"
      TracingConfig:
        Mode: "Active"
      VpcConfig:
        SecurityGroupIds: []
        SubnetIds: []
        Ipv6AllowedForDualStack: false
      RuntimeManagementConfig:
        UpdateRuntimeOn: "Auto"
      LoggingConfig:
        LogFormat: "Text"
        # this is already by default LogGroup: "/aws/lambda/crcd-support-configure-s3-event-notification"
      Code:
        ZipFile: |
          import json
          import boto3

          def lambda_handler(event, context):
              
              print("Request received:\n", json.dumps(event))
              
              # Get the necessary parameters from the event
              params = event['ResourceProperties']
              bucket_name = params['Bucket']
              notification_event_name = params['EventNotificationName']
              notification_config = params['NotificationConfiguration']


              s3 = boto3.client('s3')

              if event['RequestType'] == 'Create':
                  # Check for existing notification configuration
                  # The CRCD Dashboard will not overwrite existing event notifications, and this function will fail
                  try:
                      existing_config = s3.get_bucket_notification_configuration(Bucket=bucket_name)
                      if has_notifications(existing_config):
                          # If there's an existing configuration, fail the Lambda function
                          error_message = f"Existing S3 event configuration found on bucket {bucket_name}. CRCD dashboard deployment cannot proceed."
                          print(error_message)
                          print(f"Existing S3 event configuration: {existing_config}")
                          send_response(event, context, 'FAILED', {'Error': error_message})
                          return  # Exit the function early
                  except Exception as e:
                      error_message = f"Error checking existing configuration: {str(e)}"
                      print(error_message)
                      send_response(event, context, 'FAILED', {'Error': error_message})
                      return  # Exit the function early

              if event['RequestType'] == 'Delete':
                  # Must check if the current event notification is the one from CRCD, only in that case it will be deleted
                  # Will cycle to all notification configurations and delete only the one pertaining to CRCD
                  try:
                      existing_config = s3.get_bucket_notification_configuration(Bucket=bucket_name)

                      # Initialize new configuration
                      new_notification_config = {
                          'LambdaFunctionConfigurations': [],
                          'QueueConfigurations': [],
                          'TopicConfigurations': []
                      }
                      
                      # Flag to check if we found and removed the matching configuration
                      config_removed = False

                      # Iterate through all types of configurations
                      # There could be non overlapping event notifications out of the bucket
                      for config_type in ['LambdaFunctionConfigurations', 'QueueConfigurations', 'TopicConfigurations']:
                          for config in existing_config.get(config_type, []):
                              if config.get('Id') == notification_event_name:
                                  config_removed = True
                                  print(f"Removing CRCD configuration with Id: {notification_event_name}")
                              else:
                                  print(f"Found non-CRCD configuration: {config.get('Id')}")
                                  new_notification_config[config_type].append(config)

                      # Remove empty lists from the new_notification_config
                      # e.g. 'TopicConfigurations' if none were found
                      new_notification_config = {k: v for k, v in new_notification_config.items() if v}

                      if not config_removed:
                          print(f"No matching event notification found for {notification_event_name}. No changes made.")
                          send_response(event, context, 'SUCCESS', {})
                          return

                      # Update notification_config to be used in put_bucket_notification_configuration
                      notification_config = new_notification_config
                      
                  except Exception as e:
                      error_message = f"Error processing existing configuration: {str(e)}"
                      print(error_message)
                      send_response(event, context, 'FAILED', {'Error': error_message})
                      return

              
              try:
                  # Configure the bucket notification
                  s3.put_bucket_notification_configuration(
                      Bucket=bucket_name,
                      NotificationConfiguration=notification_config
                  )
                  
                  # Send a successful response back to CloudFormation
                  send_response(event, context, 'SUCCESS', {})
              
              except Exception as e:
                  # Send a failed response back to CloudFormation
                  send_response(event, context, 'FAILED', {'Error': str(e)})

          def has_notifications(config):
              # Check if the configuration has any S3 notifications as indicated by the types below
              # pass the outcome of s3.get_bucket_notification_configuration
              notification_types = ['TopicConfigurations', 'QueueConfigurations', 'LambdaFunctionConfigurations']
              return any(config.get(type_) for type_ in notification_types)

          def send_response(event, context, response_status, response_data):
              response_body = json.dumps({
                  'Status': response_status,
                  'Reason': 'See the details in CloudWatch Log Stream: ' + context.log_stream_name,
                  'PhysicalResourceId': context.log_stream_name,
                  'StackId': event['StackId'],
                  'RequestId': event['RequestId'],
                  'LogicalResourceId': event['LogicalResourceId'],
                  'Data': response_data
              })

              response_url = event['ResponseURL']
              
              import urllib.request
              req = urllib.request.Request(response_url, data=response_body.encode('utf-8'), method='PUT')
              with urllib.request.urlopen(req) as f:
                  print(f.read())
                  print(f.info())
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W92
            reason: "This function does not need reserved concurrent executions"


  # ********************************************************************************************************************************************************
  # ********************************************************************************************************************************************************
  # Account names discovery
  # Will find account names data from either "optimization_data"."organization_data" or "cid_cur"."account_map"
  # ********************************************************************************************************************************************************
  # ********************************************************************************************************************************************************

  IAMRoleLambdaAccountNameSetup:
    Type: AWS::IAM::Role
    Condition: IsCreateDashboardResources
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      Description: "CRCD Dashboard - Allows to create views to Athena and Glue, send logs to Cloudwatch, access Athena query results S3 bucket, query Glue tables. Each defined in separate policies"
      Path: "/"
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      - Ref: IAMManagedPolicyLambdaAthena
      - Ref: IAMManagedPolicyLambdaGlue
      - Ref: IAMManagedPolicyLambdaS3AthenaQueryResults
      - Ref: IAMManagedPolicyLambdaGlueAccountNames
      MaxSessionDuration: 3600
      RoleName: "crcd-account-names-setup-role"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Action: "sts:AssumeRole"
          Effect: "Allow"
          Principal:
            Service: "lambda.amazonaws.com"
          Sid: "AllowLambdaAssumeRole"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"


  IAMManagedPolicyLambdaGlueAccountNames:
    Type: AWS::IAM::ManagedPolicy
    Condition: IsCreateDashboardResources
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-glue-account-names-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that gives permissions to query Glue tables to CRCD Lambda Account Names Setup"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Resource:
              - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:table/*/*"
              - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:table/*"
              - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database/*"
              - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:catalog"
            Action:
              - "glue:GetTable"
            Effect: "Allow"
            Sid: "ReadGlueTables"
          - Resource:
              - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:catalog"
              - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:table/${AthenaDatabaseName}/*"
              - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database/${AthenaDatabaseName}"
            Action:
              - "glue:UpdateTable"
              - "glue:CreateTable"
            Effect: "Allow"
            Sid: "AddSystemView"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  AccountNamesConfigurationLambdaTrigger:
    Type: Custom::AccountNamesConfigurationLambda
    Condition: IsCreateDashboardResources
    Properties:
      ServiceToken: !GetAtt AccountNamesConfigurationLambda.Arn

  # Lambda triggered by CloudFormation to configure account names system view
  AccountNamesConfigurationLambda:
  # checkov:skip=CKV_AWS_115: This is called only once by the CFN template.
  # checkov:skip=CKV_AWS_116: No DLQ by design. If this lambda fails, the whole CFN template fails.
  # checkov:skip=CKV_AWS_173: Environment variables are not sensitive.
    Type: AWS::Lambda::Function
    Condition: IsCreateDashboardResources
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    # Must be sure the Glue table and Athena database are there when this function creates a view
    DependsOn:
      - GlueTable
    Properties:
      FunctionName: "crcd-support-configure-account-names"
      Description: "CRCD Dashboard - Configures a system view that contains account names (one-time execution)"
      Handler: index.lambda_handler
      LoggingConfig:
        LogFormat: "Text"
        # this is already by default LogGroup: "/aws/lambda/crcd-support-configure-account-names"
      Role: !GetAtt IAMRoleLambdaAccountNameSetup.Arn
      Timeout: 300
      Runtime: "python3.12"
      MemorySize: 128
      EphemeralStorage:
        Size: 512
      Architectures:
        - "x86_64"
      TracingConfig:
        Mode: "Active"
      VpcConfig:
        SecurityGroupIds: []
        SubnetIds: []
        Ipv6AllowedForDualStack: false
      RuntimeManagementConfig:
        UpdateRuntimeOn: "Auto"
      Environment:
        Variables:
          # Possible sources of account names, from other CID dashboards
          ACCOUNT_NAMES_SOURCE_ACCOUNT_MAP_DATABASE: "cid_cur"
          ACCOUNT_NAMES_SOURCE_ACCOUNT_MAP_TABLE: "account_map"
          ACCOUNT_NAMES_SOURCE_ORGANIZATION_DATA_DATABASE: "optimization_data"
          ACCOUNT_NAMES_SOURCE_ORGANIZATION_DATA_TABLE: "organization_data"
          # Athena view that the Lambda is going to create
          CRCD_ACCOUNT_NAMES_VIEW_NAME: "config_sys_account_data"
          # Other input parameters
          CRCD_DATABASE_NAME: !Ref AthenaDatabaseName
          ATHENA_QUERY_RESULTS_BUCKET_NAME: !Ref AthenaQueryResultBucket
          ATHENA_WORKGROUP: !Ref AthenaWorkgroupName
      Code:
        ZipFile: |
          import json
          import os
          import boto3
          import time

          LOGGING_ON = False  # enables additional logging to CloudWatch

          class AthenaException(Exception):
              ''''This is raised in case of Exception while running queries on Athnea or Glue'''
              pass

          def lambda_handler(event, context):
              print("Request received:\n", json.dumps(event))

              organization_table_exists = False
              account_map_exists = False 

              # Get the necessary parameters from the environment variables
              CRCD_DATABASE_NAME = os.environ["CRCD_DATABASE_NAME"]   
              CRCD_ACCOUNT_NAMES_VIEW_NAME = os.environ["CRCD_ACCOUNT_NAMES_VIEW_NAME"]
              ATHENA_QUERY_RESULTS_BUCKET_NAME = os.environ["ATHENA_QUERY_RESULTS_BUCKET_NAME"]
              ATHENA_WORKGROUP = os.environ['ATHENA_WORKGROUP']
              ACCOUNT_NAMES_SOURCE_ACCOUNT_MAP_DATABASE = os.environ["ACCOUNT_NAMES_SOURCE_ACCOUNT_MAP_DATABASE"]
              ACCOUNT_NAMES_SOURCE_ACCOUNT_MAP_TABLE = os.environ["ACCOUNT_NAMES_SOURCE_ACCOUNT_MAP_TABLE"]
              ACCOUNT_NAMES_SOURCE_ORGANIZATION_DATA_DATABASE = os.environ["ACCOUNT_NAMES_SOURCE_ORGANIZATION_DATA_DATABASE"]
              ACCOUNT_NAMES_SOURCE_ORGANIZATION_DATA_TABLE = os.environ["ACCOUNT_NAMES_SOURCE_ORGANIZATION_DATA_TABLE"]

              # Athena and Glue clients
              glue = boto3.client('glue') 
              athena = boto3.client('athena')


              if event['RequestType'] == 'Delete':
                  # TODO remove view CRCD_ACCOUNT_NAMES_VIEW_NAME
                  send_response(event, context, 'SUCCESS', {})

              else:
                  try:
                      # Check which tables exist
                      if check_table_exists(glue, ACCOUNT_NAMES_SOURCE_ORGANIZATION_DATA_DATABASE, ACCOUNT_NAMES_SOURCE_ORGANIZATION_DATA_TABLE):
                          organization_table_exists = True
                      else:
                          account_map_exists = check_table_exists(glue, ACCOUNT_NAMES_SOURCE_ACCOUNT_MAP_DATABASE, ACCOUNT_NAMES_SOURCE_ACCOUNT_MAP_TABLE)

                      # Prepare CREATE OR REPLACE VIEW query based on table existence
                      if organization_table_exists:
                          create_view_query = f"""
                          CREATE OR REPLACE VIEW {CRCD_DATABASE_NAME}.{CRCD_ACCOUNT_NAMES_VIEW_NAME} AS
                          SELECT 
                              id as account_id,
                              payer_id as payer_account_id,  
                              name as account_name
                          FROM "optimization_data"."organization_data"
                          """
                      elif account_map_exists:
                          create_view_query = f"""
                          CREATE OR REPLACE VIEW {CRCD_DATABASE_NAME}.{CRCD_ACCOUNT_NAMES_VIEW_NAME} AS
                          SELECT 
                              account_id,
                              parent_account_id as payer_account_id,
                              account_name
                          FROM "cid_cur"."account_map"
                          """
                      else:
                          # This creates a view with no rows, but having these columns
                          create_view_query = f"""
                          CREATE OR REPLACE VIEW {CRCD_DATABASE_NAME}.{CRCD_ACCOUNT_NAMES_VIEW_NAME} AS
                          SELECT 
                              'NO_TABLES_EXIST' as account_id,
                              'NO_TABLES_EXIST' as payer_account_id,
                              'NO_TABLES_EXIST' as account_name
                          WHERE false
                          """

                      # Execute the view creation query    
                      success = execute_athena_query(athena, create_view_query, ATHENA_WORKGROUP, CRCD_DATABASE_NAME, ATHENA_QUERY_RESULTS_BUCKET_NAME)
                      if success:
                          print(f"View {CRCD_DATABASE_NAME}.{CRCD_ACCOUNT_NAMES_VIEW_NAME} created successfully. Data sources: organization_table_exists {organization_table_exists}, account_map_exists {account_map_exists}")
                          # Send a successful response back to CloudFormation
                          send_response(event, context, 'SUCCESS', {})
                      else:
                          send_response(event, context, 'FAILED', {'Error': 'Athena query was not successful'})
                  except AthenaException as ae:
                      # Send a failed response back to CloudFormation
                      send_response(event, context, 'FAILED', {'Error': str(ae)})
                  except Exception as e:
                      # Send a failed response back to CloudFormation
                      send_response(event, context, 'FAILED', {'Error': str(e)})


          def check_table_exists(glue, database_name, table_name):
              """Check if table exists in Glue Data Catalog"""
              print(f"Checking existence of table {table_name} on database {database_name}")
              
              try:
                  glue.get_table(DatabaseName=database_name, Name=table_name)
                  if LOGGING_ON:
                      print("Table exists")
                  return True
              except glue.exceptions.EntityNotFoundException:
                  if LOGGING_ON:
                      print("Table does not exist")
                  return False
              except Exception as e:
                  print(f"Error checking table existence: {str(e)}")
                  raise AthenaException(f'Query failed: {str(e)}')

          def execute_athena_query(athena, query, athena_workgroup, crcd_database, athena_bucket):
              print(f"Executing query: {query}")
              print(f"ATHENA_WORKGROUP = {athena_workgroup}")

              try:
                  start_query_response = athena.start_query_execution(
                      QueryString=query,
                      QueryExecutionContext={
                          'Database': crcd_database
                      },
                      ResultConfiguration={
                          'OutputLocation': f's3://{athena_bucket}',
                      },
                      WorkGroup=athena_workgroup
                  )
                  query_execution_id = start_query_response['QueryExecutionId']
                  print(f'Query started with execution ID: {query_execution_id}')

                  is_query_running = True
                  while is_query_running:
                      time.sleep(1)
                      execution_status = athena.get_query_execution(
                          QueryExecutionId=query_execution_id
                      )
                      query_execution = execution_status['QueryExecution']
                      query_state = query_execution['Status']['State']
                      is_query_running = query_state in ('RUNNING', 'QUEUED')

                      if not is_query_running and query_state != 'SUCCEEDED':
                          error_reason = query_execution['Status'].get('StateChangeReason', 'No reason provided')
                          error_details = {
                              'QueryExecutionId': query_execution_id,
                              'State': query_state,
                              'StateChangeReason': error_reason,
                              'Database': crcd_database,
                              'WorkGroup': athena_workgroup,
                              'OutputLocation': f's3://{athena_bucket}'
                          }
                          print(f'Query failed: {json.dumps(error_details)}')
                          raise AthenaException(f'Query failed: {error_reason}')
                  
                  print(f'Query completed successfully. Execution ID: {query_execution_id}')
                  return query_execution_id
              except Exception as e:
                  print(f'Exception during query execution: {str(e)}')
                  raise

          def send_response(event, context, response_status, response_data):
              """Interacts with the Cloud Formation template that called this Lambda"""
              response_body = json.dumps({
                  'Status': response_status,
                  'Reason': 'See the details in CloudWatch Log Stream: ' + context.log_stream_name,
                  'PhysicalResourceId': context.log_stream_name,
                  'StackId': event['StackId'],
                  'RequestId': event['RequestId'],
                  'LogicalResourceId': event['LogicalResourceId'],
                  'Data': response_data
              })

              response_url = event['ResponseURL']
              
              import urllib.request
              req = urllib.request.Request(response_url, data=response_body.encode('utf-8'), method='PUT')
              with urllib.request.urlopen(req) as f:
                  print(f.read())
                  print(f.info())

Outputs:
  # parameter --quicksight-datasource-role on CID-CMD tool
  QuickSightDataSourceRoleName:
    Description: "Name of the IAM role created for QuickSight"
    Condition: IsCreateDashboardResources
    Value: !Ref IAMRoleQuickSightDataSource

  # Will be needed to allow S3 Config to trigger this lambda and to allow the lambda to access the bucket
  LambdaARN:
    Description: "ARN of the Lambda Partitioner function supporting CRCD Dashboard"
    Condition: IsCreateDashboardResources
    Value: !GetAtt LambdaFunctionPartitionerConfig.Arn

  # Will be needed to allow S3 Config to trigger this lambda and to allow the lambda to access the bucket
  LambdaRoleARN:
    Description: "ARN of the execution role of the CRCD Lambda Partitioner function"
    Condition: IsCreateDashboardResources
    Value: !GetAtt IAMRoleLambdaPartitionerConfig.Arn

  # must be emptied before deleting the stack
  AthenaQueryResultBucketArn:
    Description: "Amazon S3 bucket used by Athena (ARN)"
    Condition: IsCreateDashboardResources
    Value: !GetAtt AthenaQueryResultBucket.Arn

  AthenaQueryResultBucketName:
    Description: "Amazon S3 bucket used by Athena (Name)"
    Condition: IsCreateDashboardResources
    Value: !Ref AthenaQueryResultBucket

  # must be emptied before deleting the stack
  DashboardBucketArn:
    Description: "Amazon S3 bucket created in the Dashboard account (deployment on dedicated Dashboard account)"
    Condition: IsDashboardAccountDeployment
    Value: !GetAtt DashboardBucket.Arn
    
  # Will be needed in case you have to manually configure replication
  ReplicationRoleName:
    Description: "Name of the IAM role created for the cross-account S3 replication of AWS Config files"
    Condition: IsS3ReplicationOnly
    Value: !Ref ReplicationConfigurationConfigBucketRole

  ReplicationRoleArn:
    Description: "ARN of the IAM role created for the cross-account S3 replication of AWS Config files"
    Condition: IsS3ReplicationOnly
    Value: !GetAtt ReplicationConfigurationConfigBucketRole.Arn

  DashboardBucketKmsKeyArn:
    Description: "Arn of the KMS key that encrypts the Dashboard bucket. Needed for the cross-account S3 replication of AWS Config files"
    Condition: IsDashboardAccountDeploymentKms
    Value: !GetAtt DashboardBucketKmsKey.Arn

  # these are great for development 
#  ConditionIsConfigureS3EventNotificationToLambda:
#    Description: "Development parameter: user selected to create Lambda trigger from S3"
#    Value: !If [IsConfigureS3EventNotificationToLambda, 'true', 'false'#]

#  ConditionIsLogArchiveAccountDeployment: 
#    # use case 1
#    Description: "Development parameter: use case 1, full deployment on Log Archive account"
#    Value: !If [IsLogArchiveAccountDeployment, 'true', 'false']
#    
#  ConditionIsDashboardAccountDeployment: 
#    # use case 2.1
#    Description: "Development parameter: use case 2.1, first part of deployment on Dashboard account - new data collection bucket and all dashboard resources"
#    Value: !If [IsDashboardAccountDeployment, 'true', 'false']
#    
#  ConditionIsS3ReplicationOnly: 
#    # Use case 2.2
#    Description: "Development parameter: use case 2.2, second part of deployment on Dashboard account - only the S3 replication configuration"
#    Value: !If [IsS3ReplicationOnly, 'true', 'false'#]

#  ConditionIsCreateDashboardResources:
#    # True on use case 1 or 2.1
#    Description: "Development parameter: create dashboard resources in use case 1 OR 2.1"
#    Value: !If [IsCreateDashboardResources, 'true', 'false'#]

#  ConditionIsConfigureS3EventNotificationToLambdaLogArchiveAccount: 
#    # True on use case 1 with lambda trigger selected
#    Description: "Development parameter: create lambda trigger resources on Log Archive account - use case 1"
#    Value: !If [IsConfigureS3EventNotificationToLambdaLogArchiveAccount, 'true', 'false'#]

#  ConditionIsConfigureS3Replication: 
#    # True on use case 2.2 with s3 replication selected
#    Description: "Development parameter: create s3 replication configuration Log Archive account - use case 2.2"
#    Value: !If [IsConfigureS3Replication, 'true', 'false']