# ------------------------------------------------------------------------------------
# Launch on Dashboard account, same region of CRCD resources 
# Lambda function that iterates through the Dashboard bucket and creates Glue/Athena 
# partitions for your historical AWS Config records
# ------------------------------------------------------------------------------------

AWSTemplateFormatVersion: '2010-09-09'
Description: CRCD backfill function - iterates through the Dashboard S3 bucket and recreates partitions on the CRCD Glue/Athena table

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      -
        Label:
          default: "Dashboard resources - where you deploy the dashboard and where its data is" 
        Parameters:
          - DashboardBucketName
          - AthenaQueryResultBucketName
      -
        Label:
          default: "Technical Parameters (DO NOT CHANGE)"
        Parameters:
          - AthenaWorkgroupName
          - AthenaDatabaseName
          - AthenaTableName
          - LambdaBackfillProducerFunctionName
          - LambdaBackfillWorkerFunctionName 
    
    ParameterLabels:
      AthenaWorkgroupName:
        default: "Athena workgroup"
      AthenaDatabaseName:
        default: "Athena database"
      AthenaQueryResultBucketName:
        default: "Name of the Athena query results bucket"
      AthenaTableName:
        default: "Athena table for AWS Config data"
      LambdaBackfillProducerFunctionName:
        default: "AWS Lambda unction that finds all the AWS Config files that need to be added as partition to the CRCD dashboard"
      LambdaBackfillWorkerFunctionName:
        default: "AWS Lambda function that partitions AWS Config files identified previously"
      DashboardBucketName:
        default: "Dashboard bucket"

Parameters:
  DashboardBucketName:
    Type: "String"
    Description: "Name of the Amazon S3 bucket that is used as source of data for the dashboard (Required)."
    AllowedPattern: '^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$'
    MinLength: 1
    ConstraintDescription: "Dashboard bucket name is missing or does not satisfy the Amazon S3 naming convention."

  AthenaQueryResultBucketName:
    Type: "String"
    Description: "This Amazon S3 bucket was created with the CRCD resources CloudFormation template. Write here the value of AthenaQueryResultBucketName in the output section of the template  (Required)."
    # 64 characters in the bucket name
    AllowedPattern: '^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$'
    MinLength: 1
    ConstraintDescription: "Required: Name of the Athena query results bucket."

  AthenaWorkgroupName:
    Type: "String"
    Default: "crcd-dashboard"
    Description: "The Athena workgroup for the dashboard (Required)."
    MinLength: 1
    ConstraintDescription: "Required: Athena workgroup"

  AthenaDatabaseName:
    Type: "String"
    Default: "cid_crcd_database"
    Description: "The Athena/Glue database for the dashboard (Required)."
    MinLength: 1
    # The name for an Athena database
    # Max 255 characters cannot have the symbol '-' and must have lowercase character, '_' is accepted
    # This value is not meant to be changed by the user, but we'll add the allowed pattern anyway
    AllowedPattern: '^[a-z0-9][a-z0-9_]{0,253}[a-z0-9]$'
    ConstraintDescription: "Required: Athena database"

  AthenaTableName:
    Type: "String"
    Default: "cid_crcd_config"
    Description: "The name that will be assigned to the Athena table that contains the AWS Config data for the dashboard (Required)."
    MinLength: 1
    ConstraintDescription: "Required: Athena table for AWS Config data."

  LambdaBackfillProducerFunctionName:
    Type: "String"
    Default: "crcd-config-backfill-producer"
    MinLength: 1
    ConstraintDescription: "Required: AWS Lambda function that scans the Dashboard bucket."
    Description: "AWS Lambda function that scans the Dashboard bucket and identifies all the AWS Config files (Required)."

  LambdaBackfillWorkerFunctionName:
    Type: "String"
    Default: "crcd-config-backfill-worker"
    MinLength: 1
    ConstraintDescription: "Required: AWS Lambda function that partitions AWS Config files."
    Description: "Name of the AWS Lambda function that partitions AWS Config files (Required)."


Rules:
  MandatoryDashboardBucketName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref DashboardBucketName
          - ''
        AssertDescription: "Dashboard bucket name is required"


  MandatoryAthenaDatabase:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref AthenaDatabaseName
          - ''
        AssertDescription: "Athena database is required"
  
  MandatoryAthenaQueryResultBucketName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref AthenaQueryResultBucketName
          - ''
        AssertDescription: "Athena query result bucket name is required"
  
  MandatoryAthenaWorkgroup:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref AthenaWorkgroupName
          - ''
        AssertDescription: "Athena workgroup is required"
  
  MandatoryAthenaTable:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref AthenaTableName
          - ''
        AssertDescription: "Athena table name is required"

  MandatoryLambdaBackfillProducerFunctionName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref LambdaBackfillProducerFunctionName
          - ''
        AssertDescription: "Lambda Backfill Producer function name is required"

  MandatoryLambdaBackfillWorkerFunctionName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref LambdaBackfillWorkerFunctionName
          - ''
        AssertDescription: "Lambda Backfill Worker function name is required"

Resources:

  # SQS Queue
  # When Lambda receives a batch, the messages stay in the queue but are hidden for the length of the queue's visibility timeout. 
  # If your function successfully processes all messages in the batch, Lambda deletes the messages from the queue. 
  # By default, if your function encounters an error while processing a batch, all messages in that batch become visible in the queue again after the visibility timeout expires.
  SQSBackfillingQueue:
  # checkov:skip=CKV_AWS_27: We set SqsManagedSseEnabled: true
    Type: AWS::SQS::Queue
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      QueueName: crcd-backfill-sqs-queue
      # Worker Lambda has 600 seconds timeout
      # Best practice: set the queue's visibility timeout to at least six times the timeout that you configure on your function
      VisibilityTimeout: 3600
      MessageRetentionPeriod: 345600 # 4 days
      SqsManagedSseEnabled: true
      DelaySeconds: 60 # some Log Archive buckets may cointain many files, we give the worker Lambda some more time to process them
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W48
            reason: "KMS encryption specified in SqsManagedSseEnabled = true"


  # Grant Permissions to Lambda Producer function to write to the SQS Queue
  SQSQueuePolicy:
    Type: AWS::SQS::QueuePolicy
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              AWS: '*'
            Action: 'sqs:SendMessage'
            Resource: !GetAtt SQSBackfillingQueue.Arn
            Condition:
              ArnEquals:
                'aws:SourceArn': !GetAtt LambdaBackfillProducerFunction.Arn
      Queues:
        - !Ref SQSBackfillingQueue
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: F21 # SQS Queue policy should not allow * principal
            reason: "Condition ArnEquals limits the principal allowed to send messages to the queue"

  # IAM Role for Lambda
  LambdaBackfillProducerFunctionRole:
    Type: AWS::IAM::Role
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      RoleName: "crcd-backfill-producer-role"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: AllowSQSPublishS3Iteration
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: 'sqs:SendMessage'
                Resource: !GetAtt SQSBackfillingQueue.Arn
              - Effect: Allow
                Action:
                  - 's3:ListBucket'
                  - 's3:GetObject'
                Resource:
                  - !Sub 'arn:${AWS::Partition}:s3:::${DashboardBucketName}'
                  - !Sub 'arn:${AWS::Partition}:s3:::${DashboardBucketName}/*'
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Role names are as per our design"

  LambdaBackfillProducerFunctionLogGroup:
  # checkov:skip=CKV_AWS_158: No KMS encryption needed
    Type: AWS::Logs::LogGroup
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaBackfillProducerFunctionName}"
      RetentionInDays: 14
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W84
            reason: "No KMS encryption needed"
  
  # Lambda Function that produces the list of AWS Config objects that must be partitioned
  LambdaBackfillProducerFunction:
  # checkov:skip=CKV_AWS_115: Cuncurrency limit needs are customer dependent. This lambda is run only once, we don't anticipate the need for reserved concurrency for the moment.
  # checkov:skip=CKV_AWS_173: Environment variables are not sensitive.
  # checkov:skip=CKV_AWS_116: No DQL needed.
    Type: AWS::Lambda::Function
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      FunctionName: !Sub "${LambdaBackfillProducerFunctionName}"
      Description: "CRCD Dashboard Backfill Producer - Lambda function that scans your Dashboard bucket to identify AWS Config objects that will be displayed on the dashboard"
      Runtime: python3.12
      Timeout: 900
      MemorySize: 128
      EphemeralStorage:
        Size: 512
      Role: !GetAtt LambdaBackfillProducerFunctionRole.Arn
      Environment:
        Variables:
          BUCKET_NAME_VAR: !Ref DashboardBucketName
          SQS_QUEUE_URL_VAR: !Ref SQSBackfillingQueue
      Handler: index.lambda_handler
      Architectures:
        - "x86_64"
      TracingConfig:
        Mode: "Active"
      VpcConfig:
        SecurityGroupIds: []
        SubnetIds: []
        Ipv6AllowedForDualStack: false
      FileSystemConfigs: []
      LoggingConfig:
        LogFormat: "Text"
      RuntimeManagementConfig:
        UpdateRuntimeOn: "Auto"
      Code:
        ZipFile: |
          # AWS Config Resource Compliance Dashboard
          # Backfilling producer function, scans your Amazon S3 dashboard bucket and finds all files related to AWS Config that are valid.
          # These files are sent to an SQS queue that will trigger another function to add them to the dashboard data.
          # 
          # Valid files
          # 1. all Config history records (WIP: we may go back until a certain date, e.g. max 1 year)
          # 2. Config snapshot records on all accounts an regions whose date is the last day of the month, from the last day of the month until 5 monhs ago
          # 3. Config snapshot records on all accounts an regions whose date is within the last 5 days

          import boto3
          import json
          import os
          import re
          from datetime import datetime, timedelta
          from calendar import monthrange
          from dateutil.relativedelta import relativedelta

          DASHBOARD_BUCKET_NAME = os.environ["BUCKET_NAME_VAR"]
          SQS_QUEUE = os.environ["SQS_QUEUE_URL_VAR"]

          # How much in the past we want to scan (months)
          CONFIG_HISTORY_TIME_LIMIT_MONTHS = 12
          CONFIG_SNAPSHOT_TIME_LIMIT_MONTHS = 6

          # SQS client
          sqs = boto3.client('sqs')

          # Create an S3 client
          s3 = boto3.client('s3')
            
          # Define the batch size of the objects read from S3
          batch_size = 500

          LOGGING_ON = False  # enables additional logging to CloudWatch

          # This regular expressions pattern is compatible with how ControlTower Config logs AND also with how Config Logs are stored in S3 in standalone account
          # Structure for Config Snapshots: ORG-ID/AWSLogs/ACCOUNT-NUMBER/Config/REGION/YYYY/MM/DD/ConfigSnapshot/objectname.json.gz
          # Object name follows this pattern: ACCOUNT-NUMBER_Config_REGION_ConfigSnapshot_TIMESTAMP-YYYYMMDDHHMMSS_RANDOM-TEXT.json.gz
          # For example: 123412341234_Config_eu-north-1_ConfigSnapshot_20240306T122755Z_09c5h4kc-3jc7-4897-830v-d7a858325638.json.gz
          PATTERN = r'^(?P<org_id>[\w-]+)?/?AWSLogs/(?P<account_id>\d+)/Config/(?P<region>[\w-]+)/(?P<year>\d+)/(?P<month>\d+)/(?P<day>\d+)/(?P<type>ConfigSnapshot|ConfigHistory)/[^//]+$'

          # Regular expression to get the timestamp of the AWS Config file from its S3 prefix
          # Used to apply filters on files depending on the date
          DATE_PATTERN = r'^.*/(\d{4})/(\d{1,2})/(\d{1,2})/.*$'

          def lambda_handler(event, context):
            # Initialize the continuation token
            continuation_token = None
            object_counter = 0
            batch_counter = 0

            config_snapshot_counter = 0
            config_history_counter = 0
            potential_object_counter = 0
            actual_object_counter = 0
            
            # This is to register all S3 prefixes that need a partition
            # Especially with ConfigHistory records, an S3 partition (i.e. path) may contain several files and
            # it does not make sense to create the Athena partition for each file, as the partition points to the S3 prefix
            # KEYS: the S3 prefix, keys are unique and adding a key with the same value of an existing key will simply replace the key and its value
            # VALUES: any S3 object belonging to the prefix
            prefixes = dict()

            # Sends to SQS only AWS Config files 
            # Compliance and Inventory: the dashboard reports the current month of data and the full data from the previous 5 months
            # Event history: the dashbaord has all history - we limit the history to one year
            current_date = datetime.now()
            config_snapshot_date_limit = current_date - relativedelta(months=CONFIG_SNAPSHOT_TIME_LIMIT_MONTHS)
            config_history_date_limit = current_date - relativedelta(months=CONFIG_HISTORY_TIME_LIMIT_MONTHS)

            # Create datetime for last day of month 6 months ago - for Config snapshot files
            # Get year and month from limit date
            year = config_snapshot_date_limit.year
            month = config_snapshot_date_limit.month
            # Get last day of that month using monthrange
            _, last_day = monthrange(year, month)

            # This is the date after which every AWS Config snapshot file have to be sent to SQS
            min_config_snapshot_date = datetime(year, month, last_day)
            print (f'This is the minimum date for Config snapshot files: {min_config_snapshot_date}')


            # Create datetime for Config history records limit
            year = config_history_date_limit.year
            month = config_history_date_limit.month
            min_config_history_date = datetime(year, month, 1)
            print (f'This is the minimum date for Config history files: {min_config_history_date}')

            # Iterate over the object keys in batches
            while True:
              # Get the next batch of objects
              response = list_objects_batched(continuation_token)
              batch_counter +=1

              # Process the objects in the batch
              for obj in response.get('Contents', []):
                object_counter +=1
                key = obj['Key']
                    
                # all types of Config files must be sent to the worker
                if can_process(key, min_config_snapshot_date, min_config_history_date):
                  if 'ConfigHistory' in key:
                    config_history_counter +=1
                    if LOGGING_ON: print (f'This is a ConfigHistory: {key}')
                  
                  if 'ConfigSnapshot' in key:
                    config_snapshot_counter +=1
                    if LOGGING_ON: print (f'This is a ConfigSnapshot: {key}')

                  potential_object_counter += 1
                  # fakes an S3 trigger as payload for the SQS message
                  payload = {
                    "Records": [
                      {
                        "s3": {
                          "object": {
                            "key": key
                          },
                          "bucket": {
                            "name": DASHBOARD_BUCKET_NAME
                          }
                        }
                      }
                    ]
                  }
                      
                  # adding the S3 prefix to the list, this will be processed later
                  # multiple files may be in te same prefix, and I need to invoke the partitioner function only once per prefix
                  prefix_key = f'{os.path.dirname(key)}'
                  # print('This is the object key', key)
                  # print(f'[{batch_counter}/{object_counter}] This is the S3 path {prefix_key}')
                  prefixes[prefix_key] = payload

                  # -------------------------------------------------------------------------------
                  # only for development
                  # break here after a few objects
                  # if object_counter >= 20: break
                  # -------------------------------------------------------------------------------
                  

              # Check if there are more batches to process
              if response['IsTruncated']:
                continuation_token = response['NextContinuationToken']
              else:
                break
            
            # iterate through the prefixes to add them to the SQS queue
            # must do this at the very end, when I have mapped all the files
            for k in prefixes:
              actual_object_counter += 1
              
              sqs.send_message(
                QueueUrl=SQS_QUEUE, 
                MessageBody=json.dumps(prefixes[k])
              )
              if LOGGING_ON: print(f'Added to SQS queue the object : {k}')

            return {
                'statusCode': 200,
                'body': json.dumps(f'Successfully processed {object_counter} objects in the bucket. Sent to the queue for partitioning {actual_object_counter} objects out of {potential_object_counter} valid AWS Config objects, of which there were {config_history_counter} ConfigHistory and {config_snapshot_counter} ConfigSnapshot records.')
            }

          # Define the function to list objects in batches
          def list_objects_batched(continuation_token=None):
            try:
              response = s3.list_objects_v2(
                Bucket=DASHBOARD_BUCKET_NAME,
                MaxKeys=batch_size,
                ContinuationToken=continuation_token or ''
              )
            except s3.exceptions.ClientError as e:
              if e.response['Error']['Code'] == 'InvalidArgument':
                print('Invalid continuation token, starting from the beginning.')
                response = s3.list_objects_v2(Bucket=DASHBOARD_BUCKET_NAME, MaxKeys=batch_size)
              else:
                raise e
            
            return response 

          # returns true if the given object name corresponds to an AWS Config snapshot or history record
          # and it is within the time frame of historical data, i.e. current month and the previous full five months
          def can_process(object_key, min_config_snapshot_date, min_config_history_date):
            # process object key
            match  = re.match(PATTERN, object_key)

            # if match it is a config file
            if match:
              if LOGGING_ON: print(f'{object_key} is an AWS Config file, checking its timestamp.')

              # Use regex pattern for date extraction  
              date_match = re.match(DATE_PATTERN, object_key)

              if date_match:
                year, month, day = map(int, date_match.groups())
                config_file_date = datetime(year, month, day)

                if 'ConfigHistory' in object_key:
                  if config_file_date < min_config_history_date:
                    if LOGGING_ON: print(f'ConfigHistory file is dated {config_file_date}: too old for the dashboard. Skipping.')
                    match = False
                  else:
                    if LOGGING_ON: print(f'ConfigHistory file is dated {config_file_date}: will be send to the SQS queue.')
                    match = True
                
                if 'ConfigSnapshot' in object_key:
                  # only the last day of the month for the previous CONFIG_SNAPSHOT_TIME_LIMIT_MONTHS months and the last 5-ish days
                  match = is_config_snapshot_date_valid(config_file_date)
              else:
                # cannot extract the date from the prefix - should never get here
                print(f'ERROR: Cannot extract the date from prefix {object_key}. Skipping.')
                match = False
            else:
              print(f'Cannot match {object_key} as AWS Config file. Skipping.')
              match = False

            return match
            

          def is_config_snapshot_date_valid(config_file_date):
              today = datetime.now().date()
              
              # Check if date is within last 5 days
              five_days_ago = today - timedelta(days=5)
              if five_days_ago <= config_file_date.date() <= today:
                  if LOGGING_ON: print(f'ConfigSnapshot file is dated {config_file_date}: will be send to the SQS queue.')
                  return True
              
              # Check last days of previous months
              current_year = today.year
              current_month = today.month
              
              # Check previous months until the limit
              for i in range(1, CONFIG_SNAPSHOT_TIME_LIMIT_MONTHS):  
                  # Calculate the year and month we're checking
                  check_month = current_month - i
                  check_year = current_year
                  
                  # Adjust year if we need to go back to previous year
                  if check_month <= 0:
                      check_month += 12
                      check_year -= 1
                      
                  # Get the last day of that month
                  _, last_day = monthrange(check_year, check_month)
                  last_date = datetime(check_year, check_month, last_day).date()
                  
                  # Compare with config_file_date
                  if config_file_date.date() == last_date:
                      if LOGGING_ON: print(f'ConfigSnapshot file is dated {config_file_date}: will be send to the SQS queue.')
                      return True
              
              if LOGGING_ON: print(f'ConfigSnapshot file is dated {config_file_date}: too old or not a end of month date. Skipping.')
              return False            
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W92
            reason: "This function does not need reserved concurrent executions"



# ------------------------------------------------------------------------------------------------------------------------------------------------------------
# Worker
# ------------------------------------------------------------------------------------------------------------------------------------------------------------

  # Invoke worker Lambda Function from SQS Queue
  SQSEventSource:
    Type: AWS::Lambda::EventSourceMapping
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      EventSourceArn: !GetAtt SQSBackfillingQueue.Arn
      FunctionName: !Ref LambdaBackfillWorkerFunction
      # The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation (6 MB).
      # Amazon Simple Queue Service â€“ Default 10. For standard queues the max is 10,000. For FIFO queues the max is 10.
      BatchSize: 10
      # (Amazon SQS only) The scaling configuration for the event source. For more information, see Configuring maximum concurrency for Amazon SQS event sources.
      ScalingConfig: 
         MaximumConcurrency: 20 

  LambdaBackfillWorkerFunctionLogGroup:
  # checkov:skip=CKV_AWS_158: No KMS encryption needed
    Type: AWS::Logs::LogGroup
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaBackfillWorkerFunctionName}"
      RetentionInDays: 14
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W84
            reason: "No KMS encryption needed"

  # Lambda Function that is triggered by the SQS queue
  LambdaBackfillWorkerFunction:
  # checkov:skip=CKV_AWS_115: Cuncurrency limit needs are customer dependent. This lambda is run only once and triggered by SQS, we don't anticipate the need for reserved concurrency for the moment.
  # checkov:skip=CKV_AWS_173: Environment variables are not sensitive.
  # checkov:skip=CKV_AWS_116: No DQL needed.
    Type: AWS::Lambda::Function
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      FunctionName: !Sub "${LambdaBackfillWorkerFunctionName}"
      Description: "CRCD Dashboard Backfill Worker - AWS Lambda function that partitions AWS Config files to visualize your past data on the dashboard"
      Runtime: python3.12
      Timeout: 600
      MemorySize: 128
      EphemeralStorage:
        Size: 512
      Role: !GetAtt LambdaBackfillWorkerFunctionRole.Arn
      Environment:
        Variables:
          BUCKET_NAME_VAR: !Ref DashboardBucketName
          SQS_QUEUE_URL_VAR: !Ref SQSBackfillingQueue

          # Must have all the variables of the standard partitioner
          ATHENA_DATABASE_NAME: !Ref AthenaDatabaseName
          ATHENA_QUERY_RESULTS_BUCKET_NAME: !Ref AthenaQueryResultBucketName
          ATHENA_WORKGROUP: !Ref AthenaWorkgroupName
          CONFIG_TABLE_NAME: !Ref AthenaTableName
          # Adding variables to enable/disable partitioning of ConfigSnapshot and ConfigHistory records
          # By default: both ConfigSnapshot and ConfigHistory are enabled 
          # Pass 0 to skip the record
          # Pass 1 to partition the record
          PARTITION_CONFIG_SNAPSHOT_RECORDS: 1
          PARTITION_CONFIG_HISTORY_RECORDS: 1
      Handler: index.lambda_handler
      Architectures:
        - "x86_64"
      TracingConfig:
        Mode: "Active"
      VpcConfig:
        SecurityGroupIds: []
        SubnetIds: []
        Ipv6AllowedForDualStack: false
      FileSystemConfigs: []
      LoggingConfig:
        LogFormat: "Text"
      RuntimeManagementConfig:
        UpdateRuntimeOn: "Auto"

      Code:
        ZipFile: |
          # Config Resource Compliance Dashboard
          # Backfill worker function triggered by SQS
          # Capable of handling batches of S3 prefixes and create partitions in Athena/Glue 
          # with the same logic as the CRCD Lambda partitioner function

          import os
          import re
          import time
          import json
          import boto3
          import random
          import gzip
          from urllib.parse import unquote

          TABLE_NAME = os.environ.get("CONFIG_TABLE_NAME")
          ATHENA_DATABASE_NAME = os.environ["ATHENA_DATABASE_NAME"]
          ATHENA_QUERY_RESULTS_BUCKET_NAME = os.environ["ATHENA_QUERY_RESULTS_BUCKET_NAME"]
          ATHENA_WORKGROUP = os.environ['ATHENA_WORKGROUP']
          LOGGING_ON = False  # enables additional logging to CloudWatch

          # Partitioning of ConfigSnapshot and ConfigHistory records is enabled from parameters
          # Pass 0 to skip the record
          # Pass 1 to partition the record
          PARTITION_ENABLED = '1'
          PARTITION_DISABLED = '0'
          PARTITION_CONFIG_SNAPSHOT_RECORDS = os.environ['PARTITION_CONFIG_SNAPSHOT_RECORDS']
          PARTITION_CONFIG_HISTORY_RECORDS = os.environ['PARTITION_CONFIG_HISTORY_RECORDS']

          # This regular expressions pattern is compatible with how ControlTower Config logs AND also with how Config Logs are stored in S3 in standalone account
          # Structure for Config Snapshots: ORG-ID/AWSLogs/ACCOUNT-NUMBER/Config/REGION/YYYY/MM/DD/ConfigSnapshot/objectname.json.gz
          # Object name follows this pattern: ACCOUNT-NUMBER_Config_REGION_ConfigSnapshot_TIMESTAMP-YYYYMMDDHHMMSS_RANDOM-TEXT.json.gz
          # For example: 123412341234_Config_eu-north-1_ConfigSnapshot_20240306T122755Z_09c5h4kc-3jc7-4897-830v-d7a858325638.json.gz
          PATTERN = r'^(?P<org_id>[\w-]+)?/?AWSLogs/(?P<account_id>\d+)/Config/(?P<region>[\w-]+)/(?P<year>\d+)/(?P<month>\d+)/(?P<day>\d+)/(?P<type>ConfigSnapshot|ConfigHistory)/[^//]+$'

          # create the clients that are needed
          athena = boto3.client('athena')
          s3 = boto3.client('s3')

          def lambda_handler(event, context):

              for rec in event['Records']:
                  print(f'ITERATION---START--------------------------------------------------')
                  print(f'rec = {rec}')
                  event_body = rec['body']
                  print(f'event_body = {event_body}')
                  message = json.loads(event_body)
                  event_bucket_name = message['Records'][0]['s3']['bucket']['name']
                  # print(f'event_bucket_name = {event_bucket_name}')
                  event_object_key = message['Records'][0]['s3']['object']['key']
                  # print(f'event_object_key = {event_object_key}')

                  print(f'Backfilling item: bucket = {event_bucket_name}, object = {event_object_key}')

                  backfill(event_bucket_name, event_object_key)

                  print(f'ITERATION---END--------------------------------------------------')

              print(f'CRCD Backfill Worker DONE')
              return {
                  'statusCode': 200,
                  'body': 'CRCD Backfill complete.'
              }


          def backfill(event_bucket_name, event_object_key):
              object_key_parent = f's3://{event_bucket_name}/{os.path.dirname(event_object_key)}/'

              # deciding what is enabled
              isPartitionConfigSnapshot = (PARTITION_CONFIG_SNAPSHOT_RECORDS == PARTITION_ENABLED)
              isPartitionConfigHistory = (PARTITION_CONFIG_HISTORY_RECORDS == PARTITION_ENABLED)

              # process object key
              match  = re.match(PATTERN, event_object_key)
              if not match:
                  print(f'Cannot match {event_object_key} as AWS Config file, skipping.')
                  return
              if LOGGING_ON:
                  print('match.groupdict() = ', match.groupdict())
              
              accountid = match.groupdict()['account_id']
              region = match.groupdict()['region']
              date = '{year}-{month}-{day}'.format(**match.groupdict())

              if 'ConfigSnapshot' in event_object_key:
                  dataSource = 'ConfigSnapshot'

                  # If not enabled, I can return
                  if not isPartitionConfigSnapshot:
                      print(f'SKIPPING: {event_object_key} is a ConfigSnapshot. These records are disabled in the function\'s environment variables.')
                      return
              elif 'ConfigHistory' in event_object_key:
                  dataSource = 'ConfigHistory'
                  # If not enabled, I can return
                  if not isPartitionConfigHistory:
                      print(f'SKIPPING: {event_object_key} is a ConfigHistory. These records are disabled in the function\'s environment variables.')
                      return
              else:
                  # I can never get here, if the string passed the regex where ConfigSnapshot and ConfigHistory are checks
                  print(f'ERROR - Cannot match {event_object_key} as AWS Config file, skipping.')
                  return
              
              drop_partition(accountid, region, date, dataSource)
              add_partition(accountid, region, date, object_key_parent, dataSource)

          # Adds an Athena partition with exponential backoff
          def add_partition(accountid, region, date, location, dataSource):
              backoff_retry(
                  lambda: execute_query(f"""
                          ALTER TABLE {TABLE_NAME}
                          ADD PARTITION (accountid='{accountid}', dt='{date}', region='{region}', dataSource='{dataSource}')
                          LOCATION '{location}'
                      """)
              )

          # Drops an Athena partition with exponential backoff
          def drop_partition(accountid, region, date, dataSource):
              backoff_retry(
                  lambda: execute_query(f"""
                          ALTER TABLE {TABLE_NAME}
                          DROP IF EXISTS PARTITION (accountid='{accountid}', dt='{date}', region='{region}', dataSource='{dataSource}')
                      """)
              )

          # Runs an SQL statemetn against Athena
          def execute_query(query):
              print('Executing query:', query)
              start_query_response = athena.start_query_execution(
                  QueryString=query,
                  QueryExecutionContext={
                      'Database': ATHENA_DATABASE_NAME
                  },
                  ResultConfiguration={
                      'OutputLocation': f's3://{ATHENA_QUERY_RESULTS_BUCKET_NAME}',
                  },
                  WorkGroup=ATHENA_WORKGROUP
              )
              print('Query started')

              is_query_running = True
              while is_query_running:
                  time.sleep(1)
                  execution_status = athena.get_query_execution(
                      QueryExecutionId=start_query_response['QueryExecutionId']
                  )
                  query_state = execution_status['QueryExecution']['Status']['State']
                  is_query_running = query_state in ('RUNNING', 'QUEUED')

                  if not is_query_running and query_state != 'SUCCEEDED':
                      raise AthenaException('Query failed')
              print('Query completed')

          # Exponential backoff implementation
          def backoff_retry(func, max_retries=10, base_delay=1, max_delay=120):
              retries = 0
              while True:
                  try:
                      return func()
                  except Exception as e:
                      print(f"Error in backoff retry: {e}")
                      retries += 1
                      if retries > max_retries:
                          raise e
                      
                      # Calculate delay with exponential backoff and jitter
                      delay = min(max_delay, base_delay * (2 ** (retries - 1)))
                      jitter = random.uniform(0, 0.1 * delay)
                      total_delay = delay + jitter
                      
                      print(f"Retry {retries} after {total_delay:.2f} seconds")
                      time.sleep(total_delay)

          class AthenaException(Exception):
              ''''This is raised only if the Query is not in state SUCCEEDED'''
              pass

    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W92
            reason: "This function does not need reserved concurrent executions"

  LambdaBackfillWorkerFunctionRole:
    Type: AWS::IAM::Role
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      Description: "CRCD Dashboard - Allows to add partitions to Athena and Glue, send logs to Cloudwatch, access Athena query results S3 bucket, receive from SQS queue. Each defined in separate policies"
      Path: "/"
      ManagedPolicyArns:
      - Ref: IAMManagedPolicyLambdaAthena
      - Ref: IAMManagedPolicyLambdaGlue
      - Ref: IAMManagedPolicyLambdaS3AthenaQueryResults
      - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      - Ref: IAMManagedPolicyLambdaS3ConfigObject
      - Ref: IAMManagedPolicySQSReceiveMessage

      MaxSessionDuration: 3600
      RoleName: "crcd-backfill-worker-role"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Action: "sts:AssumeRole"
          Effect: "Allow"
          Principal:
            Service: "lambda.amazonaws.com"
          Sid: "AllowLambdaAssumeRole"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  IAMManagedPolicySQSReceiveMessage:
    Type: AWS::IAM::ManagedPolicy
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-backfill-sqs-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that gives CRCD backfill Lambda execution role permission to receive SQS messages"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource: !GetAtt SQSBackfillingQueue.Arn
          Action:
          - "sqs:ReceiveMessage"
          - "sqs:DeleteMessage"
          - "sqs:GetQueueAttributes"
          Effect: "Allow"
          Sid: "MessagesFromSQS"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  IAMManagedPolicyLambdaAthena:
    Type: AWS::IAM::ManagedPolicy
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-backfill-athena-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that gives Athena permissions to CRCD backfill Lambda execution role"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource: !Sub "arn:${AWS::Partition}:athena:${AWS::Region}:${AWS::AccountId}:workgroup/${AthenaWorkgroupName}"
          Action:
          - "athena:StartQueryExecution"
          - "athena:GetQueryExecution"
          Effect: "Allow"
          Sid: "AthenaAccess"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  IAMManagedPolicyLambdaGlue:
    Type: AWS::IAM::ManagedPolicy
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-backfill-glue-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that gives Glue permissions to CRCD backfill Lambda execution role"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource:
          - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:table/${AthenaDatabaseName}/*"
          - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database/${AthenaDatabaseName}"
          - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:catalog"
          Action:
          - "glue:UpdatePartition"
          - "glue:GetTables"
          - "glue:GetTable"
          - "glue:GetPartitions"
          - "glue:GetPartition"
          - "glue:DeletePartition"
          - "glue:CreatePartition"
          - "glue:BatchGetPartition"
          - "glue:BatchDeletePartition"
          - "glue:BatchCreatePartition"
          Effect: "Allow"
          Sid: "GluePartitions"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  IAMManagedPolicyLambdaS3AthenaQueryResults:
    Type: AWS::IAM::ManagedPolicy
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-backfill-s3-athenaqueryresults-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that gives permissions on Athena query results S3 bucket to CRCD backfill Lambda execution role"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource:
          - !Sub "arn:${AWS::Partition}:s3:::${AthenaQueryResultBucketName}"
          - !Sub "arn:${AWS::Partition}:s3:::${AthenaQueryResultBucketName}/*"
          Action:
          - "s3:PutObject"
          - "s3:ListMultipartUploadParts"
          - "s3:ListBucket"
          - "s3:GetObject"
          - "s3:GetBucketLocation"
          Effect: "Allow"
          Sid: "S3AthenaQueryResults"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  IAMManagedPolicyLambdaS3ConfigObject:
    Type: AWS::IAM::ManagedPolicy
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-backfill-s3-configfile-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that allows CRCD backfill Lambda to read objects from the Config S3 bucket"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource:
          - !Sub "arn:${AWS::Partition}:s3:::${DashboardBucketName}"
          - !Sub "arn:${AWS::Partition}:s3:::${DashboardBucketName}/*"
          Action:
          - "s3:GetObject"
          - "s3:ListBucket"
          - "s3:ListBucketVersions"
          - "s3:GetObjectVersion"
          - "s3:GetLifecycleConfiguration"
          Effect: "Allow"
          Sid: "S3ConfigFileObject"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"


Outputs:
  SQSBackfillQueueArn:
    Description: ARN of the SQS Queue
    Value: !Ref SQSBackfillingQueue
