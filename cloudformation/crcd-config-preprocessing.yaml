AWSTemplateFormatVersion: '2010-09-09'
Description: 'Preprocessing AWS Config files using AWS Glue'


Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      # TODO create a group for input and output of glue job, unless it would become too big
      -
        Label:
          default: "AWS Config logging - the source of data for the pre-processing job is where your AWS Config files are delivered"
        Parameters:
          # TODO ask for the LogArchive account id and validate it is THE CURRENT account? - LogArchiveAccountId
          - LogArchiveBucketName
          # TODO support KMS encrypted log archive bucket - LogArchiveBucketKmsKeyArn
      -
        Label:
          default: "Dashboard bucket - destination bucket of the pre-processing job and source of data for the dashboard" 
        Parameters:
          - DashboardBucketName
      -
        Label:
          default: "Technical Parameters (DO NOT CHANGE)"
        Parameters:
          # - LambdaPartitioningFunctionName
          - PreProcessingGlueJobName
          - DynamoDBJobTrackingTableName
    
    ParameterLabels:
      LogArchiveBucketName:
        default: "Log Archive bucket"
      DashboardBucketName:
        default: "Dashboard bucket"
      PreProcessingGlueJobName:
        default: "CRCD Glue Job"
      DynamoDBJobTrackingTableName:
        default: "Dynamo DB tracking table"
          

Parameters:
  LogArchiveBucketName:
    Type: String
    Description: "Name of the Amazon S3 bucket collecting AWS Config files (Required)."
    MinLength: 1
    AllowedPattern: '^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$'
    ConstraintDescription: "Log Archive bucket name is missing or does not satisfy the Amazon S3 naming convention."

  DashboardBucketName:
    # TODO say it will prefix account ID and region taken from where the cfn is run
    Type: String
    Description: "Name of the Amazon S3 bucket that is used as source of data for the dashboard. The Dashboard bucket will be created with the name you specify here (Required)."
    AllowedPattern: '^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$'
    MinLength: 1
    Default: 'crcd-dashboard-bucket' 
    ConstraintDescription: "Dashboard bucket name is missing or does not satisfy the Amazon S3 naming convention."

  PreProcessingGlueJobName:
    Type: String
    Default: "crcd-config-file-preprocessing-job"
    MinLength: 1
    ConstraintDescription: "Required: name of the AWS Glue job that pre-processes AWS Config files."
    Description: "Name of the AWS Glue job that pre-processes AWS Config files (Required)."

  DynamoDBJobTrackingTableName:
    Type: String
    Default: 'CRCDPreprocessingJobTracking'
    MinLength: 1
    ConstraintDescription: "Required: name of the Amazon Dynamo DB table used to track the activities of the pre-processing Glue job."
    Description: "Name of the Amazon Dynamo DB table (Required)."

Rules:
  MandatoryLogArchiveBucketName:
    Assertions:
      - Assert: !Not
         - !Equals
          - !Ref LogArchiveBucketName
          - ''
        AssertDescription: "Log Archive bucket name is required"
  MandatoryDashboardBucketName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref DashboardBucketName
          - ''
        AssertDescription: "Dashboard bucket name is required"
  MandatoryPreProcessingGlueJobName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref PreProcessingGlueJobName
          - ''
        AssertDescription: "AWS Glue job name is required"

Resources:
  # S3 Buckets
  # TODO S3 notifications to trigger a lambda function same as the other CFN - hange the temp lambda name because CRCD will create itw own
  #LogArchiveBucket:
  #  Type: AWS::S3::Bucket
  #  Properties:
  #    BucketName: !Ref LogArchiveBucketName
  #    NotificationConfiguration:
  #      LambdaConfigurations:
  #        - Event: 's3:ObjectCreated:*'
  #          Filter:
  #            S3Key:
  #              Rules:
  #                - Name: suffix
  #                  Value: .json.gz
  #          Function: !GetAtt LambdaFunctionFlatteningJobTrigger.Arn


  DashboardBucket:
    # TODO, more properties to the bucket
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${DashboardBucketName}-${AWS::AccountId}-${AWS::Region}'
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      AccessControl: BucketOwnerFullControl
      OwnershipControls:
        Rules:
          - ObjectOwnership: BucketOwnerEnforced
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
        
  # DynamoDB Table
  CRCDJobTrackingTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Ref DynamoDBJobTrackingTableName
      AttributeDefinitions:
        - AttributeName: source_file
          AttributeType: S
        - AttributeName: job_run_id
          AttributeType: S
      KeySchema:
        - AttributeName: source_file
          KeyType: HASH
        - AttributeName: job_run_id
          KeyType: RANGE
      BillingMode: PAY_PER_REQUEST
      TimeToLiveSpecification:
        AttributeName: ttl
        Enabled: true

  # IAM Roles
  PreProcessingGlueJobRole:
    Type: AWS::IAM::Role
    #DependsOn:  # TODO I refer to this bucket on the policy below, maybe not needed, delete if it installs with these commented
    #     - DashboardBucket
    Properties:
      Description: "CRCD Dashboard - " # TODO description and name
      RoleName: "crcd-preprocessing-glue-job-role"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      Policies:
        - PolicyName: crcd-dynamodb-dashboard-bucket-policy # TODO old name GlueJobCustomPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:ListBucket
                Resource:
                  # Log Archive bucket is passed as name, Dashboard bucket created here
                  - !Sub '${DashboardBucket.Arn}/*'
                  - !GetAtt DashboardBucket.Arn
                  - !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}"
                  - !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}/*"
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                  - dynamodb:UpdateItem
                  - dynamodb:GetItem
                Resource: !GetAtt CRCDJobTrackingTable.Arn

  LambdaPreProcessingJobTriggerRole:
    Type: AWS::IAM::Role
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      RoleName: "crcd-lambda-preprocessing-job-trigger-role"
      Description: "CRCD Dashboard - Allows to trigger Glue jobs, send logs to Cloudwatch, receive objects from AWS Config bucket."
      Path: "/"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: crcd-glue-dynamodb-s3 
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - glue:StartJobRun
                Resource: !Sub 'arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:job/${FlatteningGlueJob}'
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                Resource: !GetAtt CRCDJobTrackingTable.Arn
              # TODO check this works, I added it manually
              - Effect: Allow
                Action:
                  - "s3:GetObject"
                  - "s3:ListBucket"
                  - "s3:ListBucketVersions"
                  - "s3:GetObjectVersion"
                  - "s3:GetLifecycleConfiguration"
                Resource: 
                  - !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}"
                  - !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}/*"


  # Lambda Function
  LambdaFunctionFlatteningJobTrigger:
    Type: AWS::Lambda::Function
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    DependsOn:
         - DashboardBucket
    Properties:
      FunctionName: "crcd-flattening-job-trigger" # TODO move this to a technical parameters section !Sub "${LambdaPartitioningFunctionName}" 
      Description: "CRCD Dashboard - Lambda function that triggers a JSON flattening Glue job when there are new AWS Config files"
      Handler: index.lambda_handler
      Role: !GetAtt IAMRoleLambdaFunctionFlatteningJobTrigger.Arn
      Runtime: python3.12
      Timeout: 30
      MemorySize: 128
      Environment:
        Variables:
          # TODO
          DYNAMODB_TRACKING_TABLE_NAME: !Ref DynamoDBJobTrackingTableName
          DEASHBOARD_BUCKET_NAME: !Ref DashboardBucket.
      Code:
        ZipFile: |
          import boto3
          import json
          from datetime import datetime
          import os

          def lambda_handler(event, context):
              glue = boto3.client('glue')
              # TODO the table name here must be passed as parameter to the lambda and taken from a 
              # do-not-change technical parameter of the CFN
              dynamodb = boto3.resource('dynamodb').Table('CRCDFlatteningJobTracking')


              # TODO this will be dynamic, hard coding for now
              DashboardBucketName = 'crcd-dashboard-bucket-058264555211-eu-north-1'
              
              records = event['Records']
              for record in records:
                  bucket = record['s3']['bucket']['name']
                  key = record['s3']['object']['key']
                  
                  timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
                  # Extract the filename from the full path
                  filename = os.path.basename(key)
                  # Remove the .json.gz extension if present
                  filename = filename.replace('.json.gz', '')

                  # TODO delete run_id = f"s3-trigger-{timestamp}"
                  run_id = f"{filename}-{timestamp}"
                  
                  # Record job start in DynamoDB
                  dynamodb.put_item(
                      Item={
                          'source_file': key,
                          'job_run_id': run_id,
                          'status': 'STARTED',
                          'start_time': datetime.now().isoformat(),
                          'ttl': int((datetime.now().timestamp()) + (90 * 24 * 60 * 60))  # 90 days TTL
                      }
                  )
                  
                  # Start Glue job
                  # TODO pass DashboardBucketName as parameter or env variable to the Lambda function
                  # TODO pass JobName as well, this must match permissions - was ConfigProcessingJob
                  try:
                      response = glue.start_job_run(
                          JobName='crcd-config-file-processing-job',
                          Arguments={
                              '--source_path': f"s3://{bucket}/{key}",
                              '--destination_path': f"s3://{DashboardBucketName}/",
                              '--CRCD_JOB_RUN_ID': run_id
                          }
                      )
                      
                      print(f"Started Glue job for bucket {bucket} on object {key} with run ID: {response['JobRunId']}")
                      
                  except Exception as e:
                      print(f"Error starting Glue job for {key}: {str(e)}")
                      raise
              
              return {
                  'statusCode': 200,
                  'body': json.dumps('Successfully processed S3 event')
              }
      

  # TODO log group for the lambda, when the name is in a technical parameters group
  #LambdaFunctionFlatteningJobTriggerLogGroup:
  # checkov:skip=CKV_AWS_158: No KMS encryption needed
  #  Type: AWS::Logs::LogGroup
  #  DeletionPolicy: "Delete"
  #  UpdateReplacePolicy: "Delete"
  #  Properties:
  #    LogGroupName: !Sub "/aws/lambda/${LambdaPartitioningFunctionName}"
  #    RetentionInDays: 14
  #  Metadata:
  #    cfn_nag:
  #      rules_to_suppress:
  #        - id: W84
  #          reason: "No KMS encryption needed"

  # S3 Bucket Permission for Lambda
  LambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref LambdaFunctionFlatteningJobTrigger
      Principal: s3.amazonaws.com
      SourceAccount: !Ref AWS::AccountId
      SourceArn: !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}"

  # -------------------------------------------------------------------------------------------------------------------------------------------------------
  # -------------------------------------------------------------------------------------------------------------------------------------------------------
  # Glue Job Script
  # Lambda triggered by CloudFormation to create the Glue script
  GlueScriptUploaderRole:
    Type: AWS::IAM::Role
    DependsOn:
         - DashboardBucket
    Properties:
      Description: "CRCD Dashboard - " # TODO description
      RoleName: "crcd-glue-script-uploader-lambda-role"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3DashboardBucketAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource: !Sub '${DashboardBucket.Arn}/crcd-scripts/*'

  GlueScriptUploader:
  # TODO see other lambda function properties deom cid-crcd-resources.yaml
  # TODO on delete remove the object? Anyway the bucket has all other config files and would need to be emptied.
    Type: AWS::Lambda::Function
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      FunctionName: "crcd-support-configure-glue-job"
      Description: "CRCD Dashboard - (one-time execution)" # TODO description
      Runtime: python3.12
      Timeout: 30
      MemorySize: 128
      Handler: index.lambda_handler
      Role: !GetAtt GlueScriptUploaderRole.Arn
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import json
          
          def lambda_handler(event, context):
              try:
                  if event['RequestType'] in ['Create', 'Update']:
                      s3 = boto3.client('s3')
                      bucket = event['ResourceProperties']['Bucket']
                      key = event['ResourceProperties']['Key']
                      content = event['ResourceProperties']['Content']
                      
                      # Upload the script to S3
                      s3.put_object(
                          Bucket=bucket,
                          Key=key,
                          Body=content,
                          ContentType='text/x-python'
                      )
                      
                      response_data = {
                          'Message': f'Successfully uploaded script to s3://{bucket}/{key}'
                      }
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, response_data)
                  
                  elif event['RequestType'] == 'Delete':
                      # Handle delete event if needed
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                      
              except Exception as e:
                  cfnresponse.send(event, context, cfnresponse.FAILED, {'Error': str(e)})
      

  GlueScript:
    # TODO check the strings that need to be CFN parameters in the final version
    Type: Custom::S3Upload
    DependsOn: DashboardBucket
    Properties:
      ServiceToken: !GetAtt GlueScriptUploader.Arn
      Bucket: !Ref DashboardBucket
      Key: !Sub 'crcd-scripts/${PreProcessingGlueJobName}-DO-NOT-DELETE.py'
      Content: |
        import sys
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        import boto3
        from datetime import datetime
        import os
        import json

        # Get job parameters
        args = getResolvedOptions(sys.argv, [
            'JOB_NAME',
            'source_path',
            'destination_path',
            'CRCD_JOB_RUN_ID'
        ])

        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)

        # Configure Spark for better GZIP processing
        spark.conf.set("spark.sql.files.maxPartitionBytes", 134217728)  # 128MB
        spark.conf.set("spark.sql.files.openCostInBytes", 134217728)  # 128MB
        spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
        spark.conf.set("spark.sql.adaptive.enabled", "true")

        # Initialize DynamoDB client
        dynamodb = boto3.resource('dynamodb')

        # TODO the table name here must be passed as parameter (if possible) and taken from a 
        # do-not-change technical parameter of the CFN
        table = dynamodb.Table('CRCDFlatteningJobTracking')

        def get_relative_path(source_path):
            """Extract the relative path from the full S3 path"""
            parts = source_path.replace('s3://', '').split('/', 1)
            if len(parts) > 1:
                return parts[1]
            return ''

        def get_destination_path(source_path, destination_base, filename):
            """Generate destination path maintaining the source directory structure"""
            relative_path = get_relative_path(source_path)
            dir_path = os.path.dirname(relative_path)
            
            if dir_path:
                return f"{destination_base.rstrip('/')}/{dir_path}/{filename}"
            return f"{destination_base.rstrip('/')}/{filename}"

        def update_job_status(status, processed_items=0, error_message=None):
            try:
                update_expression = "SET #status = :status, end_time = :end_time, processed_items = :processed_items"
                expression_values = {
                    ':status': status,
                    ':end_time': datetime.now().isoformat(),
                    ':processed_items': processed_items
                }
                
                if error_message:
                    update_expression += ", error_message = :error_message"
                    expression_values[':error_message'] = error_message

                source_file = get_relative_path(args['source_path'])
                
                table.update_item(
                    Key={
                        'source_file': source_file,
                        'job_run_id': args['CRCD_JOB_RUN_ID']
                    },
                    UpdateExpression=update_expression,
                    ExpressionAttributeNames={
                        '#status': 'status'
                    },
                    ExpressionAttributeValues=expression_values
                )
            except Exception as e:
                print(f"Error updating DynamoDB: {str(e)}")

          def process_file():
              try:
                  # Read the raw JSON content
                  sc_rdd = sc.wholeTextFiles(args['source_path'])
                  
                  processed_count = 0
                  
                  def sanitize_s3_name(name):
                      # Replace characters that could cause issues in S3 paths
                      if not name:
                          return "unknown"
                      
                      # Replace characters that could create folders or cause issues
                      replacements = {
                          '/': '-',
                          '\\': '-',
                          ':': '-',
                          '*': '-',
                          '?': '-',
                          '"': '-',
                          '<': '-',
                          '>': '-',
                          '|': '-',
                          ' ': '-',
                          '=': '-',
                          '@': '-',
                          '#': '-',
                          '$': '-',
                          '&': '-',
                          '{': '-',
                          '}': '-',
                          '[': '-',
                          ']': '-',
                          '`': '-',
                          "'": '-',
                          '!': '-',
                          '+': '-',
                          '^': '-',
                          ',': '-'
                      }
                      
                      for char, replacement in replacements.items():
                          name = name.replace(char, replacement)
                      
                      # Replace multiple consecutive dashes with a single dash
                      while '--' in name:
                          name = name.replace('--', '-')
                      
                      # Remove leading and trailing dashes
                      name = name.strip('-')
                      
                      return name if name else "unknown"
                  
                  def process_json_file(file_content):
                      nonlocal processed_count
                      # Parse the JSON content
                      json_content = json.loads(file_content[1])
                      
                      # Check if configurationItems exists and is not empty
                      if 'configurationItems' not in json_content:
                          raise ValueError("JSON file does not contain 'configurationItems' field")
                      
                      if not json_content['configurationItems']:
                          raise ValueError("'configurationItems' array is empty")
                      
                      # Get the common fields that need to be preserved
                      output_template = {
                          "fileVersion": json_content.get("fileVersion", "1.0"),
                          "configSnapshotId": json_content.get("configSnapshotId", ""),
                          "configurationItems": []
                      }
                      
                      # Process each configuration item
                      for item in json_content['configurationItems']:
                          # Create a new output object for this item
                          output_json = output_template.copy()
                          output_json['configurationItems'] = [item]
                          
                          resource_type = sanitize_s3_name(item.get('resourceType', '').replace('::', '-'))
                          resource_name = sanitize_s3_name(item.get('resourceName', ''))
                          run_id = args['CRCD_JOB_RUN_ID']
                          
                          filename = f"{resource_type}_{resource_name}_{run_id}.json.gz"
                          destination = get_destination_path(
                              args['source_path'],
                              args['destination_path'],
                              filename
                          )
                          
                          dest_parts = destination.replace('s3://', '').split('/', 1)
                          dest_bucket = dest_parts[0]
                          dest_key = dest_parts[1]
                          
                          # Convert JSON to bytes and compress
                          json_bytes = json.dumps(output_json, indent=2).encode('utf-8')
                          
                          import gzip
                          import io
                          
                          # Create a BytesIO object to hold the gzipped data
                          gzip_buffer = io.BytesIO()
                          
                          # Create a GzipFile object and write the JSON data
                          with gzip.GzipFile(mode='wb', fileobj=gzip_buffer) as gz:
                              gz.write(json_bytes)
                          
                          # Get the gzipped content
                          gzip_buffer.seek(0)
                          gzipped_content = gzip_buffer.getvalue()
                          
                          s3 = boto3.client('s3')
                          s3.put_object(
                              Bucket=dest_bucket,
                              Key=dest_key,
                              Body=gzipped_content,
                              ContentType='application/json',
                              ContentEncoding='gzip'
                          )
                          processed_count += 1
                          print(f"Saved {filename} to {destination}")
                  
                  # Process all files
                  for file_content in sc_rdd.collect():
                      process_json_file(file_content)
                      
                  update_job_status('COMPLETED', processed_count)
                  return processed_count
                  
              except ValueError as ve:
                  # Handle specific validation errors
                  error_message = str(ve)
                  print(f"Validation error: {error_message}")
                  update_job_status('FAILED', 0, error_message)
                  raise
              except Exception as e:
                  # Handle other unexpected errors
                  error_message = f"Unexpected error: {str(e)}"
                  print(error_message)
                  update_job_status('FAILED', 0, error_message)
                  raise

        try:
            items_processed = process_file()
            print(f"Successfully processed {items_processed} items")
            job.commit()
        except Exception as e:
            print(f"Error processing file: {str(e)}")
            raise

  # Glue Job
  # TODO check more suggested --conf settings to add to DefaultArguments
  # --conf spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
  # --conf spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs=true
  # --conf spark.hadoop.mapreduce.task.timeout=300000
  # --conf spark.task.maxFailures=10
  # --conf spark.network.timeout=800s
  FlatteningGlueJob:
    Type: AWS::Glue::Job
    DependsOn: GlueScript
    Properties:
      Name: !Sub "${PreProcessingGlueJobName}"  #crcd-config-file-processing-job moved to technical parameter
      Role: !GetAtt PreProcessingGlueJobRole.Arn
      Command:
        Name: glueetl
        PythonVersion: '3'
        # TODO this fails if the bucket name is  a prefix
        ScriptLocation: !Sub 's3://${DashboardBucketName}/crcd-scripts/${PreProcessingGlueJobName}-DO-NOT-DELETE.py'
      DefaultArguments:
        '--enable-metrics': 'true'
        '--enable-continuous-cloudwatch-log': 'true'
        '--job-language': 'python'
        '--enable-spark-ui': 'true'
        '--enable-job-insights': 'true'
        '--enable-glue-datacatalog': 'true'
        '--conf': 'spark.driver.maxResultSize=2g --conf spark.kryoserializer.buffer.max=512m'
      ExecutionProperty:
        MaxConcurrentRuns: 10
      GlueVersion: '4.0'
      MaxRetries: 0
      Timeout: 60
      NumberOfWorkers: 5
      WorkerType: G.2X

  # CloudWatch Log Group
  # TODO probably not needed, Glue logs in a weird way that I haveto understand
  #GlueJobLogGroup:
  #  Type: AWS::Logs::LogGroup
  #  Properties:
  #    LogGroupName: !Sub '/aws-glue/jobs/${FlatteningGlueJob}'
  #    RetentionInDays: 30

Outputs:
  # TODO delete
  #LogArchiveBucketName:
  #  Description: Name of the source S3 bucket
  #  Value: !Ref LogArchiveBucket

  DashboardBucketName:
    Description: Name of the destination S3 bucket
    Value: !Ref DashboardBucket

  DynamoDBTableName:
    Description: Name of the DynamoDB tracking table
    Value: !Ref CRCDJobTrackingTable

  GlueJobName:
    Description: Name of the Glue job
    Value: !Ref FlatteningGlueJob

  LambdaFunctionFlatteningJobTriggerArn:
    Description: ARN of the trigger Lambda function
    Value: !GetAtt LambdaFunctionFlatteningJobTrigger.Arn
