AWSTemplateFormatVersion: '2010-09-09'
Description: 'Falttening AWS Config files using AWS Glue'

# TODO support KMS encrypted log archive bucket
Parameters:
  LogArchiveBucketName:
    Type: String
    Description: "Name of the Amazon S3 bucket collecting AWS Config files (Required)."
    MinLength: 1
    AllowedPattern: '^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$'
    ConstraintDescription: "Log Archive bucket name is missing or does not satisfy the Amazon S3 naming convention."

  DashboardBucketName:
    # TODO say it will prefic account ID and region taken from where the cfn is run
    Type: String
    Description: "Name of the Amazon S3 bucket that is used as source of data for the dashboard. The Dashboard bucket will be created with the name you specify here (Required)."
    AllowedPattern: '^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$'
    MinLength: 1
    Default: 'crcd-dashboard-bucket' 
    ConstraintDescription: "Dashboard bucket name is missing or does not satisfy the Amazon S3 naming convention."

Rules:
  MandatoryLogArchiveBucketName:
    Assertions:
      - Assert: !Not
         - !Equals
          - !Ref LogArchiveBucketName
          - ''
        AssertDescription: "Log Archive bucket name is required"
  MandatoryDashboardBucketName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref DashboardBucketName
          - ''
        AssertDescription: "Dashboard bucket name is required"


Resources:
  # S3 Buckets
  # TODO notifications with a lambda function same as the other CFN
  #LogArchiveBucket:
  #  Type: AWS::S3::Bucket
  #  Properties:
  #    BucketName: !Ref LogArchiveBucketName
  #    NotificationConfiguration:
  #      LambdaConfigurations:
  #        - Event: 's3:ObjectCreated:*'
  #          Filter:
  #            S3Key:
  #              Rules:
  #                - Name: suffix
  #                  Value: .json.gz
  #          Function: !GetAtt LambdaFunctionFlatteningJobTrigger.Arn


  DashboardBucket:
    # TODO, more properties to the bucket
    # TODO in bucket name -${AWS::AccountId}-${AWS::Region}
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref DashboardBucketName
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      AccessControl: BucketOwnerFullControl
      OwnershipControls:
        Rules:
          - ObjectOwnership: BucketOwnerEnforced
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
        
  # DynamoDB Table
  CRCDJobTrackingTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: CRCDFlatteningJobTracking
      AttributeDefinitions:
        - AttributeName: source_file
          AttributeType: S
        - AttributeName: job_run_id
          AttributeType: S
      KeySchema:
        - AttributeName: source_file
          KeyType: HASH
        - AttributeName: job_run_id
          KeyType: RANGE
      BillingMode: PAY_PER_REQUEST
      TimeToLiveSpecification:
        AttributeName: ttl
        Enabled: true

  # IAM Roles
  FlatteningGlueJobRole:
    Type: AWS::IAM::Role
    DependsOn:
         - DashboardBucket
    Properties:
      Description: "CRCD Dashboard - " # TODO description and name
      RoleName: "crcd-glue-job-role"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      Policies:
        - PolicyName: GlueJobCustomPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:ListBucket
                Resource:
                  #- !Sub '${LogArchiveBucket.Arn}/*'
                  - !Sub '${DashboardBucket.Arn}/*'
                  #- !GetAtt LogArchiveBucket.Arn
                  - !GetAtt DashboardBucket.Arn
                  - !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}"
                  - !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}/*"
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                  - dynamodb:UpdateItem
                  - dynamodb:GetItem
                Resource: !GetAtt CRCDJobTrackingTable.Arn

  IAMRoleLambdaFunctionFlatteningJobTrigger:
    Type: AWS::IAM::Role
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      Description: "CRCD Dashboard - Allows to trigger Glue jobs, send logs to Cloudwatch, receive objects from Config bucket."
      Path: "/"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: CRCDLambdaFunctionFlatteningJobTriggerCustomPolicy # TODO crcd-lambda-trigger-custom-policy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - glue:StartJobRun
                Resource: !Sub 'arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:job/${FlatteningGlueJob}'
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                Resource: !GetAtt CRCDJobTrackingTable.Arn

  # Lambda Function
  LambdaFunctionFlatteningJobTrigger:
    Type: AWS::Lambda::Function
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    DependsOn:
         - DashboardBucket
    Properties:
      FunctionName: "crcd-flattening-job-trigger" # TODO move this to a technical parameters section !Sub "${LambdaPartitioningFunctionName}" 
      Description: "CRCD Dashboard - Lambda function that triggers a JSON flattening Glue job when there are new AWS Config files"
      Handler: index.lambda_handler
      Role: !GetAtt IAMRoleLambdaFunctionFlatteningJobTrigger.Arn
      Runtime: python3.12
      Timeout: 30
      MemorySize: 128
      Code:
        ZipFile: |
          import boto3
          import json
          from datetime import datetime

          def lambda_handler(event, context):
              glue = boto3.client('glue')
              dynamodb = boto3.resource('dynamodb').Table('GlueJobTracking')
              
              records = event['Records']
              for record in records:
                  bucket = record['s3']['bucket']['name']
                  key = record['s3']['object']['key']
                  
                  timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')
                  run_id = f"s3-trigger-{timestamp}"
                  
                  # Record job start in DynamoDB
                  dynamodb.put_item(
                      Item={
                          'source_file': key,
                          'job_run_id': run_id,
                          'status': 'STARTED',
                          'start_time': datetime.now().isoformat(),
                          'ttl': int((datetime.now().timestamp()) + (90 * 24 * 60 * 60))  # 90 days TTL
                      }
                  )
                  
                  # Start Glue job
                  try:
                      response = glue.start_job_run(
                          JobName='ConfigProcessingJob',
                          Arguments={
                              '--source_path': f"s3://{bucket}/{key}",
                              '--destination_path': f"s3://{DashboardBucketName}/",
                              '--JOB_RUN_ID': run_id
                          }
                      )
                      
                      print(f"Started Glue job for {key} with run ID: {response['JobRunId']}")
                      
                  except Exception as e:
                      print(f"Error starting Glue job for {key}: {str(e)}")
                      raise
              
              return {
                  'statusCode': 200,
                  'body': json.dumps('Successfully processed S3 event')
              }
      

  # TODO log group for the lambda, when the name is in a technical parameters group
  #LambdaFunctionFlatteningJobTriggerLogGroup:
  # checkov:skip=CKV_AWS_158: No KMS encryption needed
  #  Type: AWS::Logs::LogGroup
  #  DeletionPolicy: "Delete"
  #  UpdateReplacePolicy: "Delete"
  #  Properties:
  #    LogGroupName: !Sub "/aws/lambda/${LambdaPartitioningFunctionName}"
  #    RetentionInDays: 14
  #  Metadata:
  #    cfn_nag:
  #      rules_to_suppress:
  #        - id: W84
  #          reason: "No KMS encryption needed"

  # S3 Bucket Permission for Lambda
  LambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref LambdaFunctionFlatteningJobTrigger
      Principal: s3.amazonaws.com
      SourceAccount: !Ref AWS::AccountId
      SourceArn: !Sub "arn:${AWS::Partition}:s3:::${LogArchiveBucketName}"

  # -------------------------------------------------------------------------------------------------------------------------------------------------------
  # -------------------------------------------------------------------------------------------------------------------------------------------------------
  # Glue Job Script
  # Lambda triggered by CloudFormation to create the Glue script
  GlueScriptUploaderRole:
    Type: AWS::IAM::Role
    DependsOn:
         - DashboardBucket
    Properties:
      Description: "CRCD Dashboard" # TODO description
      RoleName: "crcd-glue-script-uploader-lambda-role"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3DashboardBucketAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource: !Sub '${DashboardBucket.Arn}/crcd-scripts/*'

  GlueScriptUploader:
  # TODO see other lambda function properties deom cid-crcd-resources.yaml
  # TODO on delete remove the object
    Type: AWS::Lambda::Function
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      FunctionName: "crcd-support-configure-glue-job"
      Description: "CRCD Dashboard - (one-time execution)" # TODO description
      Runtime: python3.12
      Timeout: 30
      MemorySize: 128
      Handler: index.lambda_handler
      Role: !GetAtt GlueScriptUploaderRole.Arn
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import json
          
          def lambda_handler(event, context):
              try:
                  if event['RequestType'] in ['Create', 'Update']:
                      s3 = boto3.client('s3')
                      bucket = event['ResourceProperties']['Bucket']
                      key = event['ResourceProperties']['Key']
                      content = event['ResourceProperties']['Content']
                      
                      # Upload the script to S3
                      s3.put_object(
                          Bucket=bucket,
                          Key=key,
                          Body=content,
                          ContentType='text/x-python'
                      )
                      
                      response_data = {
                          'Message': f'Successfully uploaded script to s3://{bucket}/{key}'
                      }
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, response_data)
                  
                  elif event['RequestType'] == 'Delete':
                      # Handle delete event if needed
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                      
              except Exception as e:
                  cfnresponse.send(event, context, cfnresponse.FAILED, {'Error': str(e)})
      

  GlueScript:
    Type: Custom::S3Upload
    DependsOn: DashboardBucket
    Properties:
      ServiceToken: !GetAtt GlueScriptUploader.Arn
      Bucket: !Ref DashboardBucket
      Key: crcd-scripts/crcd_flattening_job.py
      Content: |
        import sys
        from awsglue.transforms import *
        from awsglue.utils import getResolvedOptions
        from pyspark.context import SparkContext
        from awsglue.context import GlueContext
        from awsglue.job import Job
        from pyspark.sql.functions import *
        import boto3
        from datetime import datetime
        import os
        import json

        # Get job parameters
        args = getResolvedOptions(sys.argv, [
            'JOB_NAME',
            'source_path',
            'destination_path',
            'JOB_RUN_ID'
        ])

        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)

        # Configure Spark for better GZIP processing
        spark.conf.set("spark.sql.files.maxPartitionBytes", 134217728)  # 128MB
        spark.conf.set("spark.sql.files.openCostInBytes", 134217728)  # 128MB
        spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
        spark.conf.set("spark.sql.adaptive.enabled", "true")

        # Initialize DynamoDB client
        dynamodb = boto3.resource('dynamodb')
        table = dynamodb.Table('GlueJobTracking')

        def get_relative_path(source_path):
            """Extract the relative path from the full S3 path"""
            parts = source_path.replace('s3://', '').split('/', 1)
            if len(parts) > 1:
                return parts[1]
            return ''

        def get_destination_path(source_path, destination_base, filename):
            """Generate destination path maintaining the source directory structure"""
            relative_path = get_relative_path(source_path)
            dir_path = os.path.dirname(relative_path)
            
            if dir_path:
                return f"{destination_base.rstrip('/')}/{dir_path}/{filename}"
            return f"{destination_base.rstrip('/')}/{filename}"

        def update_job_status(status, processed_items=0, error_message=None):
            try:
                update_expression = "SET #status = :status, end_time = :end_time, processed_items = :processed_items"
                expression_values = {
                    ':status': status,
                    ':end_time': datetime.now().isoformat(),
                    ':processed_items': processed_items
                }
                
                if error_message:
                    update_expression += ", error_message = :error_message"
                    expression_values[':error_message'] = error_message

                source_file = get_relative_path(args['source_path'])
                
                table.update_item(
                    Key={
                        'source_file': source_file,
                        'job_run_id': args['JOB_RUN_ID']
                    },
                    UpdateExpression=update_expression,
                    ExpressionAttributeNames={
                        '#status': 'status'
                    },
                    ExpressionAttributeValues=expression_values
                )
            except Exception as e:
                print(f"Error updating DynamoDB: {str(e)}")

        def process_file():
            try:
                df = spark.read \
                    .option("multiLine", True) \
                    .option("compression", "gzip") \
                    .option("mode", "PERMISSIVE") \
                    .option("columnNameOfCorruptRecord", "_corrupt_record") \
                    .json(args['source_path'])
                
                if "_corrupt_record" in df.columns:
                    corrupt_records = df.filter(col("_corrupt_record").isNotNull()).count()
                    if corrupt_records > 0:
                        raise Exception(f"Found {corrupt_records} corrupt records in the input file")
                
                exploded_df = df.select(explode('configurationItems').alias('item'))
                num_partitions = max(1, int(exploded_df.count() / 1000))
                exploded_df = exploded_df.repartition(num_partitions)
                
                processed_count = 0
                
                def save_item(row):
                    nonlocal processed_count
                    item = row.asDict()['item']
                    resource_type = item.get('resourceType', '').replace('::', '-')
                    resource_name = item.get('resourceName', '')
                    run_id = args['JOB_RUN_ID']
                    
                    filename = f"{resource_type}_{resource_name}_{run_id}.json"
                    destination = get_destination_path(
                        args['source_path'],
                        args['destination_path'],
                        filename
                    )
                    
                    dest_parts = destination.replace('s3://', '').split('/', 1)
                    dest_bucket = dest_parts[0]
                    dest_key = dest_parts[1]
                    
                    s3 = boto3.client('s3')
                    s3.put_object(
                        Bucket=dest_bucket,
                        Key=dest_key,
                        Body=json.dumps(item, indent=2),
                        ContentType='application/json'
                    )
                    processed_count += 1
                    print(f"Saved {filename} to {destination}")
                
                exploded_df.foreachPartition(lambda partition: [save_item(row) for row in partition])
                update_job_status('COMPLETED', processed_count)
                return processed_count
                
            except Exception as e:
                update_job_status('FAILED', 0, str(e))
                raise

        try:
            items_processed = process_file()
            print(f"Successfully processed {items_processed} items")
            job.commit()
        except Exception as e:
            print(f"Error processing file: {str(e)}")
            raise

  # Glue Job
  FlatteningGlueJob:
    Type: AWS::Glue::Job
    DependsOn: GlueScript
    Properties:
      Name: crcd-config-file-processing-job # ConfigProcessingJob
      Role: !GetAtt FlatteningGlueJobRole.Arn
      Command:
        Name: glueetl
        PythonVersion: '3'
        ScriptLocation: !Sub 's3://${DashboardBucketName}/crcd-scripts/crcd_flattening_job.py'
      DefaultArguments:
        '--enable-metrics': 'true'
        '--enable-continuous-cloudwatch-log': 'true'
        '--job-language': 'python'
        '--enable-spark-ui': 'true'
        '--enable-job-insights': 'true'
        '--enable-glue-datacatalog': 'true'
        '--conf': 'spark.driver.maxResultSize=2g --conf spark.kryoserializer.buffer.max=512m'
      ExecutionProperty:
        MaxConcurrentRuns: 10
      GlueVersion: '4.0'
      MaxRetries: 0
      Timeout: 60
      NumberOfWorkers: 5
      WorkerType: G.2X

  # CloudWatch Log Group
  GlueJobLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws-glue/jobs/${FlatteningGlueJob}'
      RetentionInDays: 30

Outputs:
  # TODO delete
  #LogArchiveBucketName:
  #  Description: Name of the source S3 bucket
  #  Value: !Ref LogArchiveBucket

  DashboardBucketName:
    Description: Name of the destination S3 bucket
    Value: !Ref DashboardBucket

  DynamoDBTableName:
    Description: Name of the DynamoDB tracking table
    Value: !Ref CRCDJobTrackingTable

  GlueJobName:
    Description: Name of the Glue job
    Value: !Ref FlatteningGlueJob

  LambdaFunctionFlatteningJobTriggerArn:
    Description: ARN of the trigger Lambda function
    Value: !GetAtt LambdaFunctionFlatteningJobTrigger.Arn
