# ------------------------------------------------------------------------------------
# Launch on Dashboard account, same region of CRCD resources 
# Solution that iterates through the Dashboard bucket and creates Glue/Athena 
# partitions for your historical AWS Config records
# ------------------------------------------------------------------------------------

AWSTemplateFormatVersion: '2010-09-09'
Description: "CRCD Dashboard - Backfill function: iterates through the Dashboard S3 bucket and recreates partitions on the Glue/Athena table"

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      -
        Label:
          default: "Dashboard resources - where you deploy the dashboard and where its data is" 
        Parameters:
          - DashboardBucketName
          - AthenaQueryResultBucketName
          - AWSOrganizationID
      -
        Label:
          default: "Technical Parameters (DO NOT CHANGE)"
        Parameters:
          - AthenaWorkgroupName
          - AthenaDatabaseName
          - AthenaTableName
          - LambdaBackfillProducerFunctionName
          - LambdaBackfillWorkerFunctionName 
    
    ParameterLabels:
      AthenaWorkgroupName:
        default: "Athena workgroup"
      AthenaDatabaseName:
        default: "Athena database"
      AthenaQueryResultBucketName:
        default: "Name of the Athena query results bucket"
      AthenaTableName:
        default: "Athena table for AWS Config data"
      LambdaBackfillProducerFunctionName:
        default: "AWS Lambda function that finds the AWS Config prefixes that need to be added as partition to the dashboard"
      LambdaBackfillWorkerFunctionName:
        default: "AWS Lambda function that creates partitions"
      DashboardBucketName:
        default: "Dashboard bucket"
      AWSOrganizationID:
        default: "AWS Organization ID"

Parameters:
  AWSOrganizationID:
    Type: "String"
    Description: "The AWS Organization ID. Leave empty if you run this template on a standalone account."

  DashboardBucketName:
    Type: "String"
    Description: "Name of the Amazon S3 bucket that is used as source of data for the dashboard (Required)."
    AllowedPattern: '^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$'
    MinLength: 1
    ConstraintDescription: "Dashboard bucket name is missing or does not satisfy the Amazon S3 naming convention."

  AthenaQueryResultBucketName:
    Type: "String"
    Description: "This Amazon S3 bucket was created with the CRCD resources CloudFormation template. Write here the value of AthenaQueryResultBucketName in the output section of the template  (Required)."
    # 64 characters in the bucket name
    AllowedPattern: '^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$'
    MinLength: 1
    ConstraintDescription: "Required: Name of the Athena query results bucket."

  AthenaWorkgroupName:
    Type: "String"
    Default: "crcd-dashboard"
    Description: "The Athena workgroup for the dashboard (Required)."
    MinLength: 1
    ConstraintDescription: "Required: Athena workgroup"

  AthenaDatabaseName:
    Type: "String"
    Default: "cid_crcd_database"
    Description: "The Athena/Glue database for the dashboard (Required)."
    MinLength: 1
    # The name for an Athena database
    # Max 255 characters cannot have the symbol '-' and must have lowercase character, '_' is accepted
    # This value is not meant to be changed by the user, but we'll add the allowed pattern anyway
    AllowedPattern: '^[a-z0-9][a-z0-9_]{0,253}[a-z0-9]$'
    ConstraintDescription: "Required: Athena database"

  AthenaTableName:
    Type: "String"
    Default: "cid_crcd_config"
    Description: "The name that will be assigned to the Athena table that contains the AWS Config data for the dashboard (Required)."
    MinLength: 1
    ConstraintDescription: "Required: Athena table for AWS Config data."

  LambdaBackfillProducerFunctionName:
    Type: "String"
    Default: "crcd-config-backfill-producer"
    MinLength: 1
    ConstraintDescription: "Required: AWS Lambda function that scans the Dashboard bucket."
    Description: "AWS Lambda function that scans the Dashboard bucket and identifies all the AWS Config files (Required)."

  LambdaBackfillWorkerFunctionName:
    Type: "String"
    Default: "crcd-config-backfill-worker"
    MinLength: 1
    ConstraintDescription: "Required: AWS Lambda function that partitions AWS Config files."
    Description: "Name of the AWS Lambda function that partitions AWS Config files (Required)."

Rules:
  MandatoryDashboardBucketName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref DashboardBucketName
          - ''
        AssertDescription: "Dashboard bucket name is required"

  MandatoryAthenaDatabase:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref AthenaDatabaseName
          - ''
        AssertDescription: "Athena database is required"
  
  MandatoryAthenaQueryResultBucketName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref AthenaQueryResultBucketName
          - ''
        AssertDescription: "Athena query result bucket name is required"
  
  MandatoryAthenaWorkgroup:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref AthenaWorkgroupName
          - ''
        AssertDescription: "Athena workgroup is required"
  
  MandatoryAthenaTable:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref AthenaTableName
          - ''
        AssertDescription: "Athena table name is required"

  MandatoryLambdaBackfillProducerFunctionName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref LambdaBackfillProducerFunctionName
          - ''
        AssertDescription: "Lambda Backfill Producer function name is required"

  MandatoryLambdaBackfillWorkerFunctionName:
    Assertions:
      - Assert: !Not
        - !Equals
          - !Ref LambdaBackfillWorkerFunctionName
          - ''
        AssertDescription: "Lambda Backfill Worker function name is required"

Resources:

  # SQS Queue
  # When Lambda receives a batch, the messages stay in the queue but are hidden for the length of the queue's visibility timeout. 
  # If your function successfully processes all messages in the batch, Lambda deletes the messages from the queue. 
  # By default, if your function encounters an error while processing a batch, all messages in that batch become visible in the queue again after the visibility timeout expires.
  SQSBackfillingQueue:
  # checkov:skip=CKV_AWS_27: We set SqsManagedSseEnabled: true
    Type: AWS::SQS::Queue
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      QueueName: crcd-backfill-sqs-queue
      # Worker Lambda has 600 seconds timeout
      # Best practice: set the queue's visibility timeout to at least six times the timeout that you configure on your function
      VisibilityTimeout: 3600
      MessageRetentionPeriod: 345600 # 4 days
      SqsManagedSseEnabled: true
      DelaySeconds: 60 # some Log Archive buckets may cointain many files, we give the worker Lambda some more time to process them
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W48
            reason: "KMS encryption specified in SqsManagedSseEnabled = true"


  # Grant Permissions to Lambda Producer function to write to the SQS Queue
  SQSQueuePolicy:
    Type: AWS::SQS::QueuePolicy
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              AWS: '*'
            Action: 'sqs:SendMessage'
            Resource: !GetAtt SQSBackfillingQueue.Arn
            Condition:
              ArnEquals:
                'aws:SourceArn': !GetAtt LambdaBackfillProducerFunction.Arn
      Queues:
        - !Ref SQSBackfillingQueue
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: F21 # SQS Queue policy should not allow * principal
            reason: "Condition ArnEquals limits the principal allowed to send messages to the queue"

  # IAM Role for Lambda
  LambdaBackfillProducerFunctionRole:
    Type: AWS::IAM::Role
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      RoleName: "crcd-backfill-producer-role"
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: AllowSQSPublishS3Iteration
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: 'sqs:SendMessage'
                Resource: !GetAtt SQSBackfillingQueue.Arn
              - Effect: Allow
                Action:
                  - 's3:ListBucket'
                Resource:
                  - !Sub 'arn:${AWS::Partition}:s3:::${DashboardBucketName}'
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Role names are as per our design"

  LambdaBackfillProducerFunctionLogGroup:
  # checkov:skip=CKV_AWS_158: No KMS encryption needed
    Type: AWS::Logs::LogGroup
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaBackfillProducerFunctionName}"
      RetentionInDays: 14
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W84
            reason: "No KMS encryption needed"
  
  # Lambda Function that produces the list of AWS Config objects that must be partitioned
  LambdaBackfillProducerFunction:
  # checkov:skip=CKV_AWS_115: Cuncurrency limit needs are customer dependent. This lambda is run only once, we don't anticipate the need for reserved concurrency for the moment.
  # checkov:skip=CKV_AWS_173: Environment variables are not sensitive.
  # checkov:skip=CKV_AWS_116: No DQL needed.
    Type: AWS::Lambda::Function
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      FunctionName: !Sub "${LambdaBackfillProducerFunctionName}"
      Description: "CRCD Dashboard Backfill Producer - Lambda function that scans your Dashboard bucket to identify AWS Config objects that will be displayed on the dashboard"
      Runtime: python3.12
      Timeout: 900
      MemorySize: 128
      EphemeralStorage:
        Size: 512
      Role: !GetAtt LambdaBackfillProducerFunctionRole.Arn
      Environment:
        Variables:
          BUCKET_NAME_VAR: !Ref DashboardBucketName
          SQS_QUEUE_URL_VAR: !Ref SQSBackfillingQueue
          ORGANIZATION_ID: !Ref AWSOrganizationID

      Handler: index.lambda_handler
      Architectures:
        - "x86_64"
      TracingConfig:
        Mode: "Active"
      VpcConfig:
        SecurityGroupIds: []
        SubnetIds: []
        Ipv6AllowedForDualStack: false
      FileSystemConfigs: []
      LoggingConfig:
        LogFormat: "Text"
      RuntimeManagementConfig:
        UpdateRuntimeOn: "Auto"
      Code:
        ZipFile: |
          # AWS Config Resource Compliance Dashboard
          # Backfilling producer function, scans your Amazon S3 dashboard bucket and finds all prefixes related to AWS Config.
          # The prefix structure of an AWS Config file is as follows (for an AWS Config Snapshot): 
          #   ORG-ID/AWSLogs/ACCOUNT-NUMBER/Config/REGION/YYYY/MM/DD/ConfigSnapshot/objectname.json.gz
          # For performance reasons, this function identifies all AWS Config prefixes down to REGION.
          # These prefixes are sent to an SQS queue that will trigger another function to add them to the dashboard data.

          import boto3
          import json
          import os
          import re
          from collections import deque

          # Reads mandatory environment variables
          DASHBOARD_BUCKET_NAME = os.environ["BUCKET_NAME_VAR"]
          SQS_QUEUE = os.environ["SQS_QUEUE_URL_VAR"]
          ORGANIZATION_ID = os.environ.get("ORGANIZATION_ID", "")

          # SQS and S3 client
          sqs = boto3.client('sqs')
          s3 = boto3.client('s3')

          LOGGING_ON = False  # enables additional logging to CloudWatch

          # This regular expressions pattern is compatible with how ControlTower Config logs AND also with how Config Logs are stored in S3 in standalone account
          # Structure for Config Snapshots: ORG-ID/AWSLogs/ACCOUNT-NUMBER/Config/REGION/YYYY/MM/DD/ConfigSnapshot/objectname.json.gz
          # Object name follows this pattern: ACCOUNT-NUMBER_Config_REGION_ConfigSnapshot_TIMESTAMP-YYYYMMDDHHMMSS_RANDOM-TEXT.json.gz
          # For example: 123412341234_Config_eu-north-1_ConfigSnapshot_20240306T122755Z_09c5h4kc-3jc7-4897-830v-d7a858325638.json.gz
          # This function needs just to identify the folders until the region, then the processing part can 
          # generate the dates and add a partition in case the S3 path exists
          PATTERN_REGION = r'^(?P<org_id>[\w-]+)?/?AWSLogs/(?P<account_id>\d+)/Config/(?P<region>[\w-]+)/$'
          PATTERN_REGION_COMPILED = re.compile(PATTERN_REGION)

          def lambda_handler(event, context):
              # get the ORG-ID from env variables and then build the path that points to CloudTrail files, 
              # must work on a single account too
              exclude = None
              if ORGANIZATION_ID:
                  exclude = f'{ORGANIZATION_ID}/AWSLogs/{ORGANIZATION_ID}'
                  print(f'Excluding path: {exclude}')

              prefix = ''
              if ORGANIZATION_ID:
                  # We can directly start from this and skip 2 iterations
                  # The default value starts from the beginning of the bucket
                  prefix = f'{ORGANIZATION_ID}/AWSLogs'

              try:
                  all_prefixes = set()
                  prefixes_to_explore = deque([prefix])
                  paginator = s3.get_paginator('list_objects_v2')

                  while prefixes_to_explore:
                      current_prefix = prefixes_to_explore.popleft()
                      
                      # Buckets with many objects, or that also store CloudTrail files will make the Lambda time out 
                      # before finding all the relevant AWS Config records
                      # Here we use the list_objects_v2 API with the delimiter parameter
                      page_iterator = paginator.paginate(
                          Bucket=DASHBOARD_BUCKET_NAME,
                          Prefix=current_prefix,
                          Delimiter='/'
                      )

                      for page in page_iterator:
                          if 'CommonPrefixes' in page:
                              for common_prefix in page['CommonPrefixes']:
                                  prefix_path = common_prefix['Prefix']

                                  # these prefixes do not contain AWS Config files
                                  if not ((exclude and prefix_path.startswith(exclude)) 
                                          or ('/OversizedChangeNotification/' in prefix_path)
                                          or ('/CloudTrail' in prefix_path)
                                          or ('/CloudTrail-Digest' in prefix_path)
                                      ):

                                      # avoiding duplicates
                                      if prefix_path not in all_prefixes:
                                          if matches_config_pattern_region(prefix_path):
                                              if LOGGING_ON: print(f'Found Config prefix: {prefix_path}')
                                              all_prefixes.add(prefix_path)
                                              # send it directly to SQS
                                              payload = {
                                                  "Records": [{
                                                      "s3": {
                                                          "object": {"key": prefix_path},
                                                          "bucket": {"name": DASHBOARD_BUCKET_NAME}
                                                      }
                                                  }]
                                              }
                                              sqs.send_message(
                                                  QueueUrl=SQS_QUEUE, 
                                                  MessageBody=json.dumps(payload)
                                              )
                                          else:
                                              # still a possibly valid AWS Config prefix that needs scanning
                                              if LOGGING_ON: print(f'Exploring prefix: {prefix_path}')
                                              prefixes_to_explore.append(prefix_path)
                                  else:
                                      # the folders checked above are always excluded
                                      if LOGGING_ON: print(f'Excluding prefix: {prefix_path}')
                  
                  print(f'Found {len(all_prefixes)} prefixes')
                  if LOGGING_ON: print(all_prefixes)

                  return {
                      'statusCode': 200,
                      'body': {
                          'prefixes': sorted(list(all_prefixes)),
                          'total_count': len(all_prefixes)
                      }
                  }

              except Exception as e:
                  print(f'Error: {str(e)}')
                  return {
                      'statusCode': 500,
                      'body': f'Error: {str(e)}'
                  }

          def matches_config_pattern_region(key: str) -> bool:
              """Two-stage check for Config files"""
              # Quick check before expensive regex
              if not ('/Config/' in key):
                  return False

              # Only perform regex if basic pattern matches
              return bool(re.match(PATTERN_REGION_COMPILED, key))
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W92
            reason: "This function does not need reserved concurrent executions"



# ------------------------------------------------------------------------------------------------------------------------------------------------------------
# Worker
# ------------------------------------------------------------------------------------------------------------------------------------------------------------

  # Invoke worker Lambda Function from SQS Queue
  SQSEventSource:
    Type: AWS::Lambda::EventSourceMapping
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      EventSourceArn: !GetAtt SQSBackfillingQueue.Arn
      FunctionName: !Ref LambdaBackfillWorkerFunction
      # The maximum number of records in each batch that Lambda pulls from your stream or queue and sends to your function. Lambda passes all of the records in the batch to the function in a single call, up to the payload limit for synchronous invocation (6 MB).
      # Amazon Simple Queue Service – Default 10. For standard queues the max is 10,000. For FIFO queues the max is 10.
      BatchSize: 10
      # (Amazon SQS only) The scaling configuration for the event source. For more information, see Configuring maximum concurrency for Amazon SQS event sources.
      ScalingConfig: 
         MaximumConcurrency: 20 

  LambdaBackfillWorkerFunctionLogGroup:
  # checkov:skip=CKV_AWS_158: No KMS encryption needed
    Type: AWS::Logs::LogGroup
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaBackfillWorkerFunctionName}"
      RetentionInDays: 14
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W84
            reason: "No KMS encryption needed"

  # Lambda Function that is triggered by the SQS queue
  LambdaBackfillWorkerFunction:
  # checkov:skip=CKV_AWS_115: Cuncurrency limit needs are customer dependent. This lambda is run only once and triggered by SQS, we don't anticipate the need for reserved concurrency for the moment.
  # checkov:skip=CKV_AWS_173: Environment variables are not sensitive.
  # checkov:skip=CKV_AWS_116: No DQL needed.
    Type: AWS::Lambda::Function
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      FunctionName: !Sub "${LambdaBackfillWorkerFunctionName}"
      Description: "CRCD Dashboard Backfill Worker - AWS Lambda function that partitions AWS Config files to visualize your past data on the dashboard"
      Runtime: python3.12
      Timeout: 600
      MemorySize: 128
      EphemeralStorage:
        Size: 512
      Role: !GetAtt LambdaBackfillWorkerFunctionRole.Arn
      Environment:
        Variables:
          BUCKET_NAME_VAR: !Ref DashboardBucketName
          SQS_QUEUE_URL_VAR: !Ref SQSBackfillingQueue
          # Must have all the variables of the standard partitioner
          ATHENA_DATABASE_NAME: !Ref AthenaDatabaseName
          ATHENA_QUERY_RESULTS_BUCKET_NAME: !Ref AthenaQueryResultBucketName
          ATHENA_WORKGROUP: !Ref AthenaWorkgroupName
          CRCD_TABLE_NAME: !Ref AthenaTableName
          # Reommended values for the lookback period of Config records
          CONFIG_HISTORY_TIME_LIMIT_MONTHS: 12
          CONFIG_SNAPSHOT_TIME_LIMIT_MONTHS: 6
          # Adding variables to enable/disable partitioning of ConfigSnapshot and ConfigHistory records
          # By default: both ConfigSnapshot and ConfigHistory are enabled 
          # Pass 0 to skip the record
          # Pass 1 to partition the record
          PARTITION_CONFIG_SNAPSHOT_RECORDS: 1
          PARTITION_CONFIG_HISTORY_RECORDS: 1
      Handler: index.lambda_handler
      Architectures:
        - "x86_64"
      TracingConfig:
        Mode: "Active"
      VpcConfig:
        SecurityGroupIds: []
        SubnetIds: []
        Ipv6AllowedForDualStack: false
      FileSystemConfigs: []
      LoggingConfig:
        LogFormat: "Text"
      RuntimeManagementConfig:
        UpdateRuntimeOn: "Auto"

      Code:
        ZipFile: |
          # Config Resource Compliance Dashboard
          # Backfill worker function triggered by SQS. The SQS item given here as input payload has an AWS Config prefix until the region. 
          # The prefix structure of an AWS Config file is as follows (for an AWS Config Snapshot): 
          #   ORG-ID/AWSLogs/ACCOUNT-NUMBER/Config/REGION/YYYY/MM/DD/ConfigSnapshot/objectname.json.gz
          # This function will create full AWS Config prefixes adding the 'YYYY/MM/DD/(ConfigSnapshot/ConfigHistory/) part until a configurable time in the past.
          # Then it will check if that prefix exists on the S3 Dashboard bucket, in which case the Athena partition will be created (if not existing).

          # Valid prefixes
          # 1. all Config history records
          # 2. Config snapshot records on all accounts an regions whose date is the last day of the month, from the last day of the month until 5 monhs ago
          # 3. Config snapshot records on all accounts an regions whose date is within the last 5 days

          from datetime import datetime, timedelta
          from calendar import monthrange
          from dateutil.relativedelta import relativedelta
          import calendar
          import os
          import re
          import time
          import json
          import boto3
          from botocore.exceptions import ClientError

          CRCD_TABLE_NAME = os.environ["CRCD_TABLE_NAME"]
          ATHENA_DATABASE_NAME = os.environ["ATHENA_DATABASE_NAME"]
          ATHENA_QUERY_RESULTS_BUCKET_NAME = os.environ["ATHENA_QUERY_RESULTS_BUCKET_NAME"]
          ATHENA_WORKGROUP = os.environ['ATHENA_WORKGROUP']
          LOGGING_ON = False  # enables additional logging to CloudWatch

          # Partitioning of ConfigSnapshot and ConfigHistory records is enabled from parameters
          # Pass 0 to skip the record
          # Pass 1 to partition the record
          PARTITION_ENABLED = '1'
          PARTITION_DISABLED = '0'
          PARTITION_CONFIG_SNAPSHOT_RECORDS = os.environ["PARTITION_CONFIG_SNAPSHOT_RECORDS"]
          PARTITION_CONFIG_HISTORY_RECORDS = os.environ["PARTITION_CONFIG_HISTORY_RECORDS"]

          # How much in the past we want to scan (months)
          env_parameter = os.environ.get("CONFIG_HISTORY_TIME_LIMIT_MONTHS", "12")
          CONFIG_HISTORY_TIME_LIMIT_MONTHS = int(env_parameter.strip()) if env_parameter.isdigit() else 12
          env_parameter = os.environ.get("CONFIG_SNAPSHOT_TIME_LIMIT_MONTHS", "6")
          CONFIG_SNAPSHOT_TIME_LIMIT_MONTHS = int(env_parameter.strip()) if env_parameter.isdigit() else 6

          DATA_SOURCE_CONFIG_HISTORY = 'ConfigHistory'
          DATA_SOURCE_CONFIG_SNAPSHOT = 'ConfigSnapshot'

          # This regular expressions pattern is compatible with how ControlTower Config logs AND also with how Config Logs are stored in S3 in standalone account
          # Structure for Config Snapshots: ORG-ID/AWSLogs/ACCOUNT-NUMBER/Config/REGION/YYYY/MM/DD/ConfigSnapshot/objectname.json.gz
          # Object name follows this pattern: ACCOUNT-NUMBER_Config_REGION_ConfigSnapshot_TIMESTAMP-YYYYMMDDHHMMSS_RANDOM-TEXT.json.gz
          # For example: 123412341234_Config_eu-north-1_ConfigSnapshot_20240306T122755Z_09c5h4kc-3jc7-4897-830v-d7a858325638.json.gz

          # this recognizes up to the region, it may be better for large buckets, then the processing part (this lambda)
          # can generate the dates and add a partition in case the S3 path exists
          PATTERN_REGION = r'^(?P<org_id>[\w-]+)?/?AWSLogs/(?P<account_id>\d+)/Config/(?P<region>[\w-]+)/$'
          PATTERN_REGION_COMPILED = re.compile(PATTERN_REGION)

          # create the clients that are needed
          athena = boto3.client('athena')
          s3 = boto3.client('s3')
          glue_client = boto3.client('glue')

          def lambda_handler(event, context):
              # Compliance and Inventory: the dashboard reports the current month of data and the full data from the previous 5 months
              # Event history: the dashbaord has all history - we limit the history to one year
              # This is handled in parameters CONFIG_HISTORY_TIME_LIMIT_MONTHS and CONFIG_SNAPSHOT_TIME_LIMIT_MONTHS
              current_date = datetime.now()
              config_snapshot_dates = []
              config_history_dates = []

              if PARTITION_CONFIG_SNAPSHOT_RECORDS == PARTITION_ENABLED:
                  config_snapshot_date_limit = current_date - relativedelta(months=CONFIG_SNAPSHOT_TIME_LIMIT_MONTHS)
                  # Create datetime for the last day of the month until 6 months ago - for Config snapshot files
                  # Get year and month from limit date
                  year = config_snapshot_date_limit.year
                  month = config_snapshot_date_limit.month

                  # Get last day of that month using monthrange
                  _, last_day = monthrange(year, month)
                  
                  # This is the date after which every AWS Config snapshot file have to be sent to SQS
                  min_config_snapshot_date = datetime(year, month, last_day)
                  print (f'This is the minimum date for Config snapshot files: {min_config_snapshot_date}')
                  
                  config_snapshot_dates = generate_config_snapshot_date_strings(min_config_snapshot_date)
                  if LOGGING_ON:
                      for dt in config_snapshot_dates:
                          print(f'Config snapshot date: {dt}')
              
              if PARTITION_CONFIG_HISTORY_RECORDS == PARTITION_ENABLED:
                  config_history_date_limit = current_date - relativedelta(months=CONFIG_HISTORY_TIME_LIMIT_MONTHS)
                  # Create datetime for Config history records limit
                  year = config_history_date_limit.year
                  month = config_history_date_limit.month
                  min_config_history_date = datetime(year, month, 1)
                  print (f'This is the minimum date for Config history files: {min_config_history_date}')

                  config_history_dates = generate_config_history_date_strings(min_config_history_date)
                  if LOGGING_ON:
                      for dt in config_history_dates:
                          print(f'Config history date: {dt}')

              # Now iterates through the prefixes received by SQS
              for rec in event['Records']:
                  print(f'CRCD Backfill ITERATION---START--------------------------------------------------')
                  event_body = rec['body']
                  
                  if LOGGING_ON:
                      print(f'rec = {rec}')
                      print(f'event_body = {event_body}')

                  message = json.loads(event_body)
                  event_bucket_name = message['Records'][0]['s3']['bucket']['name']
                  event_object_key = message['Records'][0]['s3']['object']['key']

                  print(f'Processing SQS payload: bucket = {event_bucket_name}, prefix = {event_object_key}')

                  # Backfill ConfigSnapshot records
                  if PARTITION_CONFIG_SNAPSHOT_RECORDS == PARTITION_ENABLED:
                      backfill(event_bucket_name, event_object_key, config_snapshot_dates, DATA_SOURCE_CONFIG_SNAPSHOT)

                  # Backfill ConfigHistory records
                  if PARTITION_CONFIG_HISTORY_RECORDS == PARTITION_ENABLED:
                      backfill(event_bucket_name, event_object_key, config_history_dates, DATA_SOURCE_CONFIG_HISTORY)

                  print(f'CRCD Backfill ITERATION---END--------------------------------------------------')

              print(f'CRCD Backfill Worker DONE')

              return {
                  'statusCode': 200,
                  'body': 'CRCD Backfill complete.'
              }

          def generate_config_snapshot_date_strings(start_date):
              """Generates the last day of the month for every month between start_date and today, plus the last 5 days"""
              today = datetime.now()
              
              date_strings = []
              current_date = start_date
              
              while current_date <= today:
                  # Get the last day of the current month
                  _, last_day = calendar.monthrange(current_date.year, current_date.month)
                  last_date_of_month = datetime(current_date.year, current_date.month, last_day)
                  
                  # If the last day of the month is not in the future, add it to the list
                  if last_date_of_month <= today:
                      # I need the '/' separator until creating the actual partition in Athena
                      date_strings.append(last_date_of_month.strftime("%Y/%-m/%-d"))
                      # date_strings.append(last_date_of_month.strftime("%Y-%-m-%-d"))
                  
                  # Move to the first day of the next month
                  if current_date.month == 12:
                      current_date = datetime(current_date.year + 1, 1, 1)
                  else:
                      current_date = datetime(current_date.year, current_date.month + 1, 1)
              
              # Add last 5 days (excluding today if it's already in the list)
              for i in range(5, 0, -1):
                  recent_date = today - timedelta(days=i)
                  # I need the '/' separator until creating the actual partition in Athena
                  date_str = recent_date.strftime("%Y/%-m/%-d")
                  # date_str = recent_date.strftime("%Y-%-m-%-d")
                  if date_str not in date_strings:  # Avoid duplicates
                      date_strings.append(date_str)

              return date_strings

          def generate_config_history_date_strings(start_date):
              """Returns an array of dates from the start_date to today"""
              today = datetime.now()
              date_strings = []
              current_date = start_date
              
              while current_date <= today:
                  # I need the '/' separator until creating the actual partition in Athena
                  date_strings.append(current_date.strftime("%Y/%-m/%-d"))
                  # date_strings.append(current_date.strftime("%Y-%-m-%-d"))
                  current_date += timedelta(days=1)
              
              return date_strings


          def check_s3_prefix(bucket_name, prefix):
              """
              Check existence of the given S3 prefix.
              Each prefix is checked individually since they're all different.
              """
              if LOGGING_ON:
                  print(f'Checking prefix: {prefix} on bucket: {bucket_name}')

              prefix_exists = False
              try:
                  response = s3.list_objects_v2(
                      Bucket=bucket_name,
                      Prefix=prefix,
                      MaxKeys=1
                  )
                  if 'Contents' in response:
                      prefix_exists = True

              except ClientError as e:
                  print(f"An error occurred: {e}")
                  raise  e # Re-raise the exception after logging   

              return prefix_exists


          def backfill(bucket_name, prefix_key, date_array, config_data_source):
              """For the given prefix will iterate through the date_array, 
                  generates S3 prefixes and create Athena partitions if the prefix is an actual S3 object"""
              if LOGGING_ON:
                  print(f'Backfilling prefix: {prefix_key} on bucket: {bucket_name} - datasource: {config_data_source}')

              for dt in date_array:
                  # prefix_key ends with '/'
                  object_key_parent = f'{prefix_key}{dt}/{config_data_source}/'
                  if LOGGING_ON:
                      print(f'Config history date: {dt} - object_key_parent = {object_key_parent}')

                  # Checks the current prefix exists on S3
                  if check_s3_prefix(bucket_name, object_key_parent):
                      # I need to run a regular expression to extract account ID and region
                      match  = re.match(PATTERN_REGION, prefix_key)
                      if not match:
                          # should never get here
                          print(f'Cannot match {prefix_key} as AWS Config file, skipping.')
                          continue
                      if LOGGING_ON:
                          print('Prefix exists! match.groupdict() = ', match.groupdict())
                      
                      accountid = match.groupdict()['account_id']
                      region = match.groupdict()['region']

                      # when handling Athena partitions, the date separator is "-" not "/"
                      dt_partition = dt.replace("/", "-")
                      print(f'Creating partition for item: bucket = {bucket_name}, accountid = {accountid}, region = {region}, dt = {dt_partition}')

                      # Check if partition exists: want both to avoid calling Athena API too much and create the partition only if it's not there already
                      # There's no need to create a partition on an S3 path that already exist - Athena will find new files in there
                      if not partition_exists(accountid, dt_partition, region, config_data_source):
                          # Create the partition if it doesn't exist
                          object_location = f's3://{bucket_name}/{object_key_parent}'
                          add_partition_if_not_exists(accountid, region, dt_partition, object_location, config_data_source)
                          print(f"Partition created for {accountid}, {dt_partition}, {region}, {config_data_source}")
                      else:
                          print(f"Partition already exists for {accountid}, {dt_partition}, {region}, {config_data_source}")

                  else:
                      if LOGGING_ON:
                          print(f'Prefix does not exist on S3: {object_key_parent} - will not create a partition in Athena')

                  
          def partition_exists(account_id, dt, region, data_source):
              """
              Check if a partition exists using the Glue API
              """
              try:
                  # Define partition values in the same order as defined in the Glue table
                  # Ensures using "-" as date separator in the partitions
                  dt_partition = dt.replace("/", "-")
                  partition_values = [account_id, dt_partition, region, data_source]

                  if LOGGING_ON:
                      print(f'Checking if partition exists: {partition_values}')
                  
                  # Get partition
                  response = glue_client.get_partition(
                      DatabaseName=ATHENA_DATABASE_NAME,
                      TableName=CRCD_TABLE_NAME,
                      PartitionValues=partition_values
                  )
                  
                  # If we get here, the partition exists
                  return True
              except ClientError as e:
                  # If the error is EntityNotFoundException, the partition doesn't exist
                  if e.response['Error']['Code'] == 'EntityNotFoundException':
                      return False
                  # For any other error, raise it
                  raise

          def add_partition_if_not_exists(accountid, region, dt, location, dataSource):
              # On S3 prefixes I needed "/" as date separator, but in the partitions I need to use "-"
              dt_partition = dt.replace("/", "-")

              execute_query(f"""
                  ALTER TABLE {CRCD_TABLE_NAME}
                  ADD IF NOT EXISTS PARTITION (accountid='{accountid}', dt='{dt_partition}', region='{region}', dataSource='{dataSource}')
                  LOCATION '{location}'
              """)

          # Runs an SQL statemetn against Athena
          def execute_query(query):
              """
                  Executes an Athena query (to create a partition) with additional logging and troubleshooting
                  It may happen that multiple objects are added to the bucket at the same time
                  Every object may need to create an Athena partition, and we need to handle concurrency and limit the impact on Athena API
              """
              print('Executing query:', query)
              try:
                  start_query_response = athena.start_query_execution(
                      QueryString=query,
                      QueryExecutionContext={
                          'Database': ATHENA_DATABASE_NAME
                      },
                      ResultConfiguration={
                          'OutputLocation': f's3://{ATHENA_QUERY_RESULTS_BUCKET_NAME}',
                      },
                      WorkGroup=ATHENA_WORKGROUP
                  )
                  query_execution_id = start_query_response['QueryExecutionId']
                  print(f'Query started with execution ID: {query_execution_id}')

                  is_query_running = True
                  while is_query_running:
                      time.sleep(1)
                      execution_status = athena.get_query_execution(
                          QueryExecutionId=query_execution_id
                      )
                      query_execution = execution_status['QueryExecution']
                      query_state = query_execution['Status']['State']
                      is_query_running = query_state in ('RUNNING', 'QUEUED')

                      if not is_query_running and query_state != 'SUCCEEDED':
                          error_reason = query_execution['Status'].get('StateChangeReason', 'No reason provided')
                          error_details = {
                              'QueryExecutionId': query_execution_id,
                              'State': query_state,
                              'StateChangeReason': error_reason,
                              'Database': ATHENA_DATABASE_NAME,
                              'WorkGroup': ATHENA_WORKGROUP,
                              'OutputLocation': f's3://{ATHENA_QUERY_RESULTS_BUCKET_NAME}'
                          }
                          print(f'Query failed: {json.dumps(error_details)}')
                          raise AthenaException(f'Query failed: {error_reason}')
                  
                  print(f'Query completed successfully. Execution ID: {query_execution_id}')
                  return query_execution_id
              except Exception as e:
                  print(f'Exception during query execution: {str(e)}')
                  raise

          class AthenaException(Exception):
              ''''This is raised only if the Query is not in state SUCCEEDED'''
              pass

    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W92
            reason: "This function does not need reserved concurrent executions"

  LambdaBackfillWorkerFunctionRole:
    Type: AWS::IAM::Role
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      Description: "CRCD Dashboard - Allows to add partitions to Athena and Glue, send logs to Cloudwatch, access Athena query results S3 bucket, receive from SQS queue. Each defined in separate policies"
      Path: "/"
      ManagedPolicyArns:
      - Ref: IAMManagedPolicyLambdaAthena
      - Ref: IAMManagedPolicyLambdaGlue
      - Ref: IAMManagedPolicyLambdaS3AthenaQueryResults
      - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      - Ref: IAMManagedPolicyLambdaS3ConfigObject
      - Ref: IAMManagedPolicySQSReceiveMessage

      MaxSessionDuration: 3600
      RoleName: "crcd-backfill-worker-role"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Action: "sts:AssumeRole"
          Effect: "Allow"
          Principal:
            Service: "lambda.amazonaws.com"
          Sid: "AllowLambdaAssumeRole"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  IAMManagedPolicySQSReceiveMessage:
    Type: AWS::IAM::ManagedPolicy
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-backfill-sqs-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that gives CRCD backfill Lambda execution role permission to receive SQS messages"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource: !GetAtt SQSBackfillingQueue.Arn
          Action:
          - "sqs:ReceiveMessage"
          - "sqs:DeleteMessage"
          - "sqs:GetQueueAttributes"
          Effect: "Allow"
          Sid: "MessagesFromSQS"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  IAMManagedPolicyLambdaAthena:
    Type: AWS::IAM::ManagedPolicy
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-backfill-athena-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that gives Athena permissions to CRCD backfill Lambda execution role"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource: !Sub "arn:${AWS::Partition}:athena:${AWS::Region}:${AWS::AccountId}:workgroup/${AthenaWorkgroupName}"
          Action:
          - "athena:StartQueryExecution"
          - "athena:GetQueryExecution"
          Effect: "Allow"
          Sid: "AthenaAccess"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  IAMManagedPolicyLambdaGlue:
    Type: AWS::IAM::ManagedPolicy
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-backfill-glue-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that gives Glue permissions to CRCD backfill Lambda execution role"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource:
          - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:table/${AthenaDatabaseName}/*"
          - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database/${AthenaDatabaseName}"
          - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:catalog"
          Action:
          - "glue:UpdatePartition"
          - "glue:GetTables"
          - "glue:GetTable"
          - "glue:GetPartitions"
          - "glue:GetPartition"
          - "glue:DeletePartition"
          - "glue:CreatePartition"
          - "glue:BatchGetPartition"
          - "glue:BatchDeletePartition"
          - "glue:BatchCreatePartition"
          Effect: "Allow"
          Sid: "GluePartitions"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  IAMManagedPolicyLambdaS3AthenaQueryResults:
    Type: AWS::IAM::ManagedPolicy
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-backfill-s3-athenaqueryresults-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that gives permissions on Athena query results S3 bucket to CRCD backfill Lambda execution role"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource:
          - !Sub "arn:${AWS::Partition}:s3:::${AthenaQueryResultBucketName}"
          - !Sub "arn:${AWS::Partition}:s3:::${AthenaQueryResultBucketName}/*"
          Action:
          - "s3:PutObject"
          - "s3:ListMultipartUploadParts"
          - "s3:ListBucket"
          - "s3:GetObject"
          - "s3:GetBucketLocation"
          Effect: "Allow"
          Sid: "S3AthenaQueryResults"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"

  IAMManagedPolicyLambdaS3ConfigObject:
    Type: AWS::IAM::ManagedPolicy
    DeletionPolicy: "Delete"
    UpdateReplacePolicy: "Delete"
    Properties:
      ManagedPolicyName: "crcd-backfill-s3-configfile-policy"
      Path: "/"
      Description: "CRCD Dashboard - Policy that allows CRCD backfill Lambda to read prefixes from the Config S3 bucket"
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
        - Resource:
          - !Sub "arn:${AWS::Partition}:s3:::${DashboardBucketName}"
          Action:
          - "s3:ListBucket"
          Effect: "Allow"
          Sid: "S3ConfigFileObject"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W28
            reason: "Policy names are as per our design"


Outputs:
  BackfillLambdaARN:
    Description: "ARN of the Lambda function that will run the backfill of AWS Config"
    Value: !GetAtt LambdaBackfillProducerFunction.Arn

  SQSBackfillQueueArn:
    Description: ARN of the SQS Queue
    Value: !GetAtt SQSBackfillingQueue.Arn
